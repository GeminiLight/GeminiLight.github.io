<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>【ML】时序预测的深度学习方法总结</title>
    <url>/2021/06/30/ML%20-%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ts-and-dl/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><img data-src="/resource/images/report/ts-and-dl/1.jpg" /></p>
<p>总结一些自己所阅读DL-based时序预测算法的文献，包括了一些基于CNN、RNN、Attention、Transformer、GNN、GAN的算法。</p>
<p>为了对抗拖延症打算先放图占个坑，后续会逐一补充文字说明。在文字总结时，自己也会继续扩充相关内容和进行勘误，并分享slide版本。</p>
<p>如您发现错误，欢迎指正~</p>
<a id="more"></a>
<p><img data-src="/resource/images/report/ts-and-dl/2.jpg" /></p>
<h2 id="时序预测简介">时序预测简介</h2>
<p><img data-src="/resource/images/report/ts-and-dl/3.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/4.jpg" /></p>
<h2 id="ts经典方法">TS经典方法</h2>
<p><img data-src="/resource/images/report/ts-and-dl/5.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/6.jpg" /></p>
<h2 id="dl方法汇总">DL方法汇总</h2>
<h3 id="基础模型canonical与seq2seq">基础模型：Canonical与Seq2Seq</h3>
<h4 id="mlp">MLP</h4>
<p><img data-src="/resource/images/report/ts-and-dl/7.jpg" /></p>
<h4 id="cnn">CNN</h4>
<p><img data-src="/resource/images/report/ts-and-dl/8.jpg" /></p>
<h4 id="rnn">RNN</h4>
<p><img data-src="/resource/images/report/ts-and-dl/9.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/10.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/11.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/12.jpg" /></p>
<h4 id="seq2seq">Seq2Seq</h4>
<p><img data-src="/resource/images/report/ts-and-dl/13.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/14.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/15.jpg" /></p>
<h3 id="attention-based-方法">Attention-based 方法</h3>
<p><img data-src="/resource/images/report/ts-and-dl/16.jpg" /></p>
<h4 id="lstnet">LSTNet</h4>
<p><img data-src="/resource/images/report/ts-and-dl/17.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/18.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/19.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/20.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/21.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/22.jpg" /></p>
<h4 id="mtnet">MTNet</h4>
<p><img data-src="/resource/images/report/ts-and-dl/23.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/24.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/25.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/26.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/27.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/28.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/29.jpg" /></p>
<h4 id="tpa-lstm">TPA-LSTM</h4>
<p><img data-src="/resource/images/report/ts-and-dl/30.jpg" /></p>
<h3 id="transformer">Transformer</h3>
<p><img data-src="/resource/images/report/ts-and-dl/31.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/32.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/33.jpg" /></p>
<h4 id="n-beats">N-BEATS</h4>
<p><img data-src="/resource/images/report/ts-and-dl/34.jpg" /></p>
<h4 id="informer">Informer</h4>
<p><img data-src="/resource/images/report/ts-and-dl/35.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/36.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/37.jpg" /></p>
<h3 id="gnn-based-方法">GNN-based 方法</h3>
<h4 id="mtgnn">MTGNN</h4>
<p><img data-src="/resource/images/report/ts-and-dl/38.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/39.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/40.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/41.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/42.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/43.jpg" /></p>
<h4 id="mtad-gat">MTAD-GAT</h4>
<p><img data-src="/resource/images/report/ts-and-dl/44.jpg" /></p>
<h4 id="gdn">GDN</h4>
<p><img data-src="/resource/images/report/ts-and-dl/45.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/46.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/47.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/48.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/49.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/50.jpg" /></p>
<p><img data-src="/resource/images/report/ts-and-dl/51.jpg" /></p>
<h4 id="summary">Summary</h4>
<p><img data-src="/resource/images/report/ts-and-dl/52.jpg" /></p>
]]></content>
      <categories>
        <category>ML - 机器学习</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>TS</tag>
      </tags>
  </entry>
  <entry>
    <title>【ML】机器学习在组合优化中的应用</title>
    <url>/2021/05/08/ML%20-%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ml-for-co/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><strong>组合优化</strong>问题遍布工业工程、交通运输、通讯网络等诸多领域，高效求解此类问题决定着实践场景中决策的质量；<strong>机器学习</strong>已经被广泛应用和落地于各种场景，用其来解决组合优化问题的尝试也成为了学术和工业界的热点。</p>
<p>自己浅研机器学习与组合优化方向已有一段时间，阅读相关顶会论文也有数十篇，故在此作出一些自己的总结和思考。作者才疏学浅，存在错误是难免的，欢迎反馈；如您兴趣相似，从事ML+CO的研究或应用，欢迎交流。</p>
]]></content>
      <categories>
        <category>ML - 机器学习</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>CO</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>【Presentation】DRL-SFCP&amp;#58 Adaptive Service Function Chains Placement with Deep Reinforcement Learning</title>
    <url>/2021/04/19/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/pre-nfv-drl-sfcp/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>This is a presentation on our paper named "DRL-SFCP" accepted by IEEE ICC 2021, where a novel algorithm integrating GNN, Seq2Seq model and DRL is proposed.</p>
<h2 id="document">Document</h2>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/1.jpg" /></p>
<a id="more"></a>
<p><strong>Title</strong>：DRL-SFCP: Adaptive Service Function Chains Placement with Deep Reinforcement Learning<br />
<strong>Authors</strong>：Tianfu Wang, Qilin Fan*, Xiuhua Li, Xu Zhang, Qingyu Xiong, Shu Fu and Min Gao<br />
<strong>Publication</strong>：ICC-2021 (CCF-C)<br />
<strong>Direction</strong>：Network Function Virtualization, Service Function Chain Placement<br />
<strong>Keywords</strong>：Service Function Chain Placement, Deep Reinforcement Learning, Asynchronous Advantage Actor-Critic, Graph Neural Network, Sequence-to-sequence Model<br />
<strong>Innovation</strong>：Propose a DRL algorithm integrating GNN and Seq2Seq model to capture sufficient information from input features and utilizing A3C methods to accelerate the training efficiency and model robustness.<br />
<a href="/resource/paper/icc-2021-drl-sfcp.pdf">paper</a> | <a href="/resource/slide/icc-2021-drl-sfcp-slide.pdf">slide</a></p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/2.jpg" /></p>
<p>This presentation is divided into 5 parts.</p>
<ul>
<li>Firstly, I will introduce the development and significance of Network Function Virtualization.</li>
<li>Secondly, the formulation of Service Function Chains Placement Problem will be provided.</li>
<li>And then, I will give the description of our proposed model.</li>
<li>Next, I will show some evaluation results.</li>
<li>Finally, our work and some future research directions will be concluded.</li>
</ul>
<h2 id="introduction-on-nfv">Introduction on NFV</h2>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/3.jpg" /></p>
<p>In the traditional network framework, with the increase of user service requests| and their diversity, the network service providers are required to invest| in expensive dedicated hardware to configure network functions| such as firewalls, video decoders and load balancers. It’s also difficult to manage and deploy.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/4.jpg" /></p>
<p>Conversely, NFV technology offers us the ability to virtualize the network service by using virtual machines. NFV reduces dependency on dedicated infrastructure, facilitating service provision and making networks relatively more flexible. It not only can improve resource utilization and security, but also allows for flexible management and maintenance.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/5.jpg" /></p>
<p>The global NFV market is expected to grow from USD twelve point nine billion in twenty nineteen to USD thirty six point three billion by twenty twenty four, at a CAGR (Compound Annual Growth Rate) of twenty two. nine % during the forecast period.<br />
The growing number of network complexities and ever-increasing demand for cloud-based services have resulted in the tremendous adoption of virtualization environments within the networking domain.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/6.jpg" /></p>
<p>Benefiting from NFV technology, a complex network service can be composed of a series of ordered virtual network functions, known as service function chain.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/7.jpg" /></p>
<p>In the online scenario, continuous SFC requests are attempted to be placed in the physical network under various resource constraints. It can be treated as a constrained combinational optimization problem which is also a NP-hard problem. However, it’s an challenging issue actively followed by infrastructure providers that place SFC requests as efficiently as possible to improve the resource utilization and quality of service.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/8.jpg" /></p>
<p>The existing solutions can be classified into three categories which all have their own limitations:</p>
<ul>
<li>Firstly, mathematical optimization-based methods require the prior knowledge of SFCs;</li>
<li>Secondly, heuristic and meta-heuristic based approaches usually fall into the local optimum;</li>
<li>Thirdly, some proposed reinforcement learning-based algorithms often confront with large search space and manually selected features.</li>
</ul>
<p>Therefore, we propose a novel deep reinforcement learning-based approach for online SFC deployment to mitigate or overcome these disadvantages.</p>
<h2 id="sfcp-formulation">SFCP Formulation</h2>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/9.jpg" /></p>
<p>Before introducing our model, we formulate the physical network as a weighted undirected graph. Similarly, each SFC Request is modelled as a weighted directed graph. Both of them have various nodes attributes such as CPU, storage, and memory. In this paper, we only take one link attributes, bandwidth, into consideration. There are four constraints when deploying SFC request in physical network.</p>
<ul>
<li>The first one is the node resources constraint: the required node resources shouldn’t exceed the remaining node resources.</li>
<li>The second one is the link bandwidth constraint: the total bandwidth mapped on the node does not exceed its remaining bandwidth.</li>
<li>The third one is the path constraint if the SFC is accepted, the path mapped in the physical network must pass through virtual network functions following the specified order in the request.</li>
<li>The last one is the deployment constraint: When handling the same SFC request, each physical node only can accommodate one VNF.</li>
</ul>
<p>Here, our objective is to maximize the long-term average revenue.</p>
<h2 id="proposed-drl-sfcp-model">Proposed DRL-SFCP Model</h2>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/10.jpg" /></p>
<p>Under the DRL framework, the interaction between agent and environment is defined as Markov decision process (MDP). We cast the placement of each SFC request as a finite-horizon MDP.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/11.jpg" /></p>
<p>The physical network and SFC requests are considered as the environment. Intuitively, the state includes the current physical network features and the SFC request demands. The action is actually one of the physical nodes to accommodate one VNF. Primarily, we employ a customized neural network as the agent to make placement decisions. According to the deployment result, the calculated reward is returned to award or punish the agent.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/12.jpg" /></p>
<p>The architecture of our model is illustrated in this figure, consisting of three main components: Embedding of physical network, Embedding of SFC request and Policy generation. Subsequently, I will explain them in detail.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/13.jpg" /></p>
<p>Graph Neural Network has recently become a hot topic in machine learning that has been widely used in many fields. Owing to its adaption to graph structure, we use GCN to extract features of physical network. Namely, the input node features are transformed into new node Representations by GCN.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/14.jpg" /></p>
<p>To capture orderly requirement of SFC requests, we utilize the encoder of the Seq2Seq model, which is implemented by gated recurrent unit (GRU) network. In the beginning, a sequence of VNF is input into the encoder to obtain the temporal relationship. Then, in each timestep, an action to place the current VNF will be output by the decoder.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/15.jpg" /></p>
<p>Overall, we use GCN and the Encoder of Seq2Seq model to capture information of physical network and SFC requests. After integrating the information, the decoder will generate the action distribution to select an action. Then, this action, a physical node, will attempt to place the current VNF.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/16.jpg" /></p>
<p>Next, the reward signal is designed to encourage the agent to place SFCs to maximize the long-term average revenue.</p>
<ul>
<li>When not finished and current VNF is placed successfully, the discounted SFC revenue will be award to the agent.</li>
<li>When not in the end and the current VNF is placed unsuccessfully, the agent will be punished with the discounted SFC revenue.</li>
<li>Particularly, when a complete SFC is placed successfully, the total SFC revenue will be returned to the agent.</li>
</ul>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/17.jpg" /></p>
<p>Advantage actor-critic is an efficient training method for discrete action space. Both actor and critic are composed of the aforementioned neural network but have disparate output. The state, including the features of the current physical network and VNF being placed, is input into the actor network. The agent selects the action based on the probability distribution, and the environment returns the reward. Meanwhile, we can obtain a value estimating the state after the same state is input into the critic network. Then, with the help of reward and this value, the Temporal error can be calculated to optimize the actor and critic.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/18.jpg" /></p>
<p>Besides, we use the well-known Asynchronous Advantage Actor-critic methods to enhance the training efficiency and model robustness. Briefly, There are several agents simultaneously explore in independent environments. At set intervals, experiences collected by them will be submitted to a master-worker to update global shared parameters. This parallel training architecture accelerates the training rate and Strengthen the model robustness.</p>
<h2 id="performance-simulation">Performance Simulation</h2>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/19.jpg" /></p>
<p>To demonstrate the performance of our approach, we conduct a series of experiments with these settings, compared with two other algorithms. Here, the orange asterisks mean corresponding parameters will be varied in the experiments.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/20.jpg" /></p>
<p>These figures compare our approach with other algorithms in terms of the long-term average revenue and acceptance rate, respectively. Obviously, when the number reaches 2-thousand, our model outperforms them in the long-term average revenue and acceptance rate.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/21.jpg" /></p>
<p>The figures provide the experiment results of three algorithms in various arrival rate conditions. Our model's impressive performance can always attain the best deployment results, which shows the excellent abilities of fitting and generalization of DNN.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/22.jpg" /></p>
<p>In this experiment, we estimate the average processing time in different SFC lengths to test our model's scalability. As the length of SFC request increases, the average processing time of our model is slightly inferior to GRC but much faster than MCTS, which demonstrates that our model is well-suited for Application on online scenarios.</p>
<h2 id="conclusion-and-future-work">Conclusion and Future work</h2>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/23.jpg" /></p>
<p>To conclude,</p>
<ul>
<li>we first model the SFC placement to MDP and employ an intelligent agent to guide SFC requests' online placement decisions.</li>
<li>Effective neural network architecture is then leveraged to extract sufficient information from the physical network and SFC requests, integrating GNN and Seq2Seq model.</li>
<li>What's more, we utilize the well-known A3C method to enhance the training efficiency and model robustness.</li>
</ul>
<p>Additionally, more powerful neural network architectures, more efficient DRL methods and More realistic modelling scenarios are expected to be studied in future.</p>
<p><img data-src="/resource/images/paper/icc-2021-drl-sfcp-slide/24.jpg" /></p>
]]></content>
      <categories>
        <category>RP - 科研论文</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>Paper</tag>
        <tag>NFV</tag>
        <tag>DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>【AI】人工智能课程集</title>
    <url>/2021/03/02/ML%20-%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/course-collection-on-ai/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cueXVxdWUuY29tL2RvY3Mvc2hhcmUvYjIyYTExN2ItMmU3ZC00NmRiLWFjODgtNjJiMjhiNTdjYzAz">Yuque优化排版<i class="fa fa-external-link-alt"></i></span>：支持筛选排序寻找自己所需要的课程</li>
</ul>
<p>这里收集了许多关于人工智能的高质量课程和教程，涵盖了</p>
<ul>
<li>机器学习 (Machine Learning, ML)</li>
<li>数据挖掘 (Data Mining, DM)</li>
<li>数学基础 Mathematics (Math)</li>
<li>深度学习 Deep Learning (DL)</li>
<li>强化学习 (Reinforcement Learning, RL)</li>
<li>深度强化学习 (Deep Reinforcement Learning, DRL)</li>
<li>分布式机器学习 (Distributed Machine Learning, DML)</li>
<li>自然语言处理 (Natural Language Processing, NLP)</li>
<li>计算机视觉 (Computer Vision, CV)</li>
<li>生成式对抗网络 (Generative Adversarial Network, GAN)</li>
<li>图神经网络 (Graph Neural Network, GNN)</li>
<li>知识图谱 (Knowledge Graph, KG)</li>
<li>推荐系统 (Recommander System, RS)</li>
<li>因果推理 (Causal Inference, CI)</li>
<li>组合优化 (Combinational Optimization, CO)</li>
<li>TensorFlow</li>
<li>PyTorch</li>
<li>...</li>
</ul>
<a id="more"></a>
<blockquote>
<p>提示:</p>
<ol type="1">
<li>点击 <code>课程名称</code> 栏中的链接一般会跳转到课程主页，在那里你可以找到更多相关资料</li>
<li>课程名称中含有中文的一般是中文授课</li>
</ol>
</blockquote>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">最近更新</th>
<th style="text-align: center;">类别</th>
<th style="text-align: center;">方向</th>
<th style="text-align: center;">课程名称</th>
<th style="text-align: center;">机构</th>
<th style="text-align: center;">教师</th>
<th style="text-align: center;">视频</th>
<th style="text-align: center;">更多资源</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">NLP</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL3dlYi5zdGFuZm9yZC5lZHUvY2xhc3MvY3MyMjRuLw==">CS224n: Natural Language Processing with Deep Learning<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: center;">Chris Manning, John Hewitt Head TA</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMXI0NDExZjd0ZA==">BiliBili (2019)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">GNN</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL3dlYi5zdGFuZm9yZC5lZHUvY2xhc3MvY3MyMjR3Lw==">CS224W: Machine Learning with Graphs<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWpFNDExNzdBND9mcm9tPXNlYXJjaCZhbXA7c2VpZD0xNDI0NjU0NjIyMjAzMzI3MjEyNA==">Bilibili (2019)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">RL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93ZWIuc3RhbmZvcmQuZWR1L2NsYXNzL2NzMjM0Lw==">CS234: Reinforcement Learning<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: center;">Emma Brunskill</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMUNjNDExaDdRUT9mcm9tPXNlYXJjaCZhbXA7c2VpZD0xMTA1MTM4NzIwOTY1Mzc0MjkzOA==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">ML</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL2NzMjI5LnN0YW5mb3JkLmVkdS9zeWxsYWJ1cy1mYWxsMjAyMC5odG1s">CS229: Machine Learning<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: center;">Tengyu Ma, Andrew Ng (吴恩达), Chris Ré</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMUpFNDExdzdVYj9mcm9tPXNlYXJjaCZhbXA7c2VpZD0xMDE4MDc5NjQxMDgyMzM5OTMw">BiliBili (2018)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZlbmdkdTc4L0NvdXJzZXJhLU1MLUFuZHJld05nLU5vdGVz">Note<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">DL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9jczIzMC5zdGFuZm9yZC5lZHUv">CS230: Deep Learning<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYW5kcmV3bmcub3JnLw==">Andrew Ng (吴恩达), Kian Katanforoosh<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVliNDExNzd0bQ==">BiliBili (2018)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">CV</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL2NzMjMxbi5zdGFuZm9yZC5lZHUv">CS231n: Convolutional Neural Networks for Visual Recognition<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9wcm9maWxlcy5zdGFuZm9yZC5lZHUvZmVpLWZlaS1saQ==">Fei-Fei Li (李飞飞)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMW5KNDExejdmZT9mcm9tPXNlYXJjaCZhbXA7c2VpZD0yNDc1MzM3NTk0ODA2OTExNTM2">Bilibili (2019)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2019</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">GAN</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9kZWVwZ2VuZXJhdGl2ZW1vZGVscy5naXRodWIuaW8v">CS236: Deep Generative Models<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: center;">Stefano Ermon, Aditya Grover</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVNKNDExYjdEOD9mcm9tPXNlYXJjaCZhbXA7c2VpZD0xNzYxMTc3MzE1MzMxNDUyODc4MA==">Bilibili (2019)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">DM</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93ZWIuc3RhbmZvcmQuZWR1L2NsYXNzL2NzMjQ2Lw==">CS246: Mining Massive Data Sets<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: center;">Jure Leskovec</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVNDNHkxODd4MT9mcm9tPXNlYXJjaCZhbXA7c2VpZD03MzExNjU2NTYwMjgxODkxOTk=">Bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">RL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9jczMzMi5zdGFuZm9yZC5lZHUvIyFzeWxsYWJ1cy5tZA==">CS332: Advanced Survey of Reinforcement Learning<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: center;">Emma Brunskill</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">KG</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93ZWIuc3RhbmZvcmQuZWR1L2NsYXNzL2NzNTIwLw==">CS520: Knowledge Graphs<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: center;">Vinay K. Chaudhri, Naren Chittar, Michael Genesereth</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMUQ1NDExVzdSaQ==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93ZWIuc3RhbmZvcmQuZWR1L2NsYXNzL2VlMzY0YS9pbmRleC5odG1s">EE364a: Convex Optimization I<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: center;">John Duchi</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">DL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL2RlZXBsZWFybmluZy5jcy5jbXUuZWR1L1MyMS9pbmRleC5odG1s">CS 11-785: Introduction to Deep Learning<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">CMU</td>
<td style="text-align: center;">Graham Neubig</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWhvNHkxZDdFYT9mcm9tPXNlYXJjaCZhbXA7c2VpZD0xODMyMTE3MjM4NzQ4ODYyNzY1NA==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2019</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">NLP</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL3d3dy5waG9udHJvbi5jb20vY2xhc3Mvbm40bmxwMjAxOS8=">CS 11-747: Neural Networks for NLP<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">CMU</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWt0NDExaDdLdA==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">AI</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9pbnN0LmVlY3MuYmVya2VsZXkuZWR1L35jczE4OC8=">CS 188: Introduction to Artificial Intelligence<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">UC Berkeley</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWtBNDExRTdKZz9mcm9tPXNlYXJjaCZhbXA7c2VpZD03MjMyNzY2MDkzMDY1NDg1ODAw">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">DL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL25ldXJvdGVjaC1iZXJrZWxleS9uZXVyb3RlY2gtY291cnNl">CS 198-96: Introduction to Neurotechnology<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">UC Berkeley</td>
<td style="text-align: center;">Deven Navani</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMUU1NHkxZDdSQj9mcm9tPXNlYXJjaCZhbXA7c2VpZD04ODcwNTE4NjYzNjE4MjA5MjE4">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">DRL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL3JhaWwuZWVjcy5iZXJrZWxleS5lZHUvZGVlcHJsY291cnNlLw==">CS 285: Deep Reinforcement Learning<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">UC Berkeley</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9wZW9wbGUuZWVjcy5iZXJrZWxleS5lZHUvfnN2bGV2aW5lLw==">Sergey Levine<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWtBNDExNzdOYQ==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">DL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL2ludHJvdG9kZWVwbGVhcm5pbmcuY29tL2luZGV4Lmh0bWw=">MIT 6. S191: Introduction to Deep Learning<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">MIT</td>
<td style="text-align: center;">Alexander Amini, Ava Soleimany</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWpvNHkxZDdSNj9mcm9tPXNlYXJjaCZhbXA7c2VpZD03ODcyNTg2MTA5MjY2NjYzOTIz">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">AI</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9jczUwLmhhcnZhcmQuZWR1Lw==">CS50: Introduction to Artificial Intelligence with Python<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Harvard</td>
<td style="text-align: center;">David J. Malan</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVVLNHkxcjd4bQ==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">DL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9uaWVzc25lci5naXRodWIuaW8vSTJETC8=">Introduction to Deep Learning<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">TUM</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cubmllc3NuZXJsYWIub3JnL2luZGV4Lmh0bWw=">Matthias Nießner<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMUJRNHkxQTd3cQ==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">GNN</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9nbm4uc2Vhcy51cGVubi5lZHUv">ESE 680: Graph Neural Networks<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">UPenn</td>
<td style="text-align: center;">Alejandro Ribeiro</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTd5NHkxQjdKTg==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2015</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">RL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2VuZ2dlbi9EZWVwTWluZC1BZHZhbmNlZC1EZWVwLUxlYXJuaW5nLWFuZC1SZWluZm9yY2VtZW50LUxlYXJuaW5n">Reinforcement Learning<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">UCL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuZGF2aWRzaWx2ZXIudWsv">David Silver<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWt0NDExRDc2ZT9mcm9tPXNlYXJjaCZhbXA7c2VpZD0xMTU5MTIzODg0Njc2MTcxMTgzMA==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj0ycFd2N0dPdnVmMA==">Youtube<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">DL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9hdGNvbGQuZ2l0aHViLmlvL3B5dG9yY2gtRGVlcC1MZWFybmluZy8=">Deep Learning (Pytorch)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">NYU</td>
<td style="text-align: center;">Yann Lecun, Alfredo Canziani</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTk3NDExTTdnRw==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">AI</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9ldGh6LmNoL2RlLmh0bWw=">Course Series Including AI, BD, IR et al.<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">ETH Zürich</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMU55NHkxNjc3UQ==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">GAN</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuZGVlcGxlYXJuaW5nLmFpL3Byb2dyYW0vZ2VuZXJhdGl2ZS1hZHZlcnNhcmlhbC1uZXR3b3Jrcy1nYW5zLXNwZWNpYWxpemF0aW9uLw==">Generative Adversarial Networks (GANs) Specialization<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">DeepLearning. AI</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYW5kcmV3bmcub3JnLw==">Andrew Ng (吴恩达)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVR2NDExZTdmQw==">Bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RzdWlyYWsvZGVlcGxlYXJuaW5nLmFp">Code<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2019</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">AI</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2xlYXJuL2FpLWZvci1ldmVyeW9uZQ==">AI For Everyone<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">DeepLearning. AI</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYW5kcmV3bmcub3JnLw==">Andrew Ng (吴恩达)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMXJKNDExdDdVUQ==">bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">CO</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9kdXZlbmF1ZC5naXRodWIuaW8vbGVhcm5pbmctdG8tc2VhcmNoLw==">CSC2547: Learning to Search<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">UofT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">ML</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cueHVldGFuZ3guY29tL2NvdXJzZS9USFUwODA5MTAwMTAyNi81ODgyNzk2P2NoYW5uZWw9c2VhcmNoX3Jlc3VsdA==">大数据机器学习<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">THU</td>
<td style="text-align: center;">袁春</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cueHVldGFuZ3guY29tL2NvdXJzZS9USFUwODA5MTAwMTAyNi81ODgyNzk2P2NoYW5uZWw9c2VhcmNoX3Jlc3VsdA==">学堂在线<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">TensorFlow</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuaWNvdXJzZTE2My5vcmcvbGVhcm4vUEtVLTEwMDI1MzYwMDI/dGlkPTE0NTI5Mzc0NzEjL2xlYXJuL2Fubm91bmNl">人工智能实践：Tensorflow笔记<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">PKU</td>
<td style="text-align: center;">曹健</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuaWNvdXJzZTE2My5vcmcvY291cnNlL1BLVS0xMDAyNTM2MDAyP3RpZD0xNDUyOTM3NDcx">MOOC<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">AI</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuaWNvdXJzZTE2My5vcmcvY291cnNlL1BLVS0xMDAzNDcxMDA5P3RpZD0xNDUwMjA5NDQ0">人工智能与信息社会<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">PKU</td>
<td style="text-align: center;">陈斌</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuaWNvdXJzZTE2My5vcmcvY291cnNlL1BLVS0xMDAzNDcxMDA5P3RpZD0xNDUwMjA5NDQ0Iy9pbmZv">MOOC<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">AI</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuaWNvdXJzZTE2My5vcmcvY291cnNlL1BLVS0xMDAyMTg4MDAzIy9pbmZv">人工智能原理<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">PKU</td>
<td style="text-align: center;">王文敏</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuaWNvdXJzZTE2My5vcmcvY291cnNlL1BLVS0xMDAyMTg4MDAz">MOOC<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">AI</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuaWNvdXJzZTE2My5vcmcvY291cnNlL1pKVS0xMDAzMzc3MDI3Iy9pbmZv">人工智能：模型与算法<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">ZJU</td>
<td style="text-align: center;">吴飞</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuaWNvdXJzZTE2My5vcmcvY291cnNlL1pKVS0xMDAzMzc3MDI3Iy9pbmZv">MOOC<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2017</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">ML</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL3d3dy56anV2aXBhaS5jb20v">机器学习<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">ZJU</td>
<td style="text-align: center;">胡浩基</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWRKNDExQjdnaA==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">NLP</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cueHVldGFuZ3guY29tL2NvdXJzZS9TSlRVMDg1NDAwMjIwNy81OTQ0NDczP2NoYW5uZWw9c2VhcmNoX3Jlc3VsdA==">自然语言处理<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">SJTU</td>
<td style="text-align: center;">赵海</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cueHVldGFuZ3guY29tL2NvdXJzZS9TSlRVMDg1NDAwMjIwNy81OTQ0NDczP2NoYW5uZWw9c2VhcmNoX3Jlc3VsdA==">学堂在线<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">ML</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9pbnMuc2p0dS5lZHUuY24vcGVvcGxlL3h1emhpcWluL21sY291cnNlLmh0bWw=">统计计算与机器学习<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">SJTU</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9pbnMuc2p0dS5lZHUuY24vcGVvcGxlL3h1emhpcWluLw==">许志钦<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9zcGFjZS5iaWxpYmlsaS5jb20vOTU5NzU0NDEvY2hhbm5lbC9kZXRhaWw/Y2lkPTEwNzcyNQ==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2018</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">ML</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL29jdy5zanR1LmVkdS5jbi9HMlMvT0NXL2NuL0NvdXJzZUhvbWUuaHRtbA==">机器学习导论<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">SJTU</td>
<td style="text-align: center;">张志华</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWp0NDExYjc2bg==">Bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">DM</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cueHVldGFuZ3guY29tL2NvdXJzZS90am51MDgwOTEwMDIzNzIvNTg4MzczOD9jaGFubmVsPXNlYXJjaF9yZXN1bHQ=">数据挖掘<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">TJU</td>
<td style="text-align: center;">喻梅, 王建荣, 于健</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cueHVldGFuZ3guY29tL2NvdXJzZS90am51MDgwOTEwMDIzNzIvNTg4MzczOD9jaGFubmVsPXNlYXJjaF9yZXN1bHQ=">学堂在线<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">CV</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9jdi14dWViYS5jbHViLw==">计算机视觉与深度学习<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">BUPT</td>
<td style="text-align: center;">鲁鹏</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVY1NHkxQjdLMw==">Bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">CV</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9jdi14dWViYS5jbHVi">计算机视觉基础<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">BUPT</td>
<td style="text-align: center;">鲁鹏</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMW56NHkxOTdRdg==">Bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">ML</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9zcGVlY2guZWUubnR1LmVkdS50dy9+aHlsZWUvbWwvMjAyMS1zcHJpbmcuaHRtbA==">Machine Learning (机器学习)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">NTU</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9zcGVlY2guZWUubnR1LmVkdS50dy9+dGxrYWdrLw==">李宏毅<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVd2NDExaDdrTj9mcm9tPXNlYXJjaCZhbXA7c2VpZD0xNTE3MDk5MzQ2MDM5NTkwNTExNA==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2RhdGF3aGFsZWNoaW5hL2xlZW1sLW5vdGVz">Note<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">NLP</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL3NwZWVjaC5lZS5udHUuZWR1LnR3L350bGthZ2svY291cnNlc19ETEhMUDIwLmh0bWw=">Deep Learning for Human Language Processing (深度学习人类语言处理)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">NTU</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9zcGVlY2guZWUubnR1LmVkdS50dy9+dGxrYWdrLw==">李宏毅<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVFFNDExcDd6Mw==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL3NwZWVjaC5lZS5udHUuZWR1LnR3L350bGthZ2svY291cnNlcy5odG1s">Linear Algebra (线性代数)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">NTU</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9zcGVlY2guZWUubnR1LmVkdS50dy9+dGxrYWdrLw==">李宏毅<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTh2NDExQjdGUQ==">Bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">DL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuY3NpZS5udHUuZWR1LnR3L35taXVsYWIvczEwOC1hZGwvc3lsbGFidXM=">Applied Deep Learning (应用机器学习)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">NTU</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuY3NpZS5udHUuZWR1LnR3L355dmNoZW4v">陈蕴侬<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTlnNHkxYjd2eD9mcm9tPXNlYXJjaCZhbXA7c2VpZD0xMDEyMDQ4MjU0ODc0NTA0MDc5Mw==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">RL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vYy9TaHVzZW5XYW5nL3BsYXlsaXN0cz92aWV3PTUwJmFtcDtzb3J0PWRkJmFtcDtzaGVsZl9pZD0y">Reinforcement Learning (强化学习)<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">SIT</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL3dhbmdzaHVzZW4uZ2l0aHViLmlvLw==">Shusen Wang<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMUJFNDExVzdUQT9mcm9tPXNlYXJjaCZhbXA7c2VpZD00MzMwODczMDQ1ODk3MDI1ODQz">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Tutorial</td>
<td style="text-align: center;">ML</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMXU3NDExNjd2Sw==">机器学习: 从0到1学Tensorflow<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Google</td>
<td style="text-align: center;">Laurence Moroney</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMXU3NDExNjd2Sw==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2019</td>
<td style="text-align: center;">Tutorial</td>
<td style="text-align: center;">TensorFlow</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vcGxheWxpc3Q/bGlzdD1QTFFZMkg4clJveXZ3TGJ6Ym5LSjU5TmtadlFBVzl3TGJ4">Coding TensorFlow<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Google</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTRFNDExOTdORw==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Tutorial</td>
<td style="text-align: center;">GNN</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1BhZGRsZVBhZGRsZS9QR0wvdHJlZS9tYWluL2NvdXJzZQ==">PGL: 图神经网络7日打卡营<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Baidu</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMXJmNHkxdjdjVQ==">Bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1BhZGRsZVBhZGRsZS9QR0wvdHJlZS9tYWluL2NvdXJzZQ==">Slide<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Tutorial</td>
<td style="text-align: center;">ML</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9tb2ZhbnB5LmNvbS8=">有趣的机器学习<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Individual</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9tb2ZhbnB5LmNvbS8=">莫烦Python<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9zcGFjZS5iaWxpYmlsaS5jb20vMjQzODIxNDg0P3NwbV9pZF9mcm9tPTMzMy43ODguYl83NjVmNzU3MDY5NmU2NjZmLjI=">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">ML</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWFFNDExbzdxZA==">机器学习-白板推导系列<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Individual</td>
<td style="text-align: center;">Shuhuai</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWFFNDExbzdxZA==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cueXVxdWUuY29tL2Jvb2tzL3NoYXJlL2Y0MDMxZjY1LTcwYzEtNDkwOS1iYTAxLWM0N2MzMTM5ODQ2Nj8j">Note<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr class="even">
<td style="text-align: center;">2019</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">ML</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMXZKNDExODdoaw==">菜菜的机器学习sklearn<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Individual</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9zcGFjZS5iaWxpYmlsaS5jb20vMjkzMjIwNz9zcG1faWRfZnJvbT0zMzMuNzg4LmJfNzY1Zjc1NzA2OTZlNjY2Zi4y">菜菜TsaiTsai<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMXZKNDExODdoaw==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3lzMTMwNS9za2xlYXJuLW1s">Code<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Course</td>
<td style="text-align: center;">CI</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYnJhZHluZWFsLmNvbS9jYXVzYWwtaW5mZXJlbmNlLWNvdXJzZQ==">Introduction to Causal Inference<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Individual</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYnJhZHluZWFsLmNvbS9hYm91dG1l">Brady Neal<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMW5aNHkxSzc4aQ==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Tutorial</td>
<td style="text-align: center;">TensorFlow</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj10UFlqM2ZGSkdqaw==">TensorFlow 2.0 Complete Course - Python Neural Networks for Beginners Tutorial<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">freeCodeCamp</td>
<td style="text-align: center;">Tim Ruscica</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMXRFNDExeDdxNQ==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2019</td>
<td style="text-align: center;">Lecture</td>
<td style="text-align: center;">GNN</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTM1NDExdDc2WD9wPTI=">图形图像学科前沿研习班：图神经网络专题<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">中国图象图形学学会</td>
<td style="text-align: center;">唐杰, 崔鹏, 石川, 严骏驰</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTM1NDExdDc2WD9wPTI=">Bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9wYW4uYmFpZHUuY29tL3MvMUl2dmhOZmpQZFByeXpiVHh6MmhsblE=">Silde (密码: w6vg)<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Workshop</td>
<td style="text-align: center;">GNN</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9ldmVudC5iYWFpLmFjLmNuL2Nvbi9nbm4tb25saW5lLXdvcmtzaG9wLTIwMjAv">图表示学习和图神经网络的最新理论进展和应用<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">BAAI (智源研究院)</td>
<td style="text-align: center;">宋国杰, 沈华伟, 唐杰, 石川</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMXpwNHkxMTdiQg==">Bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Conference Tutorial</td>
<td style="text-align: center;">DML</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvYWFhaS0yMDIxLXR1dG9yaWFsLWFoOS9ob21l">AAAI 2021 Tutorial: Simplifying and Automating Parallel Machine Learning via a Programmable and Composable Parallel ML System<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">AAAI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTNONDExOTdBRD9wPTU=">Bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Conference Tutorial</td>
<td style="text-align: center;">ML</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9leHBsYWlubWwtdHV0b3JpYWwuZ2l0aHViLmlvL2FhYWkyMQ==">AAAI 2021 Tutorial: Explaining Machine Learning Predictions: State-of-the-art, Challenges, and Opportunities<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">AAAI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTNONDExOTdBRD9wPTI=">Bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Conference Tutorial</td>
<td style="text-align: center;">GNN</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL2NzZS5tc3UuZWR1L35tYXlhbzQvdHV0b3JpYWxzL2FhYWkyMDIxLw==">AAAI 2021 Tutorial: Graph Neural Networks: Models and Applications<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">AAAI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMUdBNDExTTdQTD9mcm9tPXNlYXJjaCZhbXA7c2VpZD0xMjI3MDk3OTM2OTgxMTQxMDk1MA==">Bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Conference Tutorial</td>
<td style="text-align: center;">CO</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvbWwtY28tYWFhaS0yMS8=">AAAI 2021 Tutorial: ML &amp; CO<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">AAAI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021</td>
<td style="text-align: center;">Conference Tutorial</td>
<td style="text-align: center;">Meta Learning</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL21pdC5lZHUvYWFhaTIwMjFtZXRhbGVhcm5pbmd0dXRvcmlhbA==">AAAI 2021 Tutorial: Meta Learning<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">AAAI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWV5NHkxbjdZOA==">Bilibili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Conference Tutorial</td>
<td style="text-align: center;">RS</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vcGxheWxpc3Q/bGlzdD1QTF9sc2JBc0xfbzJDdzBWWFE0R2Q4Ql81V1VtWnFRdUlN">KDD 2020 Tutorial: Building Recommender Systems with PyTorch<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">Facebook</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTJ0NHkxaTdaeA==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Conference Tutorial</td>
<td style="text-align: center;">CI</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL2tkZDIwMjB0dXRvcmlhbC50aHVtZWRpYWxhYi5jb20vIw==">KDD 2020 Tutorial: Causal Inference Meets Machine Learning<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">KDD</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Conference Tutorial</td>
<td style="text-align: center;">GNN</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cDovL3d3dy5jYWx2aW56YW5nLmNvbS9rZGQyMDIwX3R1dG9yaWFsX21lZGljYWxfZ3JhcGhfYW5hbHl0aWNzLmh0bWw=">KDD 2020 Tutorial: Recent Advances on Graph Analytics and Its Applications in Healthcare<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">KDD</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Conference Tutorial</td>
<td style="text-align: center;">GNN</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9haS50ZW5jZW50LmNvbS9haWxhYi9tbC9LREQtRGVlcC1HcmFwaC1MZWFybmluZy5odG1s">KDD 2020 Tutorial: Advanced Deep Graph Learning: Deeper, Faster, Robuster, and Unsupervised<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">KDD</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Conference Tutorial</td>
<td style="text-align: center;">GNN</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9jaHV4dXpoYW5nLmdpdGh1Yi5pby9LREQyMF9UdXRvcmlhbC5odG1s">KDD 2020 Tutorial: Multi-modal Network Representation Learning: Methods and Applications<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">KDD</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Conference Tutorial</td>
<td style="text-align: center;">AD</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcva2RkMjAyMGRlZXBleWUvaG9tZQ==">KDD 2020 Tutorial: Deep Learning for Anomaly Detection<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">KDD</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Conference Tutorial</td>
<td style="text-align: center;">NLP</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0VyaWMtV2FsbGFjZS9pbnRlcnByZXRhYmlsaXR5LXR1dG9yaWFsLWVtbmxwMjAyMC8=">EMNLP 2020 Tutorial: Interpreting Predictions of NLP Models<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">EMNLP</td>
<td style="text-align: center;">Eric Wallace, Matt Gardner, Sameer Singh</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMW5LNHkxNzdCVw==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Conference Tutorial</td>
<td style="text-align: center;">RL</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvbWJybC10dXRvcmlhbA==">ICML 2020 Tutorial: Model-Based Methods in Reinforcement Learning<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;">ICML</td>
<td style="text-align: center;">Igor Mordatch and Jessica Hamrick</td>
<td style="text-align: center;"><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWdhNHkxeDd3OA==">BiliBili<i class="fa fa-external-link-alt"></i></span></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>ML - 机器学习</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>【分布式ML】机器学习中的并行计算</title>
    <url>/2021/02/19/ML%20-%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/dml-parallel-computing-and-machine-learning/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>随着深度学习模型的不断增大、数据的不断增多，并行计算成为了解决机器学习训练难题的一种主流技术。本文主要是对Shusen Wang的《分布式机器学习》课程的笔记记录和扩展，仅是对机器学习中并行计算方式的概述。</p>
<a id="more"></a>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-1.png" /></p>
<h2 id="概述">概述</h2>
<h3 id="什么是并行计算">什么是并行计算？</h3>
<p>使用多个处理器来加速计算过程，使时钟运行时间更短 多个处理器的情况包括</p>
<ul>
<li>一台计算机上有多个CPU</li>
<li>多个计算机的CPU</li>
</ul>
<h3 id="为什么机器学习中需要并行计算">为什么机器学习中需要并行计算？</h3>
<ul>
<li>深度学习模型大。模型参数越来越多</li>
<li>训练数据多。比如ImageNet有14M张图片</li>
<li>计算资源成本高。在大模型上训练大数据集的必然结构</li>
</ul>
<h3 id="并行计算与分布式计算">并行计算与分布式计算</h3>
<p>分布式计算可以看做是一种并行计算。两者的界限是模糊的，但从机器学习研究人员角度出发</p>
<ul>
<li><p>并行计算 Parallel computing</p>
<ul>
<li>数据或模型被分划分成多份，但这些CPU都属于一台电脑，并行计算任务仍在一台电脑节点执行</li>
</ul></li>
<li><p>分布式计算 Distributed computing</p>
<ul>
<li>数据或模型被分划分成多份，且在不同的节点上进行计算</li>
</ul></li>
</ul>
<h3 id="房价预测实例">房价预测实例</h3>
<h4 id="线性预测器">线性预测器</h4>
<ul>
<li><p>输入：<span class="math inline">\(X \in \mathbb{R}^d\)</span>（有关房价的特征）</p></li>
<li><p>预测：<span class="math inline">\(f(X) = X^T W\)</span>（房价）</p>
<ul>
<li>假设有<span class="math inline">\(d\)</span>个房价相关的特征，如面积、年限等</li>
<li><span class="math inline">\(f(X) = w_1x_1 + w_2x_2 + w_3x_3 + \cdots + w_dx_d\)</span></li>
<li>其中<span class="math inline">\(x_i\)</span>表示特征，<span class="math inline">\(w_i\)</span>是其权重</li>
</ul></li>
</ul>
<h4 id="训练模型">训练模型</h4>
<ul>
<li><p>训练集</p>
<ul>
<li>输入：<span class="math inline">\(X_1, \cdots, X_n \in \mathbb{R}^d\)</span></li>
<li>标签：<span class="math inline">\(y_1, \cdots, y_n \in \mathbb{R}\)</span></li>
</ul></li>
<li><p>损失函数：<span class="math inline">\(L(\mathbf{W})=\sum_{i=1}^{n} \frac{1}{2}\left(\mathbf{X}_{i}^{T} \mathbf{W}-y_{i}\right)^{2}\)</span></p></li>
<li><p>最小二乘回归：<span class="math inline">\(\mathbf{W}^{*}=\underset{\mathbf{W}}{\operatorname{argmin}} L(\mathbf{W})\)</span></p></li>
<li><p>梯度下降：</p>
<ul>
<li>梯度：<span class="math inline">\(g(W)=\frac{\partial L(\mathrm{W})}{\partial \mathrm{W}}=\sum_{i=1}^{n} \frac{\partial \frac{1}{2}\left(\mathbf{X}_{i}^{T} \mathrm{W}-y_{i}\right)^{2}}{\partial \mathrm{W}}=\sum_{i=1}^{n}\left(\mathbf{X}_{i}^{T} \mathbf{W}-y_{i}\right) \mathbf{X}_{i}\)</span></li>
<li><span class="math inline">\(g(\mathbf{W})=\sum_{i=1}^{n} g_{i}(\mathbf{W}), \text { where } g_{i}(\mathbf{W})=\left(\mathbf{X}_{i}^{T} \mathbf{W}-y_{i}\right) \mathbf{X}_{i}\)</span></li>
<li><span class="math inline">\(W_{t+1} = W_t - \alpha \cdot g(W_t)\)</span></li>
</ul></li>
</ul>
<p>我们可以发现，当仅一个处理器处理大的模型`和海量数据的时候，计算量是巨大的。</p>
<h4 id="并行梯度下降训练">并行梯度下降训练</h4>
<p>并行梯度下降的基本思想便是利用多个处理器来分别利用自己的数据计算梯度，最后通过聚合或其他方式来实现并行计算梯度下降以加速模型训练过程。 下面的图示便展示了两个处理器各自利用一半数据计算梯度<span class="math inline">\(\tilde{g}_1\)</span>、<span class="math inline">\(\tilde{g}_2\)</span>，然后将两个结果进行聚合，实现了并行梯度下降：</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-2.png" /></p>
<h2 id="概念">概念</h2>
<h3 id="通信-communication">通信 Communication</h3>
<h4 id="内存共享-share-memory">内存共享 Share memory</h4>
<p>当处理器共用一个内存，它们可以访问相同的数据，直接进行通信</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-3.png" /></p>
<h4 id="消息传递-message-passing">消息传递 Message passing</h4>
<p>当存在多个节点的时候，不同节点上的处理器所访问的内存是不同的。这种情况下，需要通过消息传递来进行通信</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-4.png" /></p>
<h3 id="架构-architecture">架构 Architecture</h3>
<h4 id="client-server">Client-Server</h4>
<p>一个节点作为Server来协调其他节点，其他节点作为Worker来执行计算任务</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-5.png" /></p>
<h4 id="peer-to-peer">Peer-to-Peer</h4>
<p>每个节点都是平等的，它们都有邻居，邻居之间可以通信</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-6.png" /></p>
<h3 id="同步性">同步性</h3>
<h4 id="批同步-bulk-synchronous">批同步 Bulk Synchronous</h4>
<p>当全部Worker节点发送梯度信息到Server后，Server才进行一次参数更新</p>
<h4 id="异步-asynchronous">异步 Asynchronous</h4>
<p>当一个Worker将计算出的梯度传回Server时，Server随即进行一次参数更新</p>
<h3 id="并行性-parallelism">并行性 Parallelism</h3>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-7.png" /></p>
<h4 id="数据并行">数据并行</h4>
<p>这是主流的并行方式。将数据划分成若干份，分别在拥有整个模型的Worker上进行梯度计算，不同Worker间梯度计算是并行的</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-8.png" /></p>
<h4 id="模型并行">模型并行</h4>
<p>当模型太大了以致于不能把整个模型载入一个GPU中，可以考虑把整个模型按层或其他方式分解成若干份。</p>
<ul>
<li>比如每个不同的节点计算着整个模型的不同的层，计算着不同的层的梯度。</li>
<li>模型较后部分的计算必须等前面计算完成，因此不同节点间的计算实际是串行的。但每个部分计算互不妨碍，更像是流水线结构</li>
</ul>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-9.png" /></p>
<h3 id="成本与加速比-cost-speedup-ratio">成本与加速比 Cost &amp; Speedup Ratio</h3>
<p><span class="math display">\[加速比 = \frac{使用一个节点消耗的时钟时间}{使用m个节点消耗的时钟时间}\]</span></p>
<p>实际加速比是低于<span class="math inline">\(m\)</span>倍的：存在其他成本，如通信和同步</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-10.png" /></p>
<h4 id="计算成本">计算成本</h4>
<p>节点执行计算任务所消耗的时间，与模型大小、数据量和节点计算性能有关。</p>
<h4 id="通信成本">通信成本</h4>
<p>数据传输所消耗的时间，通常由通信复杂度和延迟所决定 <span class="math display">\[T = \frac{\text{complexity}}{\text{bandwith}} + \text{latency}\]</span></p>
<ul>
<li><p>通信复杂度</p>
<ul>
<li>即Server与Worker传输的数据大小</li>
<li>随模型参数量和Worker节点数量增加而增长</li>
</ul></li>
<li><p>延迟</p>
<ul>
<li>数据传输时可能会产生延迟</li>
<li>取决于计算机网络状况</li>
</ul></li>
</ul>
<h4 id="同步成本">同步成本</h4>
<p>当采用同步的计算方法时（如MapReduce），就可能会产生同步成本，因为不同节点完成相同任务的时间可能是不同的。 Stragger节点</p>
<ul>
<li>一个严重慢于其他节点的Worker，比如因为特殊情况而重启</li>
<li>同步并行计算方法实际时钟时间消耗取决于最慢的节点</li>
</ul>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-11.png" /></p>
<h2 id="主流框架">主流框架</h2>
<table>
<thead>
<tr class="header">
<th></th>
<th><strong>通信</strong></th>
<th><strong>架构</strong></th>
<th><strong>并行性</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MapReduce</td>
<td>Message passing</td>
<td>client-server</td>
<td>synchronous</td>
</tr>
<tr class="even">
<td>Parameter Server</td>
<td>Message passing</td>
<td>client-server</td>
<td>asynchronous</td>
</tr>
<tr class="odd">
<td>Decentralized</td>
<td>Message passing</td>
<td>peer-to-peer</td>
<td>synchronous or asynchronous</td>
</tr>
</tbody>
</table>
<h3 id="mapreduce">MapReduce</h3>
<p>由Google提出</p>
<ul>
<li>本质思想：Server广播参数，Worker执行Map运算并将结果返回Server，Server执行Reduce，从而完成一次参数更新</li>
<li>开源实现：Hadoop、Spark等</li>
</ul>
<h4 id="框架特点">框架特点</h4>
<ul>
<li>架构：client-server</li>
<li>通信：消息传递</li>
<li>并行方式：批量同步</li>
</ul>
<h4 id="计算流程">计算流程</h4>
<ol type="1">
<li><code>Broadcast</code> ：Server将参数发送给每个Worker</li>
<li><code>Map</code> ：每个worker利用自己的数据进行Map操作</li>
<li><code>Reduce</code> ：Worker将Map后的结果发送到Server，Server进行Reduce操作</li>
</ol>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-12.gif" /></p>
<h4 id="梯度下降">梯度下降</h4>
<p><strong>数据并行 Data Parallelism</strong>：在Worker节点之间划分数据（节点具有数据子集） 在本例中，我们假设有<span class="math inline">\(m\)</span>个节点，因此将数据分成<span class="math inline">\(m\)</span>份，每个节点拥有<span class="math inline">\(\frac{1}{m}\)</span>数据</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-13.png" /></p>
<ol type="1">
<li><p><code>Broadcast</code> ：Server广播最新的模型参数<span class="math inline">\(W_t\)</span>到各个Worker节点</p></li>
<li><p><code>Map</code> ：Worker在本地执行计算任务</p>
<ol type="1">
<li>将<span class="math inline">\((X_i, y_i, W_t)\)</span>映射到<span class="math inline">\(g_i = (X^T_i W_t - y_i)X_i\)</span></li>
<li>获得<span class="math inline">\(n\)</span>个梯度向量：<span class="math inline">\(g_i, \cdots, g_n\)</span>（<span class="math inline">\(n\)</span>是指样本个数）</li>
</ol></li>
<li><p><code>Reduce</code> ：计算梯度和<span class="math inline">\(g = \sum^n_{i=1} g_i\)</span></p>
<ol type="1">
<li>Worker在本地将自己计算出的梯度进行求和得到一个向量</li>
<li>Server根据每个Worker计算出的向量再进行一次求和</li>
</ol></li>
<li><p><code>Update</code> ：Server更新参数 <span class="math inline">\(W_{t+1} \leftarrow W_t - \alpha \cdot g\)</span></p></li>
</ol>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-14.png" /></p>
<h4 id="时间消耗">时间消耗</h4>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-15.png" /></p>
<h3 id="parameter-server">Parameter Server</h3>
<p>在<span class="exturl" data-url="aHR0cDovL3dlYi5lZWNzLnVtaWNoLmVkdS9+bW9zaGFyYWYvUmVhZGluZ3MvUGFyYW1ldGVyLVNlcnZlci5wZGY=">Scaling distributed machine learning with the parameter server<i class="fa fa-external-link-alt"></i></span>中被提出，</p>
<ul>
<li>本质思想：根据每个Worker传回的数据立即进行参数更新，即异步</li>
<li>开源实现：Ray</li>
</ul>
<h4 id="框架特点-1">框架特点</h4>
<ul>
<li>架构：client-server</li>
<li>通信：消息传递</li>
<li>并行方式：异步</li>
</ul>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-16.png" /></p>
<h4 id="梯度下降-1">梯度下降</h4>
<p>同样的，数据被划成<span class="math inline">\(m\)</span>个子集分到每个Worker节点</p>
<ul>
<li><p><strong>第<span class="math inline">\(i\)</span>个Worker节点重复：</strong></p>
<ul>
<li>从Server拉取最新参数<span class="math inline">\(W\)</span></li>
<li>利用本地数据和参数<span class="math inline">\(W\)</span>计算梯度<span class="math inline">\(\tilde{g}_i\)</span></li>
<li>将梯度推送到Server</li>
</ul></li>
<li><p><strong>Server执行：</strong></p>
<ul>
<li>从一个Worker收到梯度<span class="math inline">\(\tilde{g}_i\)</span></li>
<li>参数更新<span class="math inline">\(W \leftarrow W - \alpha \cdot \tilde{g}_i\)</span></li>
</ul></li>
</ul>
<h4 id="时间消耗-1">时间消耗</h4>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-17.png" /></p>
<h4 id="性能分析">性能分析</h4>
<ul>
<li><p>优点</p>
<ul>
<li>比同步的MapReduce更快</li>
</ul></li>
<li><p>缺点</p>
<ul>
<li>理论上，收敛速度更慢</li>
<li>如果某些节点比其他节点慢很多，梯度可能受损</li>
</ul></li>
</ul>
<p>图示：Wroker 3 在<span class="math inline">\(t_1\)</span>时刻才完成了一次梯度计算，并返回Server进行更新。而此时Server的参数已经被Worker 1 和 2 的梯度更新了数次，该梯度可能是不利于模型性能的</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-18.png" /></p>
<h3 id="decentralized-network">Decentralized Network</h3>
<h4 id="框架特点-2">框架特点</h4>
<ul>
<li>架构：P2P</li>
<li>通信：消息传递，且节点只与其邻居进行通信</li>
<li>并行方式：异步或同步</li>
</ul>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-19.png" /></p>
<h4 id="梯度下降-2">梯度下降</h4>
<p>第<span class="math inline">\(i\)</span>个节点重复执行</p>
<ul>
<li>使用本地参数<span class="math inline">\(\tilde{W_t}\)</span>来计算梯度<span class="math inline">\(\tilde{g_t}\)</span></li>
<li>拉取邻居节点的梯度，表示为<span class="math inline">\(\{\tilde{W_k}\}\)</span></li>
<li>计算梯度平均值 <span class="math inline">\(\tilde{W} \leftarrow \text{weighted average of } \tilde{W_i} \text{ and } \{\tilde{W_k}\}\)</span></li>
<li>更新参数 <span class="math inline">\(\tilde{W} \leftarrow \tilde{W} - \alpha \cdot \tilde{g}_i\)</span></li>
</ul>
<h4 id="性能分析-1">性能分析</h4>
<p>被证明是可收敛的，但收敛情况取决于网络的连接情况</p>
<ul>
<li>当网络的连接率是较好的时候，模型可以很快收敛</li>
<li>当节点间连接较少时，它可能不收敛</li>
</ul>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-20.png" /></p>
<h2 id="参考">参考</h2>
<blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1nVmNuT2U2X2M2USZhbXA7bGlzdD1QTHZPTzBidGxvUm5zNmVnWHVlaVJqdTREWFFqTlJKUWQ1">Shusen Wang《分布式机器学习》<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzUzODUxMDE0L2Fuc3dlci8xNTg3OTQ3NTI=">李哲龙. 分布式机器学习里的 数据并行 和 模型并行 各是什么意思？ 知乎<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9tZWRpdW0uY29tL0BqZXJ1OTIvZ2VvLWRpc3RyaWJ1dGVkLW1hY2hpbmUtbGVhcm5pbmctYW4tb3ZlcnZpZXctZWUzZmM0MmEwMzE5">Jeru Luke. Geo-Distributed Machine Learning: An Overview. Medium<i class="fa fa-external-link-alt"></i></span><br />
Li and others: Scaling distributed machine learning with the parameter server. In OSDI, 2014.</p>
</blockquote>
]]></content>
      <categories>
        <category>ML - 机器学习</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Latency-aware VNF Chain Deployment with Efficient Resource Reuse at Network</title>
    <url>/2021/01/13/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/paper-nfv-vnf-placement-latency-aware-ne/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>本文研究了网络边缘场景下的VNF放置问题，旨在最小化整体（服务器和链路）资源消耗，提出了一种延迟感知的两阶段方案，即搜索路径的约束深度优先搜索算法（CDFSA） 和部署VNF的基于路径的贪心算法（PGA）。</p>
<h2 id="论文简介">论文简介</h2>
<p><strong>论文名称</strong>：Latency-aware VNF Chain Deployment with Efficient Resource Reuse at Network Edge<br />
<strong>论文作者</strong>：Panpan Jin, Xincai Fei, Qixia Zhang, Qixia Zhang, Fangming Liu, Bo Li<br />
<strong>发表会议</strong>：INFOCOM 2020 (CCF-A)<br />
<strong>研究方向</strong>：NFV 网络功能虚拟化<br />
<strong>关键技术</strong>：邻接矩阵重构, 深度优先搜索, 贪婪算法<br />
<strong>主要创新</strong>：网络边缘场景、延迟保障、两阶段（先边后点）<br />
<span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkxNTUzNDU=">下载论文<i class="fa fa-external-link-alt"></i></span></p>
<a id="more"></a>
<p><strong>缩写词汇表</strong></p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">缩写</th>
<th>描述</th>
<th>全名</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">VNF</td>
<td>虚拟网络功能</td>
<td>Virtual Network Function</td>
</tr>
<tr class="even">
<td style="text-align: center;">SFC</td>
<td>服务功能链</td>
<td>Service Function Chain</td>
</tr>
<tr class="odd">
<td style="text-align: center;">AP</td>
<td>接入点</td>
<td>Access Point</td>
</tr>
</tbody>
</table>
<h2 id="研究背景">研究背景</h2>
<ul>
<li>研究问题：VNF部署问题</li>
<li>研究场景：网络边缘</li>
<li>优化目标：在满足延迟要求的前提下，最小化服务器和链路的资源消耗</li>
<li>研究难点：需同时考虑服务器和链路资源消耗
<ul>
<li>Path 1：虽选择了最短路径，但由于<span class="math inline">\(d_2\)</span>没有VNF2需要即时运行，这会产生额外的计算资源消耗</li>
<li>Path 2：虽避免了新开服务器，但链路要长于Path 1</li>
</ul></li>
</ul>
<p><img data-src="/resource/images/paper/paper-nfv-vnfp-latency-ne-problem.png" /></p>
<h2 id="问题建模">问题建模</h2>
<h3 id="network">Network</h3>
<p>无向图 <span class="math inline">\(\mathbb{G} = (\mathbb{U}, \mathbb{D}, \mathbb{E})\)</span></p>
<ul>
<li><span class="math inline">\(\mathbb{U}\)</span>：Users，表示用户的集合</li>
<li><span class="math inline">\(\mathbb{D}\)</span>：Devices，表示网络边缘中所有服务器硬件设备的集合，如接入点(ccess points)、交换器(switches)和路由器(routers)等
<ul>
<li>对于一个设备<span class="math inline">\(d_i \in \mathbb{D}\)</span>，其计算资源为<span class="math inline">\(C_i\)</span></li>
</ul></li>
<li><span class="math inline">\(\mathbb{E}\)</span>：Edges，表示服务器间物理链路的集合
<ul>
<li>对于边<span class="math inline">\(e(u,v) \in \mathbb{E}\)</span>，表示设备<span class="math inline">\(d_u\)</span>和<span class="math inline">\(d_v\)</span>间链路，其带宽资源为<span class="math inline">\(B(u,v)\)</span></li>
</ul></li>
</ul>
<h3 id="sfc-request">SFC Request</h3>
<p>不同的用户有不同的服务需求，每个SFC由一组由顺序的VNF序列组成</p>
<ul>
<li><span class="math inline">\(\mathbb{N}\)</span>：表示各种类型VNF的集合</li>
<li><span class="math inline">\(\mathbb{S}\)</span>：表示各种SFC的集合</li>
<li><span class="math inline">\(\mathbb{P}\)</span>：表示每个SFC实际部署链路的集合</li>
</ul>
<p>对于用户<span class="math inline">\(u_i \in \mathbb{U}\)</span>其请求的SFC为<span class="math inline">\(S_i\)</span>，</p>
<ul>
<li>其中VNF个数为<span class="math inline">\(|S_i|\)</span>，第<span class="math inline">\(l\)</span>个VNF为<span class="math inline">\(s_i^l \in \mathbb{S_i}\)</span></li>
<li><span class="math inline">\(\Phi_i^{ll&#39;}\)</span>表示<span class="math inline">\(s_i^l\)</span>与<span class="math inline">\(s_i^{l&#39;}\)</span>间的子链</li>
<li><span class="math inline">\(T_{i,n}^{l}\)</span>表示第<span class="math inline">\(l\)</span>个VNF的类型是不是<span class="math inline">\(n \in \mathbb{N}\)</span></li>
<li><span class="math inline">\((\phi_i, t_i, r_i)\)</span>分别表示<span class="math inline">\(S_i\)</span>的source，destiination和flow rate
<ul>
<li><span class="math inline">\(\phi_i\)</span>是一组<span class="math inline">\(S_i\)</span>的AP的集合，它们分布在不同的边缘云上，距离用户<span class="math inline">\(u_i\)</span>非常近</li>
<li>每个SFC只能选择一个<span class="math inline">\(\phi_i\)</span>种AP来进行网络访问，如第<span class="math inline">\(k\)</span>个AP <span class="math inline">\(d_{i,k} \in \phi_i\)</span></li>
<li>每一个AP <span class="math inline">\(d_{i,k}\)</span>实质仍对应一台服务器<span class="math inline">\(d_j \in \mathbb{D}\)</span>，其处理能力表示为<span class="math inline">\(\alpha_j\)</span></li>
<li>对于<span class="math inline">\(r_i\)</span>，假设流速始终不变，且为单流</li>
</ul></li>
<li><span class="math inline">\(p_i \in \mathbb{P}\)</span>是该SFC被放置的物理链路的集合
<ul>
<li>如<span class="math inline">\(\{e(d_1, d_2), e(d_2, d_3), e(d_3, d_4)\}\)</span></li>
</ul></li>
</ul>
<h3 id="问题约束">问题约束</h3>
<h4 id="计算资源约束-resource-capacity">计算资源约束 Resource Capacity</h4>
<ul>
<li>一台服务器上的VNF可以同时被不同的SFC使用，使得它的剩余处理能力得到利用</li>
<li>对于服务器<span class="math inline">\(d_j \in \mathbb{D}\)</span>，其运行的VNF种类的数量为<span class="math inline">\(Y_{i,n}\)</span></li>
<li>其中，对于类型为<span class="math inline">\(n\)</span>的VNF，其所消耗的计算资源为<span class="math inline">\(R_n\)</span></li>
<li>当服务链所需VNF类型已经运行在<span class="math inline">\(d_j\)</span>上时，不需新开VNF，即<span class="math inline">\(Y_{i,n}\)</span>不变；否则，新开一个VNF实例且<span class="math inline">\(Y_{i,n} = Y_{i,n} + 1\)</span></li>
</ul>
<p>每台服务器<span class="math inline">\(d_j\)</span>运行实例所需计算资源不超过其最大计算资源<span class="math inline">\(C_j\)</span>：</p>
<p><span class="math display">\[\sum_{n \in \mathbb{N}} Y_{j, n} R_{n} \leq C_{j}, \quad \forall d_{j} \in \mathbb{D}\]</span></p>
<h4 id="链路资源约束-bandwidth-capacity">链路资源约束 Bandwidth Capacity</h4>
<p>同样，每条链路<span class="math inline">\(e(u,v)\)</span>所负载的带宽资源不超过其最大容量<span class="math inline">\(B(u,v)\)</span>：</p>
<p><span class="math display">\[
\sum_{S_{i} \in \mathbb{S}} \sum_{\Phi_{i}^{l l^{\prime} \subseteq S_{i}}} Z_{i(u, v)}^{l l^{\prime}} \cdot r_{i} \leq B(u, v),
\forall e(u, v) \in \mathbb{E}, 1 \leq l&lt;l^{\prime} \leq\left|S_{i}\right|\]</span></p>
<ul>
<li><span class="math inline">\(Z_{i(e,u)}^{ll&#39;}\)</span>：表示服务链<span class="math inline">\(S_i\)</span>的子链<span class="math inline">\(\Phi_i^{ll&#39;}\)</span>是否经过链路<span class="math inline">\(e(u,v) \in \mathbb{E}\)</span></li>
</ul>
<h4 id="处理能力约束-processing-capacity">处理能力约束 Processing Capacity</h4>
<p>每台服务器<span class="math inline">\(d_j\)</span>对所有VNF的实际处理能力不超过其最大能力：</p>
<p><span class="math display">\[\sum_{S_{i} \in \mathbb{S}} \sum_{s_{i}^{l} \in S_{i}} T_{i, n}^{l} \cdot X_{i, j}^{l} \cdot r_{i} \leq Y_{j, n} \cdot W_{n}, \forall n \in \mathbb{N}, \forall d_{j} \in \mathbb{D}\]</span></p>
<ul>
<li><span class="math inline">\(W_n\)</span>：对type-n VNF的处理能力</li>
<li><span class="math inline">\(X_{i,j}^l\)</span>：表明SFC <span class="math inline">\(S_i\)</span> 的第<span class="math inline">\(l\)</span>个<span class="math inline">\(VNF\)</span>是否运行在服务器<span class="math inline">\(d_j\)</span>上
<ul>
<li>特别地，此处假设SFC为单流，即SFC中的一个VNF只能运行在一个服务器上</li>
<li><span class="math inline">\(\sum_{d_{j} \in \mathbb{D}} X_{i, j}^{l}=1, \quad \forall S_{i} \in \mathbb{S}, \forall l \in\left[1,\left|S_{i}\right|\right]\)</span></li>
</ul></li>
</ul>
<h4 id="延迟约束-latency-limitation">延迟约束 Latency Limitation</h4>
<p>本文考虑了两种延迟：</p>
<ul>
<li><strong>排队延迟</strong>（queueing delay）
<ul>
<li><span class="math inline">\(q_j\)</span>：SFC在AP <span class="math inline">\(d_j\)</span> 处排队产生的延迟</li>
<li>对于每个AP，将其建模为一个<span class="math inline">\(M/M/1\)</span>队列，则其满足Little law</li>
<li><span class="math inline">\(q_{j}=\frac{1}{\alpha_{j}-\sum_{S_{i} \in \mathbb{S}} A_{i, j} \cdot r_{i}}\)</span>
<ul>
<li>式中，<span class="math inline">\(A_{i,j}\)</span>表示<span class="math inline">\(S_i\)</span>是否从<span class="math inline">\(d_j\)</span>接入网络</li>
<li>每个SFC仅有一个AP，故有<span class="math inline">\(\sum_{d_{j} \in \mathbb{D}} A_{i, j}=1, \quad \forall S_{i} \in \mathbb{S}\)</span></li>
</ul></li>
</ul></li>
<li><strong>传播延迟</strong>（propagation delay &amp; transmission delay）
<ul>
<li><span class="math inline">\(l_i(u,v)\)</span>：SFC <span class="math inline">\(S_i\)</span>在链路<span class="math inline">\(e(u,v)\)</span>上进行传输产生的延迟</li>
</ul></li>
</ul>
<p>故总延迟为：</p>
<p><span class="math display">\[\begin{aligned}
\sum_{d_{j} \in \mathbb{D}} A_{i, j} \cdot q_{j}+\sum_{\Phi_{i}^{l \prime} \subseteq S_{i}} Z_{i(u, v)}^{l l^{\prime}} \cdot l_{i(u, v)} \leq \beta_{i} \\
\forall e(u, v) \in \mathbb{E}, \forall S_{i} \in \mathbb{S}, 1 \leq l&lt;l^{\prime} \leq\left|S_{i}\right|, j=u \text { if } l=1
\end{aligned}\]</span></p>
<h4 id="运行约束-running-vnf">运行约束 Running VNF</h4>
<p>对于<span class="math inline">\(S_i\)</span>，其所有VNF都应当被运行</p>
<p><span class="math display">\[\sum_{s_{i}^{l} \in S_{i}} \sum_{d_{j} \in \mathbb{D}} X_{i, j}^{l}=\left|S_{i}\right| \quad \forall S_{i} \in \mathbb{S}\]</span></p>
<h3 id="优化目标">优化目标</h3>
<p>同时最小化服务器和链路的资源消耗</p>
<p><span class="math display">\[\begin{array}{cc}
\min \sum_{d_{j} \in \mathbb{D}} \sum_{n \in \mathbb{N}} Y_{j, n} \cdot R_{n}+\sum_{e(u, v) \in \mathbb{E}} \sum_{S_{i} \in \mathbb{S}} \sum_{\Phi_{i}^{l l^{\prime}} \subseteq S_{i}} Z_{i(u, v)}^{l l^{\prime}} \cdot r_{i} \\
\text { s.t. } (1),(2),(3),(4),(5),(6),(7),(8)
\end{array}\]</span></p>
<h2 id="算法模型">算法模型</h2>
<h3 id="overview">Overview</h3>
<h4 id="问题难点">问题难点</h4>
<ul>
<li>该问题是NP-hard问题</li>
<li>无法预测未来的SFC，只能最小化当前SFC的资源消耗</li>
</ul>
<h4 id="解决方案">解决方案</h4>
<p>两阶段：选择路径 -&gt; 部署VNF</p>
<p>对于<span class="math inline">\(S_i = (\phi_i, t_i, r_i)\)</span>，</p>
<ol type="1">
<li><strong>选择路径</strong>：寻找从<span class="math inline">\(\phi_i \rightarrow d_i\)</span>的满足约束的所有预规划路径<span class="math inline">\(P_i^{\Lambda}\)</span></li>
<li><strong>部署VNF</strong>：对于每个路径<span class="math inline">\(p \in P_i^{\Lambda}\)</span>，尽可能多的复用已运行的VNF来进行实际部署</li>
<li><strong>最优方案</strong>：选择其中资源消耗最少的方案</li>
</ol>
<h3 id="选择路径">选择路径</h3>
<p>路径需满足</p>
<ul>
<li>从<span class="math inline">\(\phi_i\)</span>开始到<span class="math inline">\(t_i\)</span>结束</li>
<li>实际延迟不超过延迟限制</li>
<li>每条边都满足可用带宽约束</li>
</ul>
<h4 id="构建矩阵">构建矩阵</h4>
<p>首先，构造无向加权图（见(a)）<span class="math inline">\(\mathbb{G}^{\prime}=\left(\mathbb{D}, \mathbb{E} ; w_{1}, w_{2}\right)\)</span>，<span class="math inline">\(w_1\)</span>和<span class="math inline">\(w_2\)</span>分别表示该边的延迟和剩余带宽</p>
<h4 id="重构矩阵">重构矩阵</h4>
<p>为了找到网络中所有对服务链<span class="math inline">\(S_i = \{s_i, s_2, \cdots, s_{|S_i|}\}\)</span>可用的服务器和链路，本文构建了一个基于<span class="math inline">\(\mathbb{G}^{\prime}\)</span>加权混合多图<span class="math inline">\(\mathbb{H}^{\prime}=\left(\mathbb{D}&#39;, \mathbb{E}&#39; ; w\right)\)</span>（见(b)），<span class="math inline">\(w\)</span>是指每条边的延迟。重构步骤如下：</p>
<h5 id="考虑服务器约束">考虑服务器约束</h5>
<p>首先将<span class="math inline">\(S_i\)</span>的所有AP <span class="math inline">\(d_j \in \phi_i\)</span>加入到<span class="math inline">\(\mathbb{H}\)</span>，其它的服务器需满足以下条件之一才会被加入<span class="math inline">\(\mathbb{H}\)</span>：</p>
<ol type="1">
<li>其剩余计算资源大于<span class="math inline">\(S_i\)</span>所需的最小计算资源<span class="math inline">\(R_{min}^{i}\)</span>（整条SFC中计算资源需求最小VNF的值）</li>
<li>它正在运行着的VNF中有SFC所需的VNF类型</li>
</ol>
<h5 id="考虑链路约束">考虑链路约束</h5>
<ul>
<li>一个服务器可能会被部署两次，部署路径可能会出现环</li>
<li>为了达到优化目标，每一条边仅可被通过2次，且需不同方向</li>
</ul>
<p>每条链路<span class="math inline">\(e(u,v)\)</span>的最大通过次数有三种情况</p>
<ul>
<li>2次：当<span class="math inline">\(\left\lfloor\frac{\left.B_{(} u, v\right)}{r_{i}}\right\rfloor&gt;1\)</span>即剩余带宽资源大于需求时，加入权值为延迟<span class="math inline">\(l(e,v)\)</span>的有向边<span class="math inline">\((u,v)\)</span>和<span class="math inline">\((v,u)\)</span>到<span class="math inline">\(\mathbb{H}\)</span></li>
<li>1次：当<span class="math inline">\(\left\lfloor\frac{\left.B_{(} u, v\right)}{r_{i}}\right\rfloor=1\)</span>即剩余带宽资源等于需求时，加入权值为延迟<span class="math inline">\(l(e,v)\)</span>的无向边<span class="math inline">\((u,v)\)</span>到<span class="math inline">\(\mathbb{H}\)</span></li>
<li>0次：当<span class="math inline">\(\left\lfloor\frac{\left.B_{(} u, v\right)}{r_{i}}\right\rfloor&lt;1\)</span>即剩余带宽资源不足时，不加入<span class="math inline">\(\mathbb{H}\)</span></li>
</ul>
<h5 id="服务器通路验证">服务器通路验证</h5>
<p>如果没有边与某服务器相连，就从<span class="math inline">\(\mathbb{H}\)</span>中移除它。</p>
<p><img data-src="/resource/images/paper/paper-nfv-vnfp-latency-ne-algo-reconstruct-graph.png" /></p>
<h4 id="cdfsa">CDFSA</h4>
<p>Constrained Depth First Search Algorithm</p>
<p><strong>链路邻接矩阵 <span class="math inline">\(M\)</span></strong>：存储<span class="math inline">\(\mathbb{H}\)</span>中的信息</p>
<p><span class="math display">\[M[u][v]=M[v][u]=\left\{\begin{array}{l}
0, \text { if there is no link, } \\
1, \text { if there is an undirected link } \\
2, \text { if there are two directed links. }
\end{array}\right.\]</span></p>
<p><strong>链路延迟矩阵 <span class="math inline">\(E\)</span></strong>：其每个元素<span class="math inline">\(E[u][v]\)</span>表示链路<span class="math inline">\((u,v)\)</span>的延迟为<span class="math inline">\(l(u,v)\)</span></p>
<p><strong>算法流程</strong></p>
<ul>
<li>初始化：邻接矩阵M和延迟矩阵E</li>
<li>对于每一个AP <span class="math inline">\(d_j \in \phi_i\)</span>，
<ul>
<li>当前节点 <span class="math inline">\(u=d_j\)</span></li>
<li>总延迟 <span class="math inline">\(L_t\)</span> += <span class="math inline">\(q_j\)</span> 排队延迟</li>
<li>寻找从当前节点<span class="math inline">\(u\)</span>到<span class="math inline">\(t_i\)</span>的路径
<ul>
<li>记录节点选择顺序（加入栈<span class="math inline">\(c_p\)</span>中）</li>
<li>遍历图<span class="math inline">\(\mathbb{H}\)</span>中的每一个节点<span class="math inline">\(v\)</span>：
<ul>
<li>若边<span class="math inline">\(e(u,v)\)</span>是可通过的 (<span class="math inline">\(M[u][v]&gt;0\)</span>) 且满足延迟约束 (<span class="math inline">\(L_t+E[u][v] &lt; \beta_i\)</span>)
<ul>
<li>如果该边是有向边(<span class="math inline">\(M[u][v]=2\)</span>)，则将边<span class="math inline">\(e(u,v)\)</span>不再是可通过的边(<span class="math inline">\(M[u][v]=0\)</span>)</li>
<li>如果该边是无向边(<span class="math inline">\(M[u][v]=1\)</span>)，则将该边<span class="math inline">\(e(u,v)\)</span>和<span class="math inline">\(e(v,u)\)</span>都不再可通(<span class="math inline">\(M[u][v]=0, M[v][u]=0\)</span>)</li>
</ul></li>
<li>更新当前节点 <span class="math inline">\(u=v\)</span></li>
</ul></li>
<li>更新总延迟</li>
</ul></li>
<li>递归调用，直到<span class="math inline">\(u=t_i\)</span>，返回<span class="math inline">\(c_p\)</span>并加入<span class="math inline">\(P_t^{\Lambda}\)</span> <img data-src="/resource/images/paper/paper-nfv-vnfp-latency-ne-algo-cdfsa.png" /></li>
</ul></li>
</ul>
<h3 id="部署vnf">部署VNF</h3>
<p>部署时应满足</p>
<ul>
<li>VNF需顺序部署</li>
<li>被部署的服务器满足计算资源约束</li>
<li>尽可能重用已运行的VNF</li>
</ul>
<h4 id="构建矩阵-1">构建矩阵</h4>
<p>对于一个候选路径<span class="math inline">\(p \in P_t^{\Lambda}\)</span>，</p>
<ul>
<li>其中边个数为<span class="math inline">\(k\)</span>，则服务器个数为<span class="math inline">\(k+1\)</span></li>
<li>其服务器组成为<span class="math inline">\(p= \{sp_0,sp_1,sp_2,\cdots,sp_k\}\)</span></li>
</ul>
<p><strong><span class="math inline">\(S_i\)</span>-<span class="math inline">\(p\)</span>关系矩阵 <span class="math inline">\(A\)</span></strong></p>
<ul>
<li>矩阵形状为<span class="math inline">\(|S_i| \times (k+1)\)</span></li>
<li><span class="math inline">\(A[l][j]\)</span>表示VNF <span class="math inline">\(s_l \in S_i\)</span>能否重用服务器<span class="math inline">\(sp_j\)</span>，需满足两个约束：
<ul>
<li>该服务器已运行着此类VNF</li>
<li>该服务器对此类VNF的剩余处理能力满足约束</li>
<li><span class="math inline">\(A[l][j]=\left\{\begin{array}{ll} 1 &amp; \text { if } W_{\left(d_{j}\right),\left(n_{l}\right)}-r_{i} \geq 0 \\ 0 &amp; \text { otherwise. } \end{array}\right.\)</span></li>
</ul></li>
</ul>
<p>示例如下</p>
<p><span class="math display">\[\left.A= ( \begin{array}{cccccccc}
&amp; d_{0} &amp; d_{1} &amp; d_{2} &amp; d_{3} &amp; d_{2} &amp; d_{4} &amp; d_{4} \\
s_{0} &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\
s_{1} &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\
s_{2} &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\
s_{3} &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0
\end{array}\right)\]</span></p>
<h4 id="pga">PGA</h4>
<p>A Path-based Greedy Algorithm</p>
<p>每个VNF有两种服务器可部署</p>
<ul>
<li>type-R：可复用的，已运行该类VNF的服务器</li>
<li>type-V：可新开的，有足够资源新开VNF的服务器</li>
</ul>
<p>对于每个被部署的VNF <span class="math inline">\(s_l\)</span> 有四种情况</p>
<ul>
<li>情况1：先找到type-R，即最优方案</li>
<li>情况2：先找到type-V再找到type-R</li>
<li>情况3：仅找的type-V</li>
<li>情况4：两种都没找到，部署失败，拒绝该SFC</li>
</ul>
<p>可以发现，对于每个VNF部署，最多只需尝试2种可选服务器</p>
<p><img data-src="/resource/images/paper/paper-nfv-vnfp-latency-ne-algo-pga.png" /></p>
<h4 id="最优方案">最优方案</h4>
<p>对于每个候选路径<span class="math inline">\(p \in P_i^{\Lambda}\)</span>，利用上述算法可以得到了其局部最优方案。之后，在这些候选路径中，选择资源消耗最少的方案，即为最优方案。</p>
<h2 id="实验评估">实验评估</h2>
<h3 id="设计实验">设计实验</h3>
<h4 id="底层网络">底层网络</h4>
<h5 id="网络拓扑">网络拓扑</h5>
<p>用不同大小的网络拓扑模拟不同的网络规模</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">名称</th>
<th>节点数</th>
<th>边数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Bellsouth</td>
<td>51</td>
<td>66</td>
</tr>
<tr class="even">
<td style="text-align: center;">Cogentco</td>
<td>197</td>
<td>245</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Kdl</td>
<td>754</td>
<td>899</td>
</tr>
</tbody>
</table>
<h5 id="服务器节点">服务器节点</h5>
<p>为了模拟运行中的网络状态，每个服务器</p>
<ul>
<li>当前剩余可用资源为<span class="math inline">\([0,200]\)</span>单元</li>
<li>预先随机放置<span class="math inline">\([0,8]\)</span>种类型的VNF（共20种）</li>
</ul>
<h5 id="物理链路">物理链路</h5>
<p>每条链路的带宽资源为<span class="math inline">\([0,1000]\)</span> Mbps</p>
<h4 id="服务功能链">服务功能链</h4>
<p>SFC依次到达系统，为每个SFC从底层网络中随机挑选一组源节点（APs）和目标节点</p>
<ul>
<li>每个AP处理能力为<span class="math inline">\([100,200]\)</span></li>
<li>每个VNF所需计算资源<span class="math inline">\([20,50]\)</span>节点</li>
<li>流速为<span class="math inline">\([30，60]\)</span> Mbps</li>
<li>生存周期为<span class="math inline">\([30,80]\)</span> ms</li>
</ul>
<p><strong>每一次实验重复20次来消除偶然性</strong></p>
<h3 id="对比算法">对比算法</h3>
<ul>
<li>Shortest Path Heuristic + First Fit Assignment (SHP+FF)<br />
</li>
<li>Constrained Depth-First Search Algorithm + First Fit Assignment (CDFSA + FF)<br />
</li>
<li>Shortest Path Heuristic + Path-based Greedy Algorithm (SHP+PGA)</li>
</ul>
<h3 id="评估指标">评估指标</h3>
<h4 id="运行时间">运行时间</h4>
<p><img data-src="/resource/images/paper/paper-nfv-vnfp-latency-ne-experiment-execution-time.png" /></p>
<h4 id="资源消耗">资源消耗</h4>
<h5 id="sfc长度流速影响">SFC长度+流速影响</h5>
<p><img data-src="/resource/images/paper/paper-nfv-vnfp-latency-ne-experiment-sfc-length-and-flow-rate.png" /></p>
<h5 id="流速">流速</h5>
<p><img data-src="/resource/images/paper/paper-nfv-vnfp-latency-ne-experiment-flow-rate.png" /></p>
<h5 id="sfc长度">SFC长度</h5>
<p><img data-src="/resource/images/paper/paper-nfv-vnfp-latency-ne-experiment-sfc-length.png" /></p>
<h4 id="平均延迟">平均延迟</h4>
<p><img data-src="/resource/images/paper/paper-nfv-vnfp-latency-ne-experiment-average-latency.png" /></p>
<h2 id="总结思考">总结思考</h2>
<ul>
<li>场景新颖：在网络边缘场景，且考虑了延迟</li>
<li>矩阵重构：提出了混合图来对链路访问进行限制</li>
<li>先边后点：两阶段解决方案，先选路径后进行VNF部署</li>
</ul>
]]></content>
      <categories>
        <category>RP - 科研论文</category>
      </categories>
      <tags>
        <tag>Paper</tag>
        <tag>NFV</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文汇总】Papers on NFV-RA</title>
    <url>/2020/12/02/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/paper-list-nfv-ra/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Lastest update: JUN. 12, 2021.<br />
Welcome to ⭐ this work on <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0dlbWluaUxpZ2h0L25mdi1wYXBlcnM=">GitHub<i class="fa fa-external-link-alt"></i></span> to get updates in time.</p>
<p>This is a paper list about Resource Allocation in <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTmV0d29ya19mdW5jdGlvbl92aXJ0dWFsaXphdGlvbg==">Network Functions Virtualization<i class="fa fa-external-link-alt"></i></span> (NFV) and <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvU29mdHdhcmUtZGVmaW5lZF9uZXR3b3JraW5n">Software-Defined Networking<i class="fa fa-external-link-alt"></i></span> (SDN) including</p>
<ul>
<li>Comprehensive <strong>Surveys</strong></li>
<li><strong>VNE</strong>: Virtual Network Embedding Algorithms</li>
<li><strong>VNFC</strong>: Virtual Network Functions Chaining Algorithms</li>
<li><strong>VNFP</strong>: Virtual Network Functions Placement Algorithms</li>
<li><strong>VNFF</strong>: Virtual Network Functions Migration Algorithms</li>
<li><strong>VNFS</strong>: Virtual Network Functions Scheduling Algorithms</li>
<li><strong>Multi-domain</strong>: also known as cross-domain, multi-region or other resemble name.</li>
</ul>
<p>Particularly, we mainly collect papers from high-quality journals and conferences, and <strong>classify them according to method categories</strong>.</p>
<p><strong>Favorably receive that submit relevant papers to this repository in the appropriate format.</strong></p>
<a id="more"></a>
<p><strong>Search by Keywords</strong></p>
<p>You can search the relevant papers by following keywords:</p>
<ul>
<li><strong>Direction</strong>: <code>VNE</code>, <code>VNFC</code>, <code>VNFP</code>, <code>VNFF</code>, <code>VNFS</code>, <code>Multi-domain</code></li>
<li><strong>Publication</strong>: <code>JSAC</code>, <code>TON</code>, <code>INFOCOM</code>, <code>CN</code>, ...</li>
<li><strong>PUB-rank</strong>: <code>CCF-A</code>, <code>CCF-B</code>, <code>JCR-Q1</code>, ...</li>
<li><strong>Awareness</strong>: <code>Latency</code>, <code>Reliability</code>, <code>Congestion</code>, <code>Privacy</code></li>
<li><strong>RL-ALGO</strong>: <code>DQN</code>, <code>DDPG</code>, <code>A3C</code>, ...</li>
<li><strong>NN-type</strong>: <code>CNN</code>, <code>RNN</code>, <code>GNN</code>, ...</li>
</ul>
<h2 id="content"><a href="#content">Content</a></h2>
<ul>
<li><a href="#survey-papers">Survey</a></li>
<li><a href="#mathematical-based">Mathematical-based</a></li>
<li><a href="#heuristic-based">Heuristic-based</a>
<ul>
<li><a href="#basic-heuristics">Basic heuristics</a></li>
<li><a href="#meta-heuristics">Meta-heuristics</a></li>
</ul></li>
<li><a href="#reinforcement-learning-based">Reinforcement Learning-based</a>
<ul>
<li><a href="#basic-rl">Basic RL</a></li>
<li><a href="#deep-rl">Deep RL</a></li>
</ul></li>
<li><a href="#unassorted">Unassorted</a></li>
</ul>
<!-- Template :star:

1. **An Online Algorithm for VNF Service Chain Scaling in Datacenters**
   - `Publication`: TON 2020 (**CCF-A**)
   - `Authors`: Ziyue Luo, Chuan Wu
   - `Keyworks`: VNFS, VNFP, ILP (Integer Linear Program), Regularization, Rounding
   - `Objective`: Minimize the operating cost and deployment cost
   - `Link`: [paper](https://i.cs.hku.hk/~cwu/papers/zyluo-ton19.pdf)

-->
<!-- more -->
<h2 id="survey-papers"><a href="#content">Survey papers</a></h2>
<ol type="1">
<li><p><strong>Recent Advances of Resource Allocation in Network Function Virtualization</strong></p>
<ul>
<li><code>Publication</code>: TPDS 2021 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Song Yang, Fan Li, Stojan Trajanovski, Ramin Yahyapour, Xiaoming Fu</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkxNjk4NTc=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>SDN/NFV-Empowered Future IoV With Enhanced Communication, Computing, and Caching</strong></p>
<ul>
<li><code>Publication</code>: Proc. IEEE 2020 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Weihua Zhuang; Qiang Ye; Feng Lyu; Nan Cheng; Ju Ren</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg5MDc4NTE=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Survey of Performance Acceleration Techniques for Network Function Virtualization</strong></p>
<ul>
<li><code>Publication</code>: Proc. IEEE 2019 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Leonardo Linguaglossa; Stanislav Lange; Salvatore Pontarelli; Gábor Rétvári; Dario Rossi; Thomas Zinner; Roberto Bifulco; Michael; Jarschel; Giuseppe Bianchi</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg2NjY3NTE=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Will Serverless Computing Revolutionize NFV?</strong></p>
<ul>
<li><code>Publication</code>: Proc. IEEE 2019 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Paarijaat Aditya; Istemi Ekin Akkus; Andre Beck; Ruichuan Chen; Volker Hilt; Ivica Rimac; Klaus Satzke; Manuel Stein</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg2NTMzNzk=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>A Survey on the Placement of Virtual Resources and Virtual Network Functions</strong></p>
<ul>
<li><code>Publication</code>: IEEE Communications Surveys &amp; Tutorials 2019 (<strong>JCR-Q1</strong>)</li>
<li><code>Authors</code>: Abdelquoddouss Laghrissi and Tarik Taleb</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cDovL21vc2FpYy1sYWIub3JnL3VwbG9hZHMvcGFwZXJzLzA3OGI2MDFlLTNlMDEtNGE0Mi04YTM1LWI5OGUyZDExMzk0My5wZGY=">paper<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Resource Allocation in NFV: A Comprehensive Survey</strong></p>
<ul>
<li><code>Publication</code>: TNSM 2019 (<strong>CCF-C</strong>)</li>
<li><code>Authors</code>: Juliver Gil Herrera, Juan Felipe Botero</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzc1MzQ3NDE=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>A comprehensive survey of network function virtualization</strong></p>
<ul>
<li><code>Publication</code>: CN 2018 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Bo Yi, Xingwei Wang, Keqin Li, Sajal k. Das , Min Huang</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzEzODkxMjg2MTgzMDAzMDY=">ScienceDirect<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
</ol>
<h2 id="mathematical-based"><a href="#content">Mathematical-based</a></h2>
<h2 id="heuristic-based"><a href="#content">Heuristic-based</a></h2>
<h3 id="basic-heuristic"><a href="#content">Basic Heuristic</a></h3>
<ol type="1">
<li><p><strong>Energy and Cost Efficient Resource Allocation for Blockchain-Enabled NFV</strong></p>
<ul>
<li><code>Publication</code>: TSC 2021 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Shiva Kazemi Taskou, Mehdi Rasti, Pedro H. J. Nardelli</li>
<li><code>Keyworks</code>: VNFP, Blockchain-Enabled, HuRA (Hungarian-based Resource Allocation), HuRA (Hungarian-based Resource Allocation)</li>
<li><code>Objective</code>: Minimize the energy consumption and utilized resource cost simultaneously</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDMuMDIxMzk=">paper<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Towards Latency Optimization in Hybrid Service Function Chain Composition and Embedding</strong></p>
<ul>
<li><code>Publication</code>: INFOCOM 2020 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Panpan Jin; Xincai Fei; Qixia Zhang; Fangming Liu; Bo Li</li>
<li><code>Keyworks</code>: VNFC &amp; VNFP, HSFCE (Hybrid SFC composition and Embedding), Latency-aware, Betweenness Centrality</li>
<li><code>Objective</code>: Minimize the latency for the constructed hybrid SFP</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkxNTU1Mjk=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Latency-aware VNF Chain Deployment with Efficient Resource Reuse at Network Edge</strong></p>
<ul>
<li><code>Publication</code>: INFOCOM 2020 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Panpan Jin; Xincai Fei; Qixia Zhang; Fangming Liu; Bo Li</li>
<li><code>Keyworks</code>: VNFP, MILP (Mixed Integer Iinear Programming), Latency-aware, CDFSA (constrained depth-first search algorithm)</li>
<li><code>Objective</code>: Minimize the resource consumption of both servers and links with latency guarantees</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9mYW5nbWluZ2xpdS5naXRodWIuaW8vZmlsZXMvSU5GT0NPTTIwLUVkZ2UtTkZWLnBkZg==">paper<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>An Online Algorithm for VNF Service Chain Scaling in Datacenters</strong></p>
<ul>
<li><code>Publication</code>: TON 2020 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Ziyue Luo, Chuan Wu</li>
<li><code>Keyworks</code>: VNFP, ILP (Integer Linear Program), Regularization, Rounding</li>
<li><code>Objective</code>: Minimize the operating cost and deployment cost</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pLmNzLmhrdS5oay9+Y3d1L3BhcGVycy96eWx1by10b24xOS5wZGY=">paper<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Reliability-Aware Virtualized Network Function Services Provisioning in Mobile Edge Computing</strong></p>
<ul>
<li><code>Publication</code>: TON 2020 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Meitian Huang, Weifa Liang, Xiaojun Shen, Yu Ma, Haibin Kan</li>
<li><code>Keyworks</code>: VNFP, Reliability-aware, approximation algorithms, DP (dynamic programming), MEC (mobile edge computing)</li>
<li><code>Objective</code>: Maximize the network throughput</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg3NTg4NDY=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Congestion-Aware and Energy-Aware Virtual Network Embedding</strong></p>
<ul>
<li><code>Publication</code>: TON 2020 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Minh Pham, Doan B. Hoang, Zenon Chaczko</li>
<li><code>Keyworks</code>: VNE, relaxed LP (linear Program), Congestion-aware, Energy-aware, SDN (Software-Defined Networks), SR (Segment Routing)</li>
<li><code>Objective</code>: Multiple-objective is to save cost, save energy and avoid network congestion simultaneously</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg5NDUxNjI=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Sova: A Software-Defined Autonomic Framework for Virtual Network Allocations</strong></p>
<ul>
<li><code>Publication</code>: TPDS 2020 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Zhiyong Ye, Yang Wang, Shuibing He, Chengzhong Xu, Xian-He Sun</li>
<li><code>Keyworks</code>: VNFP, VNFM, SDN</li>
<li><code>Objective</code>: Optimize the network allocation between different services by coordinating virtual dynamic SR-IOV and virtual machine live migration in autonomic way</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkxNTEzNTg=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Optimal Virtual Network Function Deployment for 5G Network Slicing in a Hybrid Cloud Infrastructure</strong></p>
<ul>
<li><code>Publication</code>: TWC 2020 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Antonio De Domenico, Ya-Feng Liu, Wei Yu</li>
<li><code>Keyworks</code>: VNFP, ILP (Integer Linear Programming), Network Slicing</li>
<li><code>Objective</code>: Lead to high resource utilization efficiency and large gains in terms of the number of supported VNF chains</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkxNzcyODg=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Cost-Efficient VNF Placement and Scheduling in Public Cloud Networks</strong></p>
<ul>
<li><code>Publication</code>: TCOM 2020 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Tao Gao, Xin Li, Yu Wu , Weixia Zou, Shanguo Huang, Massimo Tornatore, Biswanath Mukherjee</li>
<li><code>Keyworks</code>: VNFP, VNFS, Cost Efficiency, Public Cloud</li>
<li><code>Objective</code>: /</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkwODY2MTY=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Virtual Network Embedding With Guaranteed Connectivity Under Multiple Substrate Link Failures</strong></p>
<ul>
<li><code>Publication</code>: TCOM 2020 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Zhiyong Ye, Yang Wang, Shuibing He, Chengzhong Xu, Xian-He Sun</li>
<li><code>Keyworks</code>: VNE, Connectivity, Fault Tolerance, Redundancy</li>
<li><code>Objective</code>: /</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg5MDYxNzk=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Reliability Aware Service Placement Using a Viterbi-Based Algorithm</strong></p>
<ul>
<li><code>Publication</code>: TNSM 2020 (<strong>CCF-C</strong>)</li>
<li><code>Authors</code>: Mohammad Karimzadeh-Farshbafan, Vahid Shah-Mansouri, Dusit Niyato</li>
<li><code>Keyworks</code>: VNFP, MICP (mixed integer convex programming), Viterbi-based</li>
<li><code>Objective</code>: Minimize the cost of resources of the InPs and maximizing the reliability of the service</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg5MzMxMTE=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Provably Efficient Algorithms for Placement of Service Function Chains with Ordering Constraints</strong></p>
<ul>
<li><code>Publication</code>: INFOCOM 2018 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Ziyue Luo, Chuan Wu</li>
<li><code>Keyworks</code>: VNFP, Equivalence with Hitting Set, Naive and Faster Greedy, LP-Rounding, DP (Dynamic Programming)</li>
<li><code>Objective</code>: Minimize the total deployment cost</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9oYWwuaW5yaWEuZnIvaGFsLTAxNzQzMjczL2RvY3VtZW50">paper<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Toward Profit-Seeking Virtual Network Embedding</strong></p>
<ul>
<li><code>Publication</code>: INFOCOM 2014 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Long Gong, Yonggang Wen, Zuqing Zhu and Tony Lee</li>
<li><code>Keyworks</code>: VNE, GRC (Global Resource Control)</li>
<li><code>Objective</code>: Maximize the revenue-to-cost ratio and acceptance ratio</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzY4NDc5MTg=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
</ol>
<h3 id="meta-heuristic"><a href="#content">Meta-Heuristic</a></h3>
<ol type="1">
<li><p><strong>A Constructive Particle Swarm Optimizer for Virtual Network Embedding</strong></p>
<ul>
<li><code>Publication</code>: TNSE 2020 (<strong>JCR-Q1</strong>)</li>
<li><code>Authors</code>: Yongqiang Gao; Haibing Guan; Zhengwei Qi; Yang Hou; Liang Liu</li>
<li><code>Keyworks</code>: VNE, CPSO (Constructive Particle Swarm Optimizer)</li>
<li><code>Objective</code>: MinimiziE the cost of bandwidth for embedding the VN</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg3ODYxNjY=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>A Multi-objective Ant Colony System algorithm for Virtual Machine Placement in Cloud Computing</strong></p>
<ul>
<li><code>Publication</code>: JCSS 2013 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Panpan Jin; Xincai Fei; Qixia Zhang; Fangming Liu; Bo Li</li>
<li><code>Keyworks</code>: VNFP, ACS (Ant Colony System), Multi-objective</li>
<li><code>Objective</code>: Minimize total resource wastage and power consumption</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL3BpaS9TMDAyMjAwMDAxMzAwMDYyNw==">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Virtual Network Embedding through Topology Awareness and Optimization</strong></p>
<ul>
<li><code>Publication</code>: CN 2012 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Xiang Cheng, Sen Su, Zhongbao Zhang, Kai Shuang, Fangchun Yang, Yan Luo, Jie Wang</li>
<li><code>Keyworks</code>: VNFP, PSO (Particle Swarm Optimization), Topology decomposition</li>
<li><code>Objective</code>: Minimize total resource wastage and power Consumption</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wdWJsaWNhdGlvbi8yNTc1ODIyNTNfVmlydHVhbF9uZXR3b3JrX2VtYmVkZGluZ190aHJvdWdoX3RvcG9sb2d5X2F3YXJlbmVzc19hbmRfb3B0aW1pemF0aW9u">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
</ol>
<h2 id="reinforcement-learning-based"><a href="#content">Reinforcement learning-based</a></h2>
<h3 id="basic-rl"><a href="#content">Basic RL</a></h3>
<ol type="1">
<li><p><strong>A Dynamic Reliability-Aware Service Placement for Network Function Virtualization (NFV)</strong></p>
<ul>
<li><code>Publication</code>: JSAC 2020 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Zhongxia Yan, Jingguo Ge, Yulei Wu, Liangxiong Li, Tong Li</li>
<li><code>Keyworks</code>: VNFP, Dynamic Reliability-aware, MDP (Markov Deci- sion Process), Viterbi algorithm</li>
<li><code>Objective</code>: Minimize the placement cost and maximize the number of admitted services</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MTEuMDY1MzI=">paper<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>MUVINE: Multi-Stage Virtual Network Embedding in Cloud Data Centers Using Reinforcement Learning-Based Predictions</strong></p>
<ul>
<li><code>Publication</code>: JSAC 2020 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Hiren Kumar Thakkar, Chinmaya Dehury, Prasan Kumar Sahoo</li>
<li><code>Keyworks</code>: VNE, Q-learning, ML(Machine Learning), Multi-Stage</li>
<li><code>Objective</code>: Maximize the server resources utilization and minimizing the number of physical links used</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cDovLzEyMC4xMjYuMTYuMjUwL1B1YmxpY2F0aW9uX1BERi9qb3VybmFsL2o0NC5wZGY=">paper<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>A Privacy-Preserving Reinforcement Learning Algorithm for Multi-Domain Virtual Network Embedding</strong></p>
<ul>
<li><code>Publication</code>: TNSM 2020 (<strong>CCF-C</strong>)</li>
<li><code>Authors</code>: Davide Andreoletti, Tanya Velichkova, Giacomo Verticale, Massimo Tornatore , Silvia Giordano</li>
<li><code>Keyworks</code>: VNE, Multi-domain, Privacy</li>
<li><code>Objective</code>: /</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkxODcyMDU=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Virtual Network Embedding via Monte Carlo Tree Search</strong></p>
<ul>
<li><code>Publication</code>: IEEE Trans on Cybernetics 2018 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Soroush Haeri and Ljiljana Trajkovi´c</li>
<li><code>Keyworks</code>: VNE, MCTS (Monte Carlo Tree Search)</li>
<li><code>Objective</code>: Maximize the profit of InPs (revenue-to-cost and acceptance ratio)</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wdWJsaWNhdGlvbi8zMTM4NzM5MjZfVmlydHVhbF9OZXR3b3JrX0VtYmVkZGluZ192aWFfTW9udGVfQ2FybG9fVHJlZV9TZWFyY2g=">paper<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>An Efficient Algorithm for Virtual Network Function Placement and Chaining</strong></p>
<ul>
<li><code>Publication</code>: CCNC 2017</li>
<li><code>Authors</code>: Oussama Soualah, Marouen Mechtri, Chaima Ghribi, Djamal Zeghlache</li>
<li><code>Keyworks</code>: VNFP, MCTS (Monte Carlo Tree Search)</li>
<li><code>Objective</code>: Maximize the acceptance rate of provisioning requests</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wdWJsaWNhdGlvbi8zMTg1NzkzNzNfQW5fZWZmaWNpZW50X2FsZ29yaXRobV9mb3JfdmlydHVhbF9uZXR3b3JrX2Z1bmN0aW9uX3BsYWNlbWVudF9hbmRfY2hhaW5pbmc=">paper<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
</ol>
<!-- <details><summary> more </summary>  -->
<h3 id="deep-rl"><a href="#content">Deep RL</a></h3>
<ol type="1">
<li><p><strong>Automatic Virtual Network Embedding: A Deep Reinforcement Learning Approach With Graph Convolutional Networks</strong></p>
<ul>
<li><code>Publication</code>: JSAC 2020 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Zhongxia Yan, Jingguo Ge, Yulei Wu, Liangxiong Li, Tong Li</li>
<li><code>Keyworks</code>: VNE, A3C (Asynchronous Advantage Actor-Critic), GCN (Graph Convolutional Network)</li>
<li><code>Objective</code>: Minimizing the acceptance ratio and long-term average revenue</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkwNjA5MTA=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Optimal VNF Placement via Deep Reinforcement Learning in SDN/NFV-Enabled Networks</strong></p>
<ul>
<li><code>Publication</code>: JSAC 2020 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Jianing Pei, Peilin Hong, Miao Pan, Jiangqing Liu, Jingsong Zhou</li>
<li><code>Keyworks</code>: VNFP, DDQN (Double Deep Q Network), BIP (Binary Integer Programming)</li>
<li><code>Objective</code>: Minimize the weighted cost consisting of VNF placement cost, penalty of reject SFCRs and VNFI running cost in every time interval <span class="math inline">\(\Delta t\)</span></li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg5MzI0NDU=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Intelligent VNF Orchestration and Flow Scheduling via Model-Assisted Deep Reinforcement Learning</strong></p>
<ul>
<li><code>Publication</code>: JSAC 2020 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Lin Gu, Deze Zeng, Wei Li, Song Guo, Albert Y. Zomaya, Hai Jin</li>
<li><code>Keyworks</code>: VNFS, Latency-awareness, flow, DDPG (Deep Deterministic Policy Gradient)</li>
<li><code>Objective</code>: Maximize the overall network utility with the consideration of end-to-end delay and various cost</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg5MzE3NzU=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Virtual Network Function Placement Optimization with Deep Reinforcement Learning</strong></p>
<ul>
<li><code>Publication</code>: JSAC 2019 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Ruben Solozabal, Josu Ceberio, Aitor Sanchoyerto, Luis Zabala, Bego Blanco, Fidel Liberal</li>
<li><code>Keyworks</code>: VNFP, PG (Policy Gradient), Seq2Seq (Sequence-to-Sequence)</li>
<li><code>Objective</code>: Minimize the overall power consumption</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg5NDUyOTE=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>DeepViNE: Virtual Network Embedding with Deep Reinforcement Learning</strong></p>
<ul>
<li><code>Publication</code>: INFOCOM 2019 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Mahdi Dolati, Seyedeh Bahereh Hassanpour, Majid Ghaderi, Ahmad Khonsari</li>
<li><code>Keyworks</code>: VNE, DQN (Deep Q Network), Multi-channels Representations</li>
<li><code>Objective</code>: Minimize the VN blocking probability</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9wZW9wbGUudWNhbGdhcnkuY2Evfm1naGFkZXJpL2RvY3MvaW5mb2NvbTE5LWRlZXB2aW5lLnBkZg==">paper<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Multi-domain Non-cooperative VNF-FG Embedding: A Deep Reinforcement Learning Approach</strong></p>
<ul>
<li><code>Publication</code>: INFOCOM 2019 (<strong>CCF-A</strong>)</li>
<li><code>Authors</code>: Pham Tran Anh Quang, Abbas Bradai, Kamal Deep Singh, Yassine Hadjadj-Aoul</li>
<li><code>Keyworks</code>: VNFP, DDPG (Deep Deterministic Policy Gradient), Multi-domain, Non-cooperative</li>
<li><code>Objective</code>: Maximize the number of allocated VNFs and VLs with the lowest cost</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9oYWwuYXJjaGl2ZXMtb3V2ZXJ0ZXMuZnIvaGFsLTAyMDg4ODE5L2ZpbGUvTXVsdGlEb21haW5fVk5GX0ZHX2VtYmVkZGluZ19fQV9EZWVwX3JlaW5mb3JjZW1lbnRfbGVhcm5pbmdfYXBwcm9hY2gtYXV0aG9ycyUyMHZlcnNpb24ucGRm">paper<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>Deep Reinforcement Learning based VNF Management in Geo-distributed Edge Computing</strong></p>
<ul>
<li><code>Publication</code>: ICDCS 2019 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Lin Gu, Deze Zeng, Wei Li, Song Guo, Albert Y. Zomaya, Hai Jin</li>
<li><code>Keyworks</code>: VNFS, Latency-awareness, flow, DDPG (Deep Deterministic Policy Gradient)</li>
<li><code>Objective</code>: Minimize the end-to-end delays and various operation costs</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg4ODUxNTE=">IEEE Xplore<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>VNE-TD: A virtual network embedding algorithm based on temporal-difference learning</strong></p>
<ul>
<li><code>Publication</code>: CN 2019 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Sen Wang, Jun Bi, Jianping Wu, Athanasios V. Vasilakos, Qilin Fan</li>
<li><code>Keyworks</code>: VNFP, TD (Temporal Difference), GRC (Global Resource Control)</li>
<li><code>Objective</code>: Maximize the long-term time-average revenue of the InP</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzEzODkxMjg2MTgzMDU4NFg=">ScienceDirect<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>NFVdeep: adaptive online service function chain deployment with deep reinforcement learning</strong></p>
<ul>
<li><code>Publication</code>: IWQoS 2019 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Yikai Xiao, Qixia Zhang, Fangming Liu, Jia Wang, Miao Zhao, Zhongxing Zhang, Jiaxing Zhang</li>
<li><code>Keyworks</code>: VNFP, PG (Policy Gradient), Serialization and Backtracking, Time Slots</li>
<li><code>Objective</code>: Minimize the operation cost of occupied servers and maximize the total throughput of accepted requests</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wdWJsaWNhdGlvbi8zMzM3ODk5OThfTkZWZGVlcF9hZGFwdGl2ZV9vbmxpbmVfc2VydmljZV9mdW5jdGlvbl9jaGFpbl9kZXBsb3ltZW50X3dpdGhfZGVlcF9yZWluZm9yY2VtZW50X2xlYXJuaW5n">paper<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
</ol>
<h2 id="unassorted"><a href="#content">Unassorted</a></h2>
<blockquote>
<p>They will be classified as soon as possible.</p>
</blockquote>
<ol type="1">
<li><p><strong>Virtual Network Functions Migration Cost: from Identification to Prediction</strong></p>
<ul>
<li><code>Publication</code>: CN 2020 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Rafael de JesusMartins, Cristiano Bonato Both, Juliano Araújo Wickboldt, Lisandro Zambenedett iGranville</li>
<li><code>Keyworks</code>: VNFM, Linear regression</li>
<li><code>Objective</code>: A novel architecture for orchestrating and enforcing multi-domain SFCs</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL3BpaS9TMTM4OTEyODYyMDMxMTE4WA==">ScienceDirect<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>On cross-domain Service Function Chain orchestration: An architectural framework</strong></p>
<ul>
<li><code>Publication</code>: CN 2021 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Nassima Toumi, Olivier Bernier, Djamal-Eddine Meddour, Adlen Ksentini</li>
<li><code>Keyworks</code>: VNFC &amp; VNFP, Multi-domain</li>
<li><code>Objective</code>: A novel architecture for orchestrating and enforcing multi-domain SFCs</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzEzODkxMjg2MjEwMDAwMTM=">ScienceDirect<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>pSMART: A lightweight, privacy-aware service function chain orchestration in multi-domain NFV/SDN</strong></p>
<ul>
<li><code>Publication</code>: CN 2020 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Kalpana D. Joshi , Kotaro Kataoka</li>
<li><code>Keyworks</code>: VNFC, Multi-domain, Privacy</li>
<li><code>Objective</code>: Utilize less sensitive information, to reduce privacy and security risks</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzEzODkxMjg2MTkzMTExODE=">ScienceDirect<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
<li><p><strong>End-to-end network slicing for future wireless in multi-region cloud platforms</strong></p>
<ul>
<li><code>Publication</code>: CN 2020 (<strong>CCF-B</strong>)</li>
<li><code>Authors</code>: Simona Marinova , Thomas Lin, Hadi Bannazadeh, Alberto Leon-Garcia</li>
<li><code>Keyworks</code>: VNFC &amp; VNFP, Multi-domain, E2E (End-to-end) network slicing</li>
<li><code>Objective</code>: /</li>
<li><code>Link</code>: <span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzEzODkxMjg2MTkzMTYwODE=">ScienceDirect<i class="fa fa-external-link-alt"></i></span></li>
</ul></li>
</ol>
]]></content>
      <categories>
        <category>RP - 科研论文</category>
      </categories>
      <tags>
        <tag>Paper</tag>
        <tag>NFV</tag>
        <tag>DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】GNN之GAT：Graph Attention Networks</title>
    <url>/2020/12/01/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/paper-dl-gnn-gat/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>图神经网络（GNN）是当前深度学习领域研究的焦点之一，本文提出了一种GAT（graph attention networks）网络，它使用多头的masked自注意力机制来为每个邻居节点分配不同的权重，优化了图卷积神经网络的平均加权的问题。</p>
<p><strong>论文名称</strong>：Graph Attention Networks<br />
<strong>论文作者</strong>：Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio<br />
<strong>发表期刊</strong>：ICLR 2018 (THU-A)<br />
<strong>研究方向</strong>：GNN 图神经网络<br />
<strong>关键技术</strong>：Masked self Attention, Multi Attention<br />
<strong>主要创新</strong>：将多头注意力机制应用于图神经网络，来提升特征提取效果。<br />
<span class="exturl" data-url="aHR0cHM6Ly9oYWwuYXJjaGl2ZXMtb3V2ZXJ0ZXMuZnIvaGFsLTAyMDg4ODE5L2ZpbGUvTXVsdGlEb21haW5fVk5GX0ZHX2VtYmVkZGluZ19fQV9EZWVwX3JlaW5mb3JjZW1lbnRfbGVhcm5pbmdfYXBwcm9hY2gtYXV0aG9ycyUyMHZlcnNpb24ucGRm">下载论文<i class="fa fa-external-link-alt"></i></span> | <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1BldGFyVi0vR0FUP3V0bV9zb3VyY2U9Y2F0YWx5emV4LmNvbQ==">查看源码<i class="fa fa-external-link-alt"></i></span></p>
<a id="more"></a>
<h2 id="论文简介">论文简介</h2>
<h3 id="缩写释义">缩写释义</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">缩写</th>
<th>描述</th>
<th>全称</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">GNN</td>
<td>图神经网络</td>
<td>Graph Neural Network</td>
</tr>
<tr class="even">
<td style="text-align: center;">GCN</td>
<td>图卷积网络</td>
<td>Graph Convolutional Network</td>
</tr>
<tr class="odd">
<td style="text-align: center;">GAT</td>
<td>图注意力网络</td>
<td>Graph Attention Network</td>
</tr>
<tr class="even">
<td style="text-align: center;">/</td>
<td>注意力机制</td>
<td>Attention mechanisms</td>
</tr>
</tbody>
</table>
<h3 id="研究背景">研究背景</h3>
<ul>
<li>图结构数据是广泛存在的且较难处理</li>
<li>早期，为了将图数据结构适应神经网络模型，用网格化的图数据来进一步使用CNN进行特征提取。</li>
<li>之后，尝试扩展神经网络来直接对图数据进行处理</li>
<li>受到CNN卷积思想的启发，图卷积神经网络也被提出和不断完善，通常分为谱方法和非谱方法。</li>
<li>此外，注意力机制被广泛应用在端到端的任务模型中，它可以提取当前处理的输入与整个输入序列相关性更高的信息，从而提升了特征提取的效果。</li>
</ul>
<p>本文提出一种基于注意力机制的图神经网络来GAT来使神经网络可以更高效的提取节点特征。其思想是通过关注邻域，遵循self-attention策略，计算图中每个节点的隐藏表示。</p>
<h2 id="算法模型-graph-attentional-layer">算法模型 Graph Attentional Layer</h2>
<h3 id="input-output">Input/ Output</h3>
<ul>
<li>输入：图中<span class="math inline">\(N\)</span>个节点<span class="math inline">\(F\)</span>个特征的集合 <span class="math display">\[\mathbf{h}=\left\{\vec{h}_{1}, \vec{h}_{2}, \ldots, \vec{h}_{N}\right\}, \vec{h}_{i} \in \mathbb{R}^{F}\]</span></li>
<li>输出：新的节点特征表示（<span class="math inline">\(F&#39;\)</span>为潜在基数） <span class="math display">\[\mathbf{h&#39;}=\left\{\vec{h&#39;}_{1}, \vec{h&#39;}_{2}, \ldots, \vec{h&#39;}_{N}\right\}, \vec{h&#39;}_{i} \in \mathbb{R}^{F&#39;}\]</span></li>
</ul>
<h3 id="self-attention">Self Attention</h3>
<p>为了充分提取原始节点的特征信息，模型放弃了图的结构信息而运行每个节点都可以进行信息交换，同时利用 masked 自注意机制模型来注入结构信息，即对于节点<span class="math inline">\(i\)</span>，仅计算其直接邻居<span class="math inline">\(j \in \mathcal{N_i}\)</span>（包括节点<span class="math inline">\(i\)</span>自身）之间的相关程度（权重系数）<span class="math inline">\(e_{i,j}\)</span>，并利用归一化方法方便进行加权。</p>
<p><span class="math inline">\(e_{i,j}\)</span>表示节点<span class="math inline">\(j\)</span>对节点<span class="math inline">\(i\)</span>的重要程度，类似于打分分值，计算式如下</p>
<p><span class="math display">\[e_{i j}=a\left(\mathbf{W} \vec{h}_{i}, \mathbf{W} \vec{h}_{j}\right)\]</span></p>
<p>式中，<span class="math inline">\(\mathbf{W} \in \mathbb{R}^{F^{\prime} \times F}\)</span>是一个参数矩阵，<span class="math inline">\(a: \mathbb{R}^{F^{\prime}} \times \mathbb{R}^{F^{\prime}} \rightarrow \mathbb{R}\)</span>是一种自注意力机制。</p>
<p>然后，利用Softmax进行归一化：</p>
<p><span class="math display">\[\alpha_{i j}=\operatorname{softmax}_{j}\left(e_{i j}\right)=\frac{\exp \left(e_{i j}\right)}{\sum_{k \in \mathcal{N}_{i}} \exp \left(e_{i k}\right)}\]</span></p>
<p>在本文中，注意力机制<span class="math inline">\(a\)</span>是一个参数为<span class="math inline">\(\overrightarrow{\mathbf{a}} \in \mathbb{R}^{2 F^{\prime}}\)</span>的前馈神经网络，且利用LeakyReLU作激活函数，故权重系数也可写做：</p>
<p><span class="math display">\[\alpha_{i j}=\frac{\exp \left(\text { Leaky } \operatorname{ReLU}\left(\overrightarrow{\mathbf{a}}^{T}\left[\mathbf{W} \vec{h}_{i} \| \mathbf{W} \vec{h}_{j}\right]\right)\right)}{\sum_{k \in \mathcal{N}_{i}} \exp \left(\text { LeakyReLU }\left(\overrightarrow{\mathbf{a}}^{T}\left[\mathbf{W} \vec{h}_{i} \| \mathbf{W} \vec{h}_{k}\right]\right)\right)}\]</span></p>
<p>式中，<span class="math inline">\(·T\)</span>代表转置，<span class="math inline">\(\|\)</span>表示连接操作。</p>
<p>通过上述运算，我们得到了节点<span class="math inline">\(i\)</span>与其邻居<span class="math inline">\(j\)</span>的注意力权重<span class="math inline">\(e_{i,j}\)</span>。之后进行加权求和，从而得到节点<span class="math inline">\(i\)</span>的特征输出：</p>
<p><span class="math display">\[\vec{h}_{i}^{\prime}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j} \mathbf{W} \vec{h}_{j}\right)\]</span></p>
<p>式中，<span class="math inline">\(\sigma\)</span>是激活函数。</p>
<h3 id="multi-head-attention">Multi-head Attention</h3>
<p>为了进一步提升注意力机制的性能，GAT利用了多头注意力机制，即<span class="math inline">\(K\)</span>个注意力机制分别独立执行相应操作，然后将它们的输出连接起来生成最终输出：</p>
<p><span class="math display">\[\vec{h}_{i}^{\prime}=\|_{k=1}^{K} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)\]</span></p>
<p>式中，<span class="math inline">\(\alpha_{ij}^{k}\)</span>表示第<span class="math inline">\(k\)</span>个注意力机制<span class="math inline">\(a_k\)</span>计算出的归一化注意力系数，<span class="math inline">\(\mathbf{W}^k是该注意力机制的参数矩阵\)</span>。注意到，在多头注意力机制下，最终输出中每个节点的特征数量为<span class="math inline">\(KF&#39;\)</span></p>
<p>特别地，如果在预测层（即Softmax上一层）执行多头注意力机制，将使用平均方法来代替连接操作：</p>
<p><span class="math display">\[\vec{h}_{i}^{\prime}=\sigma\left(\frac{1}{K} \sum_{k=1}^{K} \sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)\]</span></p>
<p><img data-src="/resource/images/paper/paper-dl-gnn-gat-1.png" /></p>
<p>上图描述了连接操作和平均方法的基于多头注意力机制的GAT。</p>
<p>左图中，注意力机制<span class="math inline">\(a(\mathbf{W}\vec{h_i},\mathbf{W}\vec{h_j})\)</span>分为两步：</p>
<ol type="1">
<li>线性加权得到注意力权值，即用神经网络参数<span class="math inline">\(\overrightarrow{\mathbf{a}}^{T}\)</span>与<span class="math inline">\(\left[\mathbf{W} \vec{h}_{i} \| \mathbf{W} \vec{h}_{j}\right]\)</span>相乘</li>
</ol>
<p><span class="math display">\[e_{i,j} = \overrightarrow{\mathbf{a}}^{T}\left[\mathbf{W} \vec{h}_{i} \| \mathbf{W} \vec{h}_{j}\right]\]</span></p>
<ol type="1">
<li>非线性激活进行归一化，即用softmax对注意力权值进行归一化</li>
</ol>
<p><span class="math display">\[\alpha_{i j}=\operatorname{softmax}_{j}\left(e_{i j}\right)=\frac{\exp \left(e_{i j}\right)}{\sum_{k \in \mathcal{N}_{i}} \exp \left(e_{i k}\right)}\]</span></p>
<ol type="1">
<li>加权求和得到最终输出，即用归一化注意力权值对各节点信息进行加权</li>
</ol>
<p><span class="math display">\[\vec{h}_{i}^{\prime}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j} \mathbf{W} \vec{h}_{j}\right)\]</span></p>
<p>右图中，有三条不同颜色的线分别代表三个注意力机制在节点<span class="math inline">\(i\)</span>处得到的结果，即此处<span class="math inline">\(K=3\)</span>。之后，进行连接操作或平均操作得到最终输出<span class="math inline">\(\vec{h}&#39;\)</span>。</p>
<h2 id="模型改进">模型改进</h2>
<ul>
<li><p>计算高效。无需使用特征值分解等复杂的矩阵运算，且操作基本都可以实现并行。</p></li>
<li><p>可为同一个邻居的节点分配不同的重要性，提高模型表达能力。此外，注意权重可能有助于提高解释能力。</p></li>
<li><p>注意机制以共享的方式应用于图中的所有边，因此它不依赖于对全局图结构或需预先访问其所有节点的。我们无需访问整个图，而只需要访问所关注节点的邻节点即可。这一特点的作用主要有：a) 图不需要是无向的（如果边j→i不存在，我们可以省略计算αij）。 b) 它使我们的技术直接适用于归纳学习，包括在训练过程中完全看不见的图形上评估模型的任务。</p></li>
<li><p>它是建立在所有邻节点上的，而且无需假设任何节点顺序</p></li>
<li><p>GAT可以被看作是MoNet的一个特例。特殊地是，不同于MoNet实例相比，本文模型使用节点特征进行相似性计算，而不是节点的结构属性（假设预先知道图形结构）。具体来说，可以通过将伪坐标函数（pseudo-coordinate function）设为<span class="math inline">\(u(x,y)=f(x) \| f(y)\)</span>，其中<span class="math inline">\(f(x)\)</span>表示节点<span class="math inline">\(x\)</span>的特征，<span class="math inline">\(\|\)</span>；相应的权重函数则变成了<span class="math inline">\(w_j(u)=softmax(MLP(u))\)</span>。</p></li>
</ul>
<h2 id="性能评估">性能评估</h2>
<h3 id="数据集">数据集</h3>
<p><img data-src="/resource/images/paper/paper-dl-gnn-gat-2.png" /></p>
<h3 id="测试任务">测试任务</h3>
<h4 id="转导学习transductive-learning">转导学习（Transductive Learning）</h4>
<p>先观察特定的训练样本，然后对特定的测试样本做出预测（从特殊到特殊），这类模型如k近邻、SVM等。</p>
<p>使用了三个标准的引证网络数据集——Cora、Citeseer与Pubmed。在这些数据集中，节点对应于文档，边（无向的）对应于引用关系。节点特征对应于文档的BoW表示。每个节点拥有一个类别标签（在分类时使用softmax激活函数）。</p>
<p><img data-src="/resource/images/paper/paper-dl-gnn-gat-3.png" /></p>
<h4 id="归纳学习inductive-learning">归纳学习（Inductive Learning）</h4>
<p>先从训练样本中学习到一定的模式，然后利用其对测试样本进行预测（即首先从特殊到一般，然后再从一般到特殊），这类模型如常见的贝叶斯模型。</p>
<p>本文使用了一个蛋白质关联数据集（protein-protein interaction, PPI），在其中，每张图对应于人类的不同组织。此时，使用20张图进行训练，2张图进行验证，2张图用于测试。每个节点可能的标签数为121个，而且，每个节点可以同时拥有多个标签（在分类时使用sigmoid激活函数）</p>
<p><img data-src="/resource/images/paper/paper-dl-gnn-gat-4.png" /></p>
<h3 id="评估指标">评估指标</h3>
<h2 id="总结思考">总结思考</h2>
<ul>
<li>多头注意力机制的应用：为每个邻接节点赋予不同的注意力权重，提升特征提取效果</li>
<li>不依赖全局图结构：GAT模型无需了解整个图结构，只需知道每个节点的邻节点即可</li>
</ul>
]]></content>
      <categories>
        <category>RP - 科研论文</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Multi-domain Non-cooperative VNF-FG Embedding - A DRL Approach</title>
    <url>/2020/11/29/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/paper-nfv-multi-domain-non-cooperative-vnf-fg-embedding/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>本文研究了多域非合作场景下的VNF放置问题，提出了一种深度强化学习DDPG算法与基于成本的首次拟合算法（CCF）相结合的方法来实现最大化VNF部署个数和最小化部署成本的目标。</p>
<h2 id="论文简介">论文简介</h2>
<p><strong>论文名称</strong>：Multi-domain Non-cooperative VNF-FG embedding: A deep reinforcement learning approach<br />
<strong>论文作者</strong>：Quang Tran Anh Pham, Abbas Bradai, Kamal Deep Singh, Yassine Hadjadj-Aoul<br />
<strong>发表期刊</strong>：INFOCOM-2019 (CCF-A)<br />
<strong>研究方向</strong>：NFV 网络功能虚拟化<br />
<strong>关键技术</strong>：多域非合作，虚拟网络功能嵌入, 深度强化学习<br />
<strong>主要创新</strong>：多域非合作场景；多通道的VNF-FG请求矩阵表示；基于Cost的First Fit算法<br />
<span class="exturl" data-url="aHR0cHM6Ly9oYWwuYXJjaGl2ZXMtb3V2ZXJ0ZXMuZnIvaGFsLTAyMDg4ODE5L2ZpbGUvTXVsdGlEb21haW5fVk5GX0ZHX2VtYmVkZGluZ19fQV9EZWVwX3JlaW5mb3JjZW1lbnRfbGVhcm5pbmdfYXBwcm9hY2gtYXV0aG9ycyUyMHZlcnNpb24ucGRm">下载论文<i class="fa fa-external-link-alt"></i></span></p>
<a id="more"></a>
<h2 id="问题定义">问题定义</h2>
<h3 id="缩写释义">缩写释义</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">缩写</th>
<th>描述</th>
<th>全称</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">NFV</td>
<td>网络功能虚拟化</td>
<td>Network Function Virtualization</td>
</tr>
<tr class="even">
<td style="text-align: center;">VNF-FG</td>
<td>虚拟网络功能转发图</td>
<td>Virtual Network Function - Forwarding Graph</td>
</tr>
<tr class="odd">
<td style="text-align: center;">VNF</td>
<td>虚拟网络功能</td>
<td>Virtual Network Function</td>
</tr>
<tr class="even">
<td style="text-align: center;">VL</td>
<td>虚拟链路</td>
<td>Virtual Link</td>
</tr>
<tr class="odd">
<td style="text-align: center;">QoS</td>
<td>服务质量</td>
<td>Quality of Service</td>
</tr>
<tr class="even">
<td style="text-align: center;">DDPG</td>
<td></td>
<td>deep deterministic policy gradient</td>
</tr>
</tbody>
</table>
<h3 id="vnf-fg">VNF-FG</h3>
<p>每个VNF-FG由若干由VL连接起来的VNF组成</p>
<ul>
<li><span class="math inline">\(\mathcal{N&#39;}\)</span>表示VNF的集合，<span class="math inline">\(|\mathcal{N&#39;}|\)</span>为VNF个数
<ul>
<li>对于VNF，考虑<span class="math inline">\(K_{VNF}\)</span>种资源，如CPU、RAM、Storage</li>
<li><span class="math inline">\(h_{n&#39;,k}\)</span>表示VNF <span class="math inline">\(n&#39;\)</span>对资源<span class="math inline">\(k\)</span>的请求量</li>
<li><span class="math inline">\(h_{n&#39;} = [h_{n&#39;,0}, ..., h_{n&#39;,K_{VNF-1}}]\)</span>表示VNF <span class="math inline">\(n&#39;\)</span>对各种资源的请求量</li>
</ul></li>
<li><span class="math inline">\(\mathcal{L&#39;}\)</span>表示VL的集合，<span class="math inline">\(|\mathcal{L&#39;}|\)</span>为VNF个数
<ul>
<li>对于VL，考虑<span class="math inline">\(K_{VL}\)</span>种资源，如bandwidth、latency、packet loss</li>
<li><span class="math inline">\(h_{l&#39;,k}\)</span>表示VNF <span class="math inline">\(l&#39;\)</span>对资源<span class="math inline">\(k\)</span>的请求量</li>
<li><span class="math inline">\(h_{l&#39;} = [h_{l&#39;,0}, ..., h_{l&#39;,K_{VNF-1}}]\)</span>表示VNF <span class="math inline">\(l&#39;\)</span>对各种资源的请求量</li>
</ul></li>
</ul>
<h3 id="问题约束">问题约束</h3>
<ul>
<li><p>一个VNF可以被成功部署在一个有足够的资源的主机上，且只能部署在一个主机上 <span class="math display">\[\sum_{n^{\prime}} \phi_{n}^{n^{\prime}} h_{n^{\prime}, k} \leq r_{n, k}, \forall n, k\]</span> <span class="math display">\[\sum_{n} \phi_{n}^{n^{\prime}} \leq 1, \forall n^{\prime}\]</span></p></li>
<li><p>一个虚拟链路可以被部署在拥有足够资源且满足QoS要求的底层链路上</p></li>
</ul>
<p><span class="math display">\[\sum_{l^{\prime}} \phi_{l}^{l^{\prime}} h_{l^{\prime}, b w} \leq r_{l, b w}, \forall l\]</span> <span class="math display">\[h_{l^{\prime}, \text {delay}} \leq D\left(\phi^{\prime^{\prime}}\right)\]</span> <span class="math display">\[h_{l^{\prime}, \text {loss}} \leq R\left(\phi^{l^{\prime}}\right)\]</span></p>
<h3 id="vnf-fg-请求">VNF-FG 请求</h3>
<p>一个三维矩阵 <span class="math inline">\(\left|\mathcal{N}^{\prime}\right| \times\left|\mathcal{N}^{\prime}\right| \times\left(2 \times K_{\mathrm{VNF}}+K_{\mathrm{VL}}\right)\)</span>，可以看做<span class="math inline">\(\left(2 \times K_{\mathrm{VNF}}+K_{\mathrm{VL}}\right) \text { -channel of }\left|\mathcal{N}^{\prime}\right| \times\left|\mathcal{N}^{\prime}\right|\)</span>，即<span class="math inline">\(\left(2 \times K_{\mathrm{VNF}}+K_{\mathrm{VL}}\right)\)</span>个通道的<span class="math inline">\(\left|\mathcal{N}^{\prime}\right| \times\left|\mathcal{N}^{\prime}\right|\)</span>的矩阵表示。</p>
<ul>
<li><span class="math inline">\(K_{VL}\)</span> 描述了虚拟链路的资源请求</li>
<li><span class="math inline">\(2 \times K_{VNF}\)</span> 分别描述了源VNF和目的VNF的资源请求</li>
</ul>
<h3 id="资源价格">资源价格</h3>
<p>VNF <span class="math inline">\(n&#39;\)</span>放置在底层节点<span class="math inline">\(n\)</span>上时，其资源<span class="math inline">\(m\)</span>的价格表示为<span class="math inline">\(c^m_{n,n^{\prime}}\)</span>，则VNF <span class="math inline">\(n&#39;\)</span>的价格向量可表示为：<span class="math inline">\(\mathbf{c}_{n^{\prime}}=\left[\mathbf{c}_{0, n^{\prime}}, \mathbf{c}_{1, n^{\prime}}, \ldots, \mathbf{c}_{|\mathcal{N}|-1, n^{\prime}}\right]\)</span>，其中<span class="math inline">\(c_{i,n^{\prime}}=[\mathbf{c}_{0, n^{\prime}}^{0},\cdots,\mathbf{c}_{0, n^{\prime}}^{k_{VNF}-1}]\)</span>。因此，需要用<span class="math inline">\(\left|\mathcal{N}^{\prime}\right| \times\left|\mathcal{N}^{\prime}\right| \times K_{VNF}\)</span>矩阵来表示。</p>
<p>VL <span class="math inline">\(l&#39;\)</span>带宽资源的代价表示为<span class="math inline">\(c_{i,l&#39;}\)</span>，则VL <span class="math inline">\(l&#39;\)</span>的价格向量可表示为：<span class="math inline">\(\mathbf{c}_{l^{\prime}}=\left[\mathbf{c}_{0, l^{\prime}}, \mathbf{c}_{1, l^{\prime}}, \ldots, \mathbf{c}_{|\mathcal{L}|-1, l^{\prime}}\right]\)</span>，需要用<span class="math inline">\(\left|\mathcal{L}^{\prime}\right| \times\left|\mathcal{L}^{\prime}\right|\)</span>来表示。</p>
<p>这些价格由每个与决定，它们会被发送到中心客户。</p>
<h2 id="算法模型">算法模型</h2>
<h3 id="多域非合作架构">多域非合作架构</h3>
<p>在该文中，环境是由多个非合作的域和一个中心客户（client）组成的。这些域有这些特征：</p>
<ol type="1">
<li>每个域都没有其他域的拓扑结构和资源信息</li>
<li>每个域不与其他域进行通信，</li>
<li>每个域单独做出决策将发送至中心客户，由中心客户根据价格进行选择</li>
</ol>
<p><img data-src="/resource/images/paper/paper-nfv-multi-domain-non-cooperative-vnf-fg-embedding-1.png" /></p>
<p>大致的决策流程如下：</p>
<ol type="1">
<li>中心代理将VNF-FG请求处理为一个三维向量，即<code>state</code>，然后发送给每个域</li>
<li>每个域使用actor网络根据<code>state</code>计算出<code>action</code>。每个域的动作<span class="math inline">\(A_i\)</span>是向中心客户提出的价格，如<span class="math inline">\(c_{n, n^{\prime}}^{k}, \forall n \in \mathcal{N}_{i}\)</span>和<span class="math inline">\(c_{l, l^{\prime}}, \forall l \in \mathcal{L}_{i}\)</span>分别代表节点和链路成本。</li>
<li>中心代理根据这些信息作出最终决策，每个域根据决策来部署VNF和VL，最后得到相应奖励</li>
</ol>
<h3 id="深度强化学习-ddpg">深度强化学习 DDPG</h3>
<h4 id="马尔科夫决策-mdp">马尔科夫决策 MDP</h4>
<ul>
<li>环境 Environment：多个域和一个中心客户</li>
<li>状态 State：经过处理的VNF-FG请求，是一个三维矩阵。</li>
<li>动作 Action：行为就像是对出售其资源的域名的竞价，它包括对节点资源和链路资源的报价。</li>
<li>奖励 Reward：根据部署质量和资源报价，中心代理返回给每个域相应奖励 <span class="math display">\[r_{i}=\sum_{n^{\prime} \in \mathcal{N}^{\prime}} \sum_{n \in \mathcal{N}_{i}} \sum_{k \in K_{c}} \omega_{n}^{n^{\prime}} c_{n, n^{\prime}}^{k} h_{n^{\prime}, k}+\sum_{l \in \mathcal{L}_{i}} \sum_{l^{\prime} \in \mathcal{L}^{\prime}} \omega_{l}^{l^{\prime}} c_{l, l^{\prime}} h_{l^{\prime}, b w}\]</span> 式中，<span class="math inline">\(w_n^{n&#39;}\)</span>和<span class="math inline">\(w_n^{n&#39;}\)</span>表示放置成功二进制位，当且仅当节点和链路被成功放置且满足服务质量时才为1。</li>
</ul>
<h4 id="代理架构-agent">代理架构 Agent</h4>
<p><img data-src="/resource/images/paper/paper-nfv-multi-domain-non-cooperative-vnf-fg-embedding-2.png" /></p>
<p>DDPG由两个神经网络组成</p>
<ul>
<li>Actor network: 学习策略。根据当前策略和输入的<span class="math inline">\(s_t\)</span>来生成动作，要注意的是，动作被添加了噪声<span class="math inline">\(N\)</span>来使agent更好地探索环境。</li>
<li>Critic network: 学习Q值。评估一个动作的优劣，来使actor学习到更好地策略。</li>
</ul>
<p>actor和critic神经网络由多种层组成来提取state中的信息，特别地，critic额外使用了卷积层来提取动作的特征：</p>
<ul>
<li>3层卷积层 convolutional layers
<ul>
<li>卷积层<span class="math inline">\(i\)</span>的过滤器和卷积核的大小为<span class="math inline">\(C_i\)</span>和<span class="math inline">\((F_i, F_i)\)</span></li>
<li>卷积层的输出反映了虚拟链路及其属性间的相互影响</li>
</ul></li>
<li>平均池化层 average pool layers
<ul>
<li>减少参数数量和防止过拟合</li>
</ul></li>
<li>全连接层 Fully Connected layers
<ul>
<li>将这些反映state信息相互影响的中间输出映射为动作</li>
</ul></li>
</ul>
<p><img data-src="/resource/images/paper/paper-nfv-multi-domain-non-cooperative-vnf-fg-embedding-3.png" /></p>
<p>收到每个域返回的动作（资源竞价），中心客户会做出最终的决定并执行，然后每个域会依据提供的资源收到相应的奖励，最后DRL代理会计算loss来更新参数。</p>
<p>对于域<span class="math inline">\(i\)</span>，其目标是找到一个从VNF-FG请求到本地动作的映射策略：<span class="math inline">\(\mu_i:S \rightarrow A_i\)</span>来最大化获得奖励。</p>
<h3 id="决策算法-cff">决策算法 CFF</h3>
<p>在收集了来自不同域<span class="math inline">\(A_i\)</span>的价格后，中心客户必须做出决定来部署其VNF-FG。虽然很难保证一条路径能够满足虚拟链路所需的QoS，但是可以保证VNFs的资源需求。因此，在部署VNF时，决策必须考虑底层节点的容量以及部署的成本。</p>
<p><img data-src="/resource/images/paper/paper-nfv-multi-domain-non-cooperative-vnf-fg-embedding-4.png" /></p>
<p>该文使用了将一种资源分配算法First-Fit算法改进为基于成本的First-Fit（CFF）算法来进行从VNF到底层节点的决策生成：</p>
<ul>
<li>输入：每个域的动作<span class="math inline">\(A_i\)</span></li>
<li>输出：VNF的映射集合<span class="math inline">\(\phi_n^{n&#39;}\)</span></li>
<li>初始化 <span class="math inline">\(\phi_n^{n&#39;}=0, \forall n, n&#39;\)</span>, <span class="math inline">\(A_* \leftarrow [A_1, \cdots, A_D]\)</span></li>
<li>将全局动作A_*分割为与VNF相关的成本<span class="math inline">\(A_{*,vnf}\)</span>和与VL相关的成本<span class="math inline">\(A_{*,vl}\)</span></li>
<li>对于每个VNF <span class="math inline">\(i\)</span>：
<ul>
<li>实际部署成本等于VNF <span class="math inline">\(i\)</span>的成本与其资源请求量<span class="math inline">\(h_i\)</span>的点乘：<span class="math inline">\(P = A_{*,vnf}[i]=c_i \cdot h_i\)</span>，由此可得到每个底层节点部署当前VNF的成本</li>
<li>按照部署成本<span class="math inline">\(P\)</span>进行升序排列</li>
<li>对于排列后的域序列中的单个域<span class="math inline">\(j\)</span>：
<ul>
<li>中心代理具有<span class="math inline">\(j\)</span>的域<span class="math inline">\(D(j)\)</span>发送请求</li>
<li>如果域$D(j)接受了该请求，那么
<ul>
<li><span class="math inline">\(\phi_n^{n&#39;}=1\)</span>，域<span class="math inline">\(D(j)\)</span>部署j到节点i上</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p>映射VNF策略的目标是最大化部署个数和最小化部署成本。映射虚拟链路<span class="math inline">\(l&#39;\)</span>时，根据<span class="math inline">\(A_{*,vl}[l&#39;]\)</span>作为链路权重，使用Dijkstra算法来寻找最小成本的链路。</p>
<h2 id="性能评估">性能评估</h2>
<h3 id="设计实验">设计实验</h3>
<h4 id="网络拓扑设置">网络拓扑设置</h4>
<ul>
<li>BtEurope网络拓扑：24个节点和37个全双工边</li>
<li>链路资源容量：20 Mbps, 30 Mbps, 50 Mbps, 100 Mbps随机分配</li>
<li>节点资源容量：<span class="math inline">\((0.3, 2.0)\)</span> 随机分配</li>
<li>使用延迟和丢包率来评价网络虚拟链路的部署质量</li>
</ul>
<h4 id="vnf-fg配置">VNF-FG配置</h4>
<ul>
<li>3到6个VNF组成，VNF连接率为0.5</li>
<li>VNF资源请求量被进行归一化</li>
<li>VL请求：<span class="math inline">\((1,30)\)</span> Mbps随机分配，延迟为1<sub>100毫秒，延迟率为0%</sub>0.5%</li>
</ul>
<h4 id="神经网络参数">神经网络参数</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">符号</th>
<th>含义</th>
<th>取值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">\</td>
<td>优化器</td>
<td>Adam</td>
</tr>
<tr class="even">
<td style="text-align: center;">\</td>
<td>actor学习率</td>
<td><span class="math inline">\(10^{-4}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">\</td>
<td>critic学习率</td>
<td><span class="math inline">\(10^{-3}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\gamma\)</span></td>
<td>折扣因子</td>
<td>0.99</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\tau\)</span></td>
<td>target网络更新系数</td>
<td>=0.001</td>
</tr>
<tr class="even">
<td style="text-align: center;">\</td>
<td>batch size</td>
<td>32</td>
</tr>
<tr class="odd">
<td style="text-align: center;">\</td>
<td>全连接层unit个数</td>
<td>300</td>
</tr>
<tr class="even">
<td style="text-align: center;">\</td>
<td>卷积层卷积核</td>
<td><span class="math inline">\((4,4),(2,2),(5,5)\)</span></td>
</tr>
</tbody>
</table>
<h3 id="对比算法">对比算法</h3>
<ul>
<li>CFF-SD<br />
single domain with CFF，单域CCF算法，是非竞争的</li>
<li>CFF-3D<br />
three domain with CFF，三域使用CCF算法</li>
<li>SA simulated annealing algorithm，模拟退火算法</li>
<li>SA-CFF<br />
simulated annealing algorithm with CFF，模拟退火和CCF算法结合</li>
</ul>
<h3 id="评估指标">评估指标</h3>
<h4 id="平均奖励和单位价格">平均奖励和单位价格</h4>
<p><img data-src="/resource/images/paper/paper-nfv-multi-domain-non-cooperative-vnf-exp-reward.png" /></p>
<h4 id="vnf和vl部署成功率">VNF和VL部署成功率</h4>
<p><img data-src="/resource/images/paper/paper-nfv-multi-domain-non-cooperative-vnf-exp-deploy.png" /></p>
<h4 id="部署成功率对比">部署成功率对比</h4>
<p><img data-src="/resource/images/paper/paper-nfv-multi-domain-non-cooperative-vnf-exp-comparison.png" /></p>
<h4 id="多域部署情况">多域部署情况</h4>
<p>每个域平均资源情况</p>
<ul>
<li>域1有最少的CPU和RAM资源</li>
<li>域3有最少的存储资源</li>
</ul>
<p><img data-src="/resource/images/paper/paper-nfv-multi-domain-non-cooperative-vnf-exp-resource.png" /></p>
<p>每一个域实际部署情况</p>
<ul>
<li>域2部署的VNF和VL都是最多的</li>
<li>DRL会考虑负载均衡来进行放置决策</li>
</ul>
<p><img data-src="/resource/images/paper/paper-nfv-multi-domain-non-cooperative-vnf-exp-mean-domain-deploy.png" /></p>
<h2 id="总结思考">总结思考</h2>
<ul>
<li>场景特殊：多域非合作场景下的VNF放置问题</li>
<li>资源种类多：对于VNF和VL均考虑多种资源约束</li>
<li>矩阵构造：将VNF-FG请求处理为一个包含VNF和VL请求信息的3维矩阵</li>
<li>深度强化学习：使用了DDPG的DRL方法来训练神经网络</li>
<li>改进启发式算法：改进了First Fit算法来达到最小化部署成本的目标</li>
</ul>
]]></content>
      <categories>
        <category>RP - 科研论文</category>
      </categories>
      <tags>
        <tag>Paper</tag>
        <tag>NFV</tag>
        <tag>DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>【学术讲座】CCF 2020 中国软件大会</title>
    <url>/2020/11/22/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/lecture-ccf-china-software-2020/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>CCF 2020 中国软件大会上郑志明、何积丰、等院士的讲座分享。</p>
<ul>
<li>郑志明院士：精准智能——面向动态复杂对象的AI</li>
<li>何积丰院士：思维机器与强人工智能</li>
</ul>
<a id="more"></a>
<h2 id="郑志明院士精准智能面向动态复杂对象的ai">郑志明院士：精准智能——面向动态复杂对象的AI</h2>
<h3 id="近现代科学">近现代科学</h3>
<h4 id="近代科学">近代科学</h4>
<p>时期：17-19世纪 标志：牛顿、线性化</p>
<p><img data-src="/resource/images/lecture/lecture-ccf-chinasoft-2020-zzm-1.jpg" /></p>
<p>后牛顿时代 - 研究：具有非线性、动态、随机特征的复杂动态系统 - 困难：三维以上的动态系统几乎都是复杂系统</p>
<p>线性与非线性</p>
<ul>
<li>复杂度：非线性明显高于线性</li>
<li>收敛性：非线性收敛点可能是一片</li>
</ul>
<h4 id="现代科学">现代科学</h4>
<p>时期：20世纪初以来</p>
<p><img data-src="/resource/images/lecture/lecture-ccf-chinasoft-2020-zzm-2.jpg" /></p>
<p>发展现状</p>
<ul>
<li>复杂物理或自然系统→复杂数据系统</li>
<li>传统复杂物理自然系统的数字化表征→科学进入大数据时代</li>
</ul>
<h3 id="人工智能">人工智能</h3>
<h4 id="ai介入数据的科学分析">AI介入数据的科学分析</h4>
<p><img data-src="/resource/images/lecture/lecture-ccf-chinasoft-2020-zzm-3.jpg" /></p>
<h4 id="数学角度下当前的ai">数学角度下当前的AI</h4>
<p>当前的人工智能主要是<strong>逻辑或统计+动态线性学习方法</strong>来<strong>逼近非线性动态系统的不变集</strong></p>
<p>通常所说大数据复杂，数据特征复杂仅是一方面，而导致系统复杂的关键要素是<strong>非线性随机关联关系</strong></p>
<p>如果这样的方法可行，牛顿科学就终结了现代科学</p>
<h4 id="新的尝试">新的尝试</h4>
<p>DARPA率先在基础研究层面布局，探索<strong>内嵌物理、数学和先验知识</strong>的人工智能原型，准确性、稳定性、普适性等方面的性能关破</p>
<h3 id="精准智能">精准智能</h3>
<table>
<thead>
<tr class="header">
<th>人工智能</th>
<th>→</th>
<th>精准智能</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>大数据 <br> 统计特征</td>
<td>探索内嵌先天科学结构的数据表述范式<br>基于非线性关联关系的数据表征</td>
<td>科学数据 <br> 数理表征</td>
</tr>
<tr class="even">
<td>统计+线性</td>
<td></td>
<td>近似科学数据场</td>
</tr>
<tr class="odd">
<td>基于调参学习</td>
<td>深索基于相变和分支突变系统学习<br>理论演化结构理论、小概率（小样本）事件学习方法</td>
<td>基于系统行为学习</td>
</tr>
</tbody>
</table>
<ul>
<li>爆炸式渗流——<strong>内嵌渗流系统的分支性态发现和突变学习方法</strong>，用于精准的发现信息全局爆发节点，提出抑制和控制策略 <img data-src="/resource/images/lecture/lecture-ccf-chinasoft-2020-zzm-4.jpg" /></li>
<li>AMS-02——<strong>内嵌扩散机理的复杂系统耦合、解耦和演化学习方法</strong>，在P级数据中快速准确发现含有暗物质信息的K到M级数据，误差约万分之一。 <img data-src="/resource/images/lecture/lecture-ccf-chinasoft-2020-zzm-5.jpg" /></li>
</ul>
<h2 id="何积丰院士思维机器与强人工智能">何积丰院士：思维机器与强人工智能</h2>
<h3 id="causal-derivation-based-ai">Causal Derivation based AI</h3>
<blockquote>
<p>"The knowledge gained from data-centered machine learning is weak in empirical intepretation and may be completely opposite to the original knowledge."<br />
"The next generation of AI will focus on causal derivation and be centered on science rather than data."<br />
——Judea Pearl</p>
</blockquote>
<p>现阶段的AI：</p>
<ul>
<li>数据 → 模型 <span class="math inline">\(f_k(x)\)</span> → 结果 <span class="math inline">\(y_k\)</span></li>
<li>结果的不确定性，即结果不一定是正确的</li>
<li>缺乏可解释性不可怕，人脑也是，重要的是要可与环境对话！</li>
</ul>
<h4 id="role-of-a-casual-reasoning-model">Role of a casual Reasoning Model</h4>
<p>Give machines the ability</p>
<ul>
<li>to reflect on their mistakes</li>
<li>to pinpoint weakness in their software</li>
<li>to converse naturally with humans about their own choices and intentions</li>
</ul>
<h4 id="language-of-thought">Language of Thought</h4>
<p>Use logic tools to model thinking</p>
<p><img data-src="/resource/images/lecture/lecture-ccf-chinasoft-2020-hjf-2.jpg" /></p>
<h4 id="complex-thinking-combination">Complex Thinking Combination</h4>
<p>Thinking is the process of processing and digesting knowledge and generating new knowledge, the in-depth understanding and research on the combination of complex thinking．It is the key to break through the current data-based artificial intelligence mechanisms and models.</p>
<p><img data-src="/resource/images/lecture/lecture-ccf-chinasoft-2020-hjf-3.jpg" /></p>
<h3 id="calculus-of-causation">Calculus of Causation</h3>
<h4 id="language-of-knowledge">Language of Knowledge</h4>
<p>A diagrammatic language of causal diagrams is used to express what we know. They are simply dot and_arrow pictures where</p>
<ul>
<li>the dots represent quantities of interests, and</li>
<li>the arrows represent known or suspected relationship between those quantities</li>
</ul>
<p>They are subject to the scrutiny of data, and can be falsified.</p>
<blockquote>
<p>Example: Consider the chain A→B→C. If the observed data do not show a and c to be independent, conditional on b, then we can conclude that the chain model needs to be discarded or repaired.</p>
</blockquote>
<p>The graphic properties dictate which causal models can be distinguished by data, and which will forever remain indistinguishable.</p>
<blockquote>
<p>Example: no data can distinguish A←B→C from A→B→C, because with C listening to B only, the two imply the same independence conditions.</p>
</blockquote>
<p>Each arrow can be thought of as a statement about the outcome of a hypothesis experiment.</p>
<ol type="1">
<li>An arrow from a to C means that if we would wiggle only A, then it is expected to see a change in the probability of C</li>
<li>A missing arrow from a to C means in the same experiment we would not see any change in C, once we held constant the parents of C</li>
</ol>
<p>！模型的逻辑适应性</p>
<h4 id="language-of-queries">Language of Queries</h4>
<p>To express the questions we want answers to</p>
<ol type="1">
<li>Association
<ul>
<li>Activity:　seeing, observing</li>
<li>Questions: How would seeing X change my belief of Y</li>
<li>Example: What does symptom tell me about disease</li>
</ul></li>
<li>Intervention
<ul>
<li>Activity: doing, intervening</li>
<li>Questions: How can I make Y happen</li>
<li>Example: What if we ban smoking</li>
</ul></li>
<li>Counterfactuals
<ul>
<li>Activity: imaging, retrospection, understanding</li>
<li>Questions: Was it X caused Y? What if I had acted differently</li>
<li>Example: What if I had not smoked for the last two years</li>
</ul></li>
</ol>
<h3 id="big-data-and-causal-model">Big Data and Causal Model</h3>
<p>The amount of raw data has grown at a staggering rate in recent years The rise of huge databases in science</p>
<p>数据为中心的AI：</p>
<ol type="1">
<li>观测现有数据</li>
<li>动态生成衍生数据来探索更多场景</li>
<li>人机互联，环境交互</li>
</ol>
<h4 id="symbiosis-between-big-data-and-causal-inference">Symbiosis between Big Data and Causal Inference</h4>
<ul>
<li>Background<br />
Thanks to Big Data, we can access an enormous number of studio, conducted in different location and under different condition</li>
<li>Questions<br />
Can we combine the results of these studio and translate them to new population. i.e there a complete criterion for deciding when results are transportable and when they are　not</li>
</ul>
<h4 id="inference-machines">Inference Machines</h4>
<p><img data-src="/resource/images/lecture/lecture-ccf-chinasoft-2020-hjf-1.jpg" /></p>
<h3 id="strong-ai">Strong AI</h3>
<ol type="1">
<li><p>A machine can reflect on its actions and learn from past mistake.</p></li>
<li><p>It should be able to understand the statement"i should have acted differently.</p></li>
<li><p>Intent is a very important part of personal decision making. The ability to conceive of one's own intent and then use it as a pieces of evidence in casual reasoning is a level of self-awareness.</p></li>
<li><p>The discussion of 3 leads to a major issue for strong ai free will</p></li>
<li><p>The ability to reason about one's own beliefs, intents, and desires has been a major challenge to ai and defines the notion of "agency".<br />
The software package that can give a thinking machine the benefits of agency should consist at least following parts:</p>
<ul>
<li>A causal model of the world</li>
<li>A causal model of its own software</li>
<li>A memory that records how intents in its mind correspond to events in the outside world</li>
</ul></li>
</ol>
<blockquote>
<p>Example</p>
<ul>
<li>I have done X=X, and the outcome was <span class="math inline">\(Y=y\)</span>. But if I had acted differently, say <span class="math inline">\(X=x\)</span>, then the outcome would have been better, perhaps <span class="math inline">\(Y\)</span>.</li>
<li>If we are asking a machine to have the intent to do <span class="math inline">\(X=x\)</span>, because aware of it, and choose to do <span class="math inline">\(X=x&#39;\)</span> instead, we seem to be asking it to have free will</li>
</ul>
</blockquote>
<p>What to Think about machines? That Think</p>
<ol type="1">
<li>Have we already made machines that think</li>
<li>Can we make machines that think</li>
<li>Will we make machines that think</li>
<li>Should we make machines that think</li>
</ol>
<h3 id="research-content">Research Content</h3>
<ol type="1">
<li>Thinking-oriented Knowledge Expression
<ul>
<li>Knowledge is the essential factor for summarizing and refining thinking</li>
<li>Existing forms of knowledge expression are not sufficient to reflect the process of thinking.</li>
<li>Knowledge expression reflecting thinking process.</li>
</ul></li>
<li>Interaction Model of Knowledge and Thinking
<ul>
<li>The relationship between conscious thinking and knowledge is more specific than other consciousness.</li>
</ul></li>
<li>Compositionality of Thinking</li>
<li>Formation Mechanism of New Knowledge and New Architecture
<ul>
<li>Combinable thinking → Create new knowledge</li>
</ul></li>
</ol>
<h2 id="我的感悟">我的感悟</h2>
<p>有很多打破我原有认知的观点，也有很多自己目前难以理解的知识。希望多读几遍自己可以获得更好的理解。</p>
]]></content>
      <categories>
        <category>RP - 科研论文</category>
      </categories>
      <tags>
        <tag>Lecture</tag>
        <tag>Research</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>【Web】《CQU软件综合实践平台》项目总结</title>
    <url>/2020/09/03/WD%20-%20WEB%E5%BC%80%E5%8F%91/wd-web-cquse-system/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script>
    <div id="aplayer-rzaAfYmi" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="002C8n7x1RdgXe" data-server="tencent" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#ad7a86"
    ></div>
<p>在大二小学期软件实训课程中，自己做了一个“CQU软件综实践平台”的项目，于是在此总结一些自己第一次做自己的全栈开发项目的经验。（其实这篇总结应该很早就写的，至于为什么拖到了现在？“不愧是我——那只Monkey又在作怪”）</p>
<a id="more"></a>
<h2 id="前言">前言</h2>
<h3 id="为什么">为什么</h3>
<p>为什么会选择做“CQU软件综合实践平台”这个项目呢？主要是因为自己在入门Vue后便很少去实践，关于后端的知识也少有动力去精进，恰巧课程名称《软件综合实践》且主要假以钉钉、QQ作为课程平台。于是在此特定的时机场景，便萌生了做一个重庆大学软件综合实践平台的想法。</p>
<h3 id="做什么">做什么</h3>
<p>现有的网络课程平台或是专门针对实验课程的平台繁多且不乏高质量网站，个人深知能力有限，并不渴求可以做出功能十分完善、需求十分满足的工业级项目。更多的出于自我学习的目的，项目偏向于一些必要需求的实现和一些特定场景的优化，因此，在前期需求分析阶段所作出的分析也主要是针对所学课程当中的常见需求和场景。</p>
<h3 id="怎么做">怎么做</h3>
<p>该平台是基于B/S的实验课程管理系统，故离不开前端与后端的技术支撑。在具体实现过程中，主要用到了以下技术栈：</p>
<ul>
<li>前端：HTML、CSS、JS、Vue、Element UI</li>
<li>后端：SpringBoot、Redis、Mybatis plus、AliYun oss</li>
<li>数据库：MySQL、CURD、乐观锁、触发器、范式设计</li>
</ul>
<h2 id="展示">展示</h2>
<p><img data-src="/resource/images/web/web-cquse-overview.png" /></p>
<p>以下为项目中部分界面的展示</p>
<h3 id="统一登录入口">统一登录入口</h3>
<p><img data-src="/resource/images/web/web-cquse-login.webp" /></p>
<h3 id="学生功能页面">学生功能页面</h3>
<h4 id="学生主页">学生主页</h4>
<p><img data-src="/resource/images/web/web-cquse-stu-home.webp" /></p>
<h4 id="学生签到界面">学生签到界面</h4>
<p><img data-src="/resource/images/web/web-cquse-stu-sign.webp" /></p>
<h4 id="学生上传报告">学生上传报告</h4>
<p><img data-src="/resource/images/web/web-cquse-stu-report.webp" /></p>
<h4 id="学生成绩查询">学生成绩查询</h4>
<p><img data-src="/resource/images/web/web-cquse-stu-grade.webp" /></p>
<h4 id="学生个人中心">学生个人中心</h4>
<p><img data-src="/resource/images/web/web-cquse-stu-user.webp" /></p>
<h3 id="教师功能页面">教师功能页面</h3>
<h4 id="教师主页">教师主页</h4>
<p><img data-src="/resource/images/web/web-cquse-tch-home.webp" /></p>
<h4 id="教师登记成绩">教师登记成绩</h4>
<p><img data-src="/resource/images/web/web-cquse-tch-grade.webp" /></p>
<h4 id="教师查看选题">教师查看选题</h4>
<p><img data-src="/resource/images/web/web-cquse-tch-project.webp" /></p>
<h3 id="后台功能页面">后台功能页面</h3>
<h4 id="学生信息管理">学生信息管理</h4>
<p><img data-src="/resource/images/web/web-cquse-admin-stu.webp" /></p>
<h4 id="修改学生信息">修改学生信息</h4>
<p><img data-src="/resource/images/web/web-cquse-admin-modify.webp" /></p>
<h2 id="总结">总结</h2>
<h3 id="前端方面">前端方面</h3>
<p>Vue、React、Angular等等这些流行的前端框架一定程度上改变了前端开发的模式，工程化地提高了前端开发的效率，但也对前端开发提出了更高的要求。 项目中，通过进一步对Vue的学习，发现Vue很多新的特性和难的特性都可以在JavaScript、TypeScript中找到源痕迹。Vue在新beta版本中加入的新特性，也越来越偏重于TS的语言特性，更加注重模板与类的使用。但奈何个人对TypeScript了解甚少，因此对于一些语法糖的使用规范理解得并不是很好。所以编码时，秉承着真不会就不用的原则..还有要感谢Element UI让我写出了还能看的UI。</p>
<p><strong>感想：基础为底石，方能站得稳，看得深，做得好。</strong></p>
<h3 id="后端方面">后端方面</h3>
<p>从Django到Springboot的过渡可能如学语言时从python到java的过渡，虽然可说是由“简”入“繁”，但整个项目体验下来，不得不承认java语言的工程性会更强些。 整个项目的完成离不开对各种插件的使用，如Mybatis-plus、lombok、easy-excel等等，这些插件对一些常见的方法进行了封装实现，明显提高了开发效率。但其实过渡封装反而会对自定义方法的实现带来不便。 另外，Mybatis-plus官方文档在自定义SQL方面，特别xml实现方面，写的就很…不容易理解，也可能需要Mybatis的基础吧，然而课程时间限制来不及去补充了解下。现在感觉要想真正了解一些高效的插件，离不开对其实现代码的阅读。</p>
<p><strong>感想：所谓的高效开发往往可能是对底层实现的不求甚解。</strong></p>
<h3 id="数据库方面">数据库方面</h3>
<p>原本以为数据库应该是整个项目最简单的部分，但不得承认数据库改动次数还是挺多的，而且数据库的一变前后端都得变，就很繁琐。虽说数据库设计上也算还可以，但是由于数据量、实践和个人能力的问题，对数据库中关于并发、缓存、锁粒度等等并没有得到很好地应用。 此外，《高性能MySQL》这本书让我知道数据库理论并不是简单的，要想达到最优的性能，就必须了解如join、in等等这些选择方式背后的原理实现。当然，自以为然地掌握了原理并不是软件工程的最终目的，考卷上三四张表的设计已经满足了考察水平和时间要求，但现实生活中应用场景要远远复杂，真正要达到较高的掌握程度就要真正去实践。</p>
<p><strong>感想：检验理论掌握程度的最佳标准是实践。</strong></p>
<h2 id="后记">后记</h2>
<p>三周的小学期对于拖延症患者而言并不长，精确些来说只是三天。至于说Deadline前的三天是如何度过，我只能期望大脑中的Monkey尽早能更加有远见些。</p>
]]></content>
      <categories>
        <category>WD - 网站开发</category>
      </categories>
      <tags>
        <tag>front-end</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title>【随笔】时光刻痕——记博客重启</title>
    <url>/2020/08/25/MY%20-%20%E6%84%9F%E6%82%9F%E9%9A%8F%E7%AC%94/my-record-our-moments/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script>
    <div id="aplayer-Vyrpppkx" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="000ycaRl2V0Ojm" data-server="tencent" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#ad7a86"
    ></div>
<p>日历翻至八月，空气日渐升温。在这个特殊的年份，暑假的宿舍只余下我一人——这也是我第一次独自在远离家乡的“专属”空间中“自由”生活。</p>
<a id="more"></a>
<h2 id="始">始</h2>
<p>新颖的事物总能在最初的时刻吸引好奇的目光，但即便它再精致华丽，也大都会因日久生乏味而迎来退散的人潮。</p>
<p>最后一位舍友拉着行李走出了宿舍，作为初试者的自己也全然不知接下来的40天会有怎样的故事来填充生活，只是想到可以体验“真正”的成年人独居生活就满心欢喜。机械键盘随意地敲出节奏，音响外放着喜欢的音乐，灯光只按自己的时间表作息。狭小的寝室空间里挤满了自由的字样，我身处其中，随性而活。</p>
<p>生活逐渐循环往复，曾以为的趣事给予的多巴胺似乎也在线性消淡。思维总爱东奔西跑，它也开始忧心自己未来是否一直这样单调。惊慌把储蓄数天的存乐罐打翻，忧虑把原本红色的神经染成蓝色，难道始终有趣事物只是海市蜃楼般存在吗？</p>
<h2 id="思">思</h2>
<p>时光从不为单个人驻足，每个人都乘着路过拼命地向其中塞满经历。少数轰轰烈烈为世人所铭记，大多平平淡淡连自己都忘却。</p>
<p>滴答清单上计划着每日的生活，但偶然翻阅历史，发现还是那些熟悉的条目：“代码”、“论文”、“电影”、“跑步”......前天写代码、看电影；昨天读论文、跑跑步.....难道生活真的就只是这些事情的排列组合吗？我试图回忆自己刚过的数天，发现原本清晰完整的记忆已被雨水冲刷殆尽。难道平凡的生活中真的就没有值得自己珍藏的画面吗？我开始有点心悸：需要多少爱好才能让生活多彩，需要多努力记忆才能不淡忘？</p>
<h2 id="悟">悟</h2>
<blockquote class="blockquote-center">
<p>概念之下藏有形象之地</p>

</blockquote>
<p>或许这世间本就不存在昼夜耀眼的事物，但这也不意味着平凡人生的平淡生活必定庸碌无奇。或许值得铭记于心的记忆一直都在发生，只是因那些概括性的标签而被自己忽略。</p>
<p>我试图缘着回忆里所剩无几的痕迹，探寻那时或许值得珍藏的碎片。同样是写代码，某一天我重构了我的暑期实训项目，那是我第一个全栈开发的项目；而另一天我写出了完整的A3C算法，那是我学强化学习以来所构建第一个完整的模型......同样是看电影，有一天《天气之子》播放在屏幕上，依旧是我很欣赏的新海诚画风；有一天我重温了《越光宝盒》，被无厘头的情节逗得哈哈大笑......掀开“代码”、“电影”这些抽象概念的面纱，原来那些我一直寻找着的记忆光点一直栖息于此。美好事物并非永恒，亦未绝迹，它们始终都在每个人的生活里闪耀着。无需把目光扩宽或是聚焦，只轻轻抚去那层笼统概念的遮蔽，便发现我们平凡的生活亦熠熠生辉。</p>
<h2 id="行">行</h2>
<blockquote class="blockquote-center">
<p>时间流淌不息，记忆刻痕于石</p>

</blockquote>
<p>我们穷极一生追求快乐与意义。有人生而不凡，一举一动皆会载入史书；有人生而平凡，奋斗不渝依旧默默无闻。所幸我们都拥有自己的记忆，不必乞求他人的旁观，亦可留痕于时间。所幸我们都拥有自己的故事，不必渴望他人的相伴，亦可闪烁于回忆。</p>
<p>沙滩上的足迹总有一天会被冲散，我们无力阻拦时间海浪，但可以将记忆的痕迹加深。将路途中每份值得铭记的感动和泪水镌刻于石，置于沙滩。哪怕只自己一人在乎与阅读，它们都始终躺在海滩上，愈冲刷愈闪亮。或许偶尔有人途径此处，不经意地捡起其中一块无奇的石头，却好奇地细瞥其上的文字，记录的故事或恰可给予他一些帮助。</p>
<p>每个人都有独家记忆值得被刻录于石。时光流逝不止，或骇浪冲击，或风吹日晒，它们却日益生辉。</p>
<h2 id="愿">愿</h2>
<p>此后某天，携着日历上模糊的记忆，我浏览起自己的博客。捡起一块对应日期的石块，迎着海风阅读石上字痕，原来一些回忆不曾随时间降温，重温依旧炽热。</p>
]]></content>
      <categories>
        <category>MY - 感悟随笔</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】VNF placement optimization with DLR</title>
    <url>/2020/08/20/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/paper-nfv-vnf-placement-optimization-drl/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>本篇论文对资源消耗成本最小化的强化学习Policy Gradient算法，结合Seq2Seq模型，提出了一种虚拟网络服务放置的优化算法。</p>
<h2 id="论文简介">论文简介</h2>
<p><strong>论文名称</strong>：Virtual Network Function placement optimization with Deep Reinforcement Learning<br />
<strong>论文作者</strong>：Ruben Solozabal, Josu Ceberio, Aitor Sanchoyerto, Luis Zabala, Bego Blanco, Fidel Liberal<br />
<strong>发表期刊</strong>：JSAC-2020 (CCF-A)<br />
<strong>研究方向</strong>：NFV 网络功能虚拟化<br />
<strong>关键技术</strong>：NFV 网络功能虚拟化, RL 强化学习, Seq2Seq<br />
<strong>主要创新</strong>：以最小化资源消耗为目标，并用策略梯度算法结合Seq2Seq来搭建模型<br />
<span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg5NDUyOTE=">论文地址<i class="fa fa-external-link-alt"></i></span></p>
<a id="more"></a>
<h2 id="问题定义">问题定义</h2>
<h3 id="专业词汇">专业词汇</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">缩写</th>
<th>描述</th>
<th>全名</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">NFV</td>
<td>网络功能虚拟化</td>
<td>Network Function Virtualization</td>
</tr>
<tr class="even">
<td style="text-align: center;">VNF</td>
<td>虚拟网络请求</td>
<td>Virtual Network Functions</td>
</tr>
<tr class="odd">
<td style="text-align: center;">NS</td>
<td>网络服务</td>
<td>Network Service</td>
</tr>
<tr class="even">
<td style="text-align: center;">VNF-FGE</td>
<td>VNF正向图嵌入问题</td>
<td>VNF Forward Graph Embedding problem</td>
</tr>
</tbody>
</table>
<h3 id="vnf-fge">VNF-FGE</h3>
<p><img data-src="/resource/images/rp-vnf-placement-optimization-with-dlr-1.webp" /></p>
<p><img data-src="/resource/images/rp-vnf-placement-optimization-with-dlr-2.webp" /></p>
<p>对于一组网络服务，</p>
<ul>
<li>它必须被最优地被放置在一组主机服务器上，即 <span class="math inline">\(h \in H\)</span></li>
<li>满足主机服务器在计算、存储链路容量等方面的现在，即 <span class="math inline">\(s \in S\)</span></li>
</ul>
<p>问题目的：最小化对底层资源的消耗</p>
<p><span class="math inline">\(H = \{ h_1, h_2, \dots, h_n \}\)</span> 主机服务器<br />
<span class="math inline">\(V\)</span> 可用的VNF<br />
<span class="math inline">\(m \in \{1, \dots, M\}\)</span> 一系列VNF组成的网络服务 <span class="math inline">\(s \in \{f_1, f_2, \dots, f_m \}\)</span> ，且<span class="math inline">\(f \in M\)</span> 一条服务链<br />
<span class="math inline">\(S\)</span> 所有的服务链组合</p>
<p><span class="math inline">\(x \in \{0, 1\}^{m\times n}\)</span></p>
<ul>
<li><span class="math inline">\(x_{fh}\)</span> 表示功能 <span class="math inline">\(f \in V\)</span> 是否被放置在主机 <span class="math inline">\(h \in H\)</span> 中（1放置，0未放置）</li>
</ul>
<p>动作搜索路径：<span class="math inline">\(\Omega = \{0,1\}^{m \times n}\)</span> s.t. <span class="math inline">\(\left.\sum_{h} x_{f h}=1 \forall f \in s\right\}\)</span></p>
<ul>
<li>对于一条服务链，它只能放置在一个主机一次</li>
</ul>
<p>辅助变量</p>
<ul>
<li><span class="math inline">\(y_h \in \{0,1\}\)</span>：服务器激活变量。1代表服务器正在执行VNF，0反之</li>
<li><span class="math inline">\(g_i \in \{0,1\}\)</span>：链路激活变量。1代表链路正在承载流量，0反之</li>
</ul>
<p>功耗相关</p>
<ul>
<li>服务器主机功耗 <span class="math inline">\(W_h^{cpu}\)</span>
<ul>
<li>激活运行时（<span class="math inline">\(y_h = 1\)</span>）的最低功耗为 <span class="math inline">\(W_h^{min}\)</span></li>
<li>功耗随着VNF的CPU需求总和而增加（线性增长）</li>
</ul></li>
<li>链路消耗 <span class="math inline">\(W_{net}\)</span>
<ul>
<li>带宽利用量 x 单位成本</li>
</ul></li>
</ul>
<p>可用资源 <span class="math inline">\(r \in R\)</span></p>
<p><img data-src="/resource/images/rp-vnf-placement-optimization-with-dlr-3.webp" /></p>
<h2 id="算法模型">算法模型</h2>
<p><img data-src="/resource/images/rp-vnf-placement-optimization-with-dlr-4.webp" /></p>
<p><img data-src="/resource/images/rp-vnf-placement-optimization-with-dlr-5.webp" /></p>
<h2 id="性能评估">性能评估</h2>
<h3 id="设计实验">设计实验</h3>
<h3 id="对比算法">对比算法</h3>
<h3 id="评估指标">评估指标</h3>
<h2 id="主要创新">主要创新</h2>
<ul>
<li>基于RL+GCN的自动虚拟网络嵌入算法</li>
<li>并行的策略梯度训练方法</li>
<li>多指标的奖励函数</li>
</ul>
<h2 id="总结思考">总结思考</h2>
<ul>
<li>GCN较CNN可以更好地提取非欧数据的特征</li>
<li>A3C算法较其他RL算法性能表现更佳</li>
<li>并行的梯度训练策略更适应于实际场景</li>
<li>多指标的Reward可以让Agent学习到更好地策略来提高收益</li>
<li>在模型评估部分，实验的设计比较完善</li>
</ul>
]]></content>
      <categories>
        <category>RP - 科研论文</category>
      </categories>
      <tags>
        <tag>Paper</tag>
        <tag>NFV</tag>
        <tag>DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>【SciPy】Sparse稀疏矩阵主要存储格式总结</title>
    <url>/2020/08/18/DA%20-%20%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/da-scipy-sparse/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>在数据科学和深度学习等领域常会采用矩阵格式来存储数据，但当矩阵较为庞大且非零元素较少时，运算效率和存储有效率并不高。所以，通常我们采用Sparse稀疏矩阵的方式来存储矩阵，提高存储和运算效率。下面将对SciPy中七种常见的存储方式（COO/ CSR/ CSC/ BSR/ DOK/ LIL/ DIA）的概念和用法进行介绍和对比总结。</p>
<a id="more"></a>
<h2 id="稀疏矩阵简介">稀疏矩阵简介</h2>
<h3 id="稀疏矩阵">稀疏矩阵</h3>
<table>
<thead>
<tr class="header">
<th><em>numpy.sparse</em></th>
<th>稀疏矩阵</th>
<th>sparse matrix</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>numpy.ndarray</em></td>
<td>密集矩阵</td>
<td>array matrix</td>
</tr>
<tr class="even">
<td><em>numpy.matrix</em></td>
<td>密集矩阵</td>
<td>dense matrix</td>
</tr>
</tbody>
</table>
<ul>
<li>稀疏矩阵
<ul>
<li>具有少量非零项的矩阵 - <strong>N</strong>umber of <strong>N</strong>on-<strong>Z</strong>ero (<span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2Mvc2NpcHkvcmVmZXJlbmNlL2dlbmVyYXRlZC9zY2lweS5zcGFyc2Uuc3BtYXRyaXguZ2V0bm56Lmh0bWw=">NNZ<i class="fa fa-external-link-alt"></i></span>) &lt; 0.5</li>
<li>（在矩阵中，若数值0的元素数目远多于非0元素的数目，并且非0元素分布没有规律）</li>
</ul></li>
<li>矩阵的稠密度
<ul>
<li>非零元素的总数比上矩阵所有元素的总数为矩阵的稠密度。</li>
</ul></li>
</ul>
<figure>
<img data-src="/resource/images/da-scipy-sparse-dense.gif" alt="By Matt" /><figcaption aria-hidden="true"><span class="exturl" data-url="aHR0cHM6Ly9tYXR0ZWRpbmcuZ2l0aHViLmlvLzIwMTkvMDQvMjUvc3BhcnNlLW1hdHJpY2VzLw==">By Matt<i class="fa fa-external-link-alt"></i></span></figcaption>
</figure>
<h3 id="压缩存储">压缩存储</h3>
<p>存储矩阵的一般方法是采用二维数组，其优点是可以随机地访问每一个元素，因而能够容易实现矩阵的各种运算。<br />
对于稀疏矩阵，它通常具有很大的维度，有时甚大到整个矩阵（零元素）占用了绝大部分内存<br />
采用二维数组的存储方法既浪费大量的存储单元来存放零元素，又要在运算中浪费大量的时间来进行零元素的无效运算。因此必须考虑对稀疏矩阵进行压缩存储（只存储非零元素）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line">help(sparse)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Sparse Matrix Storage Formats</span></span><br><span class="line"><span class="string">There are seven available sparse matrix types:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        1. csc_matrix: Compressed Sparse Column format</span></span><br><span class="line"><span class="string">        2. csr_matrix: Compressed Sparse Row format</span></span><br><span class="line"><span class="string">        3. bsr_matrix: Block Sparse Row format</span></span><br><span class="line"><span class="string">        4. lil_matrix: List of Lists format</span></span><br><span class="line"><span class="string">        5. dok_matrix: Dictionary of Keys format</span></span><br><span class="line"><span class="string">        6. coo_matrix: COOrdinate format (aka IJV, triplet format)</span></span><br><span class="line"><span class="string">        7. dia_matrix: DIAgonal format</span></span><br><span class="line"><span class="string">        8. spmatrix: Sparse matrix base clas</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="矩阵属性">矩阵属性</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csr_matrix</span><br><span class="line"></span><br><span class="line"><span class="comment">### 共有属性</span></span><br><span class="line">mat.shape  <span class="comment"># 矩阵形状</span></span><br><span class="line">mat.dtype  <span class="comment"># 数据类型</span></span><br><span class="line">mat.ndim  <span class="comment"># 矩阵维度</span></span><br><span class="line">mat.nnz   <span class="comment"># 非零个数</span></span><br><span class="line">mat.data  <span class="comment"># 非零值, 一维数组</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### COO 特有的</span></span><br><span class="line">coo.row  <span class="comment"># 矩阵行索引</span></span><br><span class="line">coo.col  <span class="comment"># 矩阵列索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### CSR\CSC\BSR 特有的</span></span><br><span class="line">bsr.indices    <span class="comment"># 索引数组</span></span><br><span class="line">bsr.indptr     <span class="comment"># 指针数组</span></span><br><span class="line">bsr.has_sorted_indices  <span class="comment"># 索引是否排序</span></span><br><span class="line">bsr.blocksize  <span class="comment"># BSR矩阵块大小</span></span><br></pre></td></tr></table></figure>
<h3 id="通用方法">通用方法</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"></span><br><span class="line"><span class="comment">### 转换矩阵格式</span></span><br><span class="line">tobsr()、tocsr()、to_csc()、to_dia()、to_dok()、to_lil()</span><br><span class="line">mat.toarray()  <span class="comment"># 转为array</span></span><br><span class="line">mat.todense()  <span class="comment"># 转为dense</span></span><br><span class="line"><span class="comment"># 返回给定格式的稀疏矩阵</span></span><br><span class="line">mat.asformat(format)</span><br><span class="line"><span class="comment"># 返回给定元素格式的稀疏矩阵</span></span><br><span class="line">mat.astype(t)  </span><br><span class="line"></span><br><span class="line"><span class="comment">### 检查矩阵格式</span></span><br><span class="line">issparse、isspmatrix_lil、isspmatrix_csc、isspmatrix_csr</span><br><span class="line">sp.issparse(mat)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 获取矩阵数据</span></span><br><span class="line">mat.getcol(j)  <span class="comment"># 返回矩阵列j的一个拷贝，作为一个(mx 1) 稀疏矩阵 (列向量)</span></span><br><span class="line">mat.getrow(i)  <span class="comment"># 返回矩阵行i的一个拷贝，作为一个(1 x n)  稀疏矩阵 (行向量)</span></span><br><span class="line">mat.nonzero()  <span class="comment"># 非0元索引</span></span><br><span class="line">mat.diagonal()   <span class="comment"># 返回矩阵主对角元素</span></span><br><span class="line">mat.max([axis])  <span class="comment"># 给定轴的矩阵最大元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 矩阵运算</span></span><br><span class="line">mat += mat     <span class="comment"># 加</span></span><br><span class="line">mat = mat * <span class="number">5</span>  <span class="comment"># 乘</span></span><br><span class="line">mat.dot(other)  <span class="comment"># 坐标点积</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">resize(self, *shape)</span><br><span class="line">transpose(self[, axes, copy])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr />
<h2 id="稀疏矩阵分类">稀疏矩阵分类</h2>
<h3 id="coo---coo_matrix">COO - <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2Mvc2NpcHkvcmVmZXJlbmNlL2dlbmVyYXRlZC9zY2lweS5zcGFyc2UuY29vX21hdHJpeC5odG1s">coo_matrix<i class="fa fa-external-link-alt"></i></span></h3>
<h4 id="coordinate-matrix-对角存储矩阵">Coordinate Matrix 对角存储矩阵</h4>
<ul>
<li>采用三元组<code>(row, col, data)</code>(或称为ijv format)的形式来存储矩阵中非零元素的信息</li>
<li>三个数组 <code>row</code> 、<code>col</code> 和 <code>data</code> 分别保存非零元素的行下标、列下标与值（一般长度相同）</li>
<li>故 <code>coo[row[k]][col[k]] = data[k]</code> ，即矩阵的第 <code>row[k]</code> 行、第 <code>col[k]</code> 列的值为 <code>data[k]</code> </li>
</ul>
<p><img data-src="/resource/images/da-scipy-sparse-coo.gif" /></p>
<ul>
<li>当 <code>row[0] = 1</code> , <code>column[0] = 1</code> 时， <code>data[0] = 2</code> ，故 <code>coo[1][1] = 2</code></li>
<li>当 <code>row[3] = 0</code> , <code>column[3] = 2</code> 时， <code>data[3] = 9</code> ，故 <code>coo[0][3] = 9</code></li>
</ul>
<h4 id="适用场景">适用场景</h4>
<ul>
<li>主要用来创建矩阵，因为coo_matrix无法对矩阵的元素进行增删改等操作</li>
<li>一旦创建之后，除了将之转换成其它格式的矩阵，几乎无法对其做任何操作和矩阵运算</li>
</ul>
<h4 id="优缺点">优缺点</h4>
<h5 id="优点">①优点</h5>
<ul>
<li>转换成其它存储格式很快捷简便（<code>tobsr()</code>、<code>tocsr()</code>、<code>to_csc()</code>、<code>to_dia()</code>、<code>to_dok()</code>、<code>to_lil()</code>）</li>
<li>能与CSR / CSC格式的快速转换</li>
<li>允许重复的索引（例如在1行1列处存了值2.0，又在1行1列处存了值3.0，则转换成其它矩阵时就是2.0+3.0=5.0）</li>
</ul>
<h5 id="缺点">②缺点</h5>
<ul>
<li>不支持切片和算术运算操作</li>
<li>如果稀疏矩阵<strong>仅包含非0元素的对角线</strong>，则对角存储格式(DIA)可以减少非0元素定位的信息量</li>
<li>这种存储格式对有限元素或者有限差分离散化的矩阵尤其有效</li>
</ul>
<h4 id="实例化方法">实例化方法</h4>
<ul>
<li><code>coo_matrix(D)</code>：D代表密集矩阵；</li>
<li><code>coo_matrix(S)</code>：S代表其他类型稀疏矩阵</li>
<li><code>coo_matrix((M, N), [dtype])</code>：构建一个shape为M*N的空矩阵，默认数据类型是<code>d</code>，</li>
<li><code>coo_matrix((data, (i, j)), [shape=(M, N)]))</code>：三元组初始化
<ul>
<li><code>i[:]</code> : 行索引</li>
<li><code>j[:]</code> : 列索引</li>
<li><code>A[i[k], j[k]]=data[k]</code></li>
</ul></li>
</ul>
<h4 id="特殊属性">特殊属性</h4>
<ul>
<li><code>data</code>：稀疏矩阵存储的值，是一个一维数组</li>
<li><code>row</code>：与<code>data</code>同等长度的一维数组，表征<code>data</code>中每个元素的行号</li>
<li><code>col</code>：与<code>data</code>同等长度的一维数组，表征<code>data</code>中每个元素的列号</li>
</ul>
<h4 id="代码示例">代码示例</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据</span></span><br><span class="line">row = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">col = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成coo格式的矩阵</span></span><br><span class="line"><span class="comment"># &lt;class &#x27;scipy.sparse.coo.coo_matrix&#x27;&gt;</span></span><br><span class="line">coo_mat = sparse.coo_matrix((data, (row, col)), shape=(<span class="number">4</span>, <span class="number">4</span>),  dtype=np.int)</span><br><span class="line"></span><br><span class="line"><span class="comment"># coordinate-value format</span></span><br><span class="line">print(coo)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(0, 0)        1</span></span><br><span class="line"><span class="string">(1, 1)        2</span></span><br><span class="line"><span class="string">(2, 2)        3</span></span><br><span class="line"><span class="string">(3, 3)        4</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据</span></span><br><span class="line">coo.data</span><br><span class="line">coo.row</span><br><span class="line">coo.col</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转化array</span></span><br><span class="line"><span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line">coo_mat.toarray()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[1, 0, 0, 0],</span></span><br><span class="line"><span class="string">       [0, 2, 0, 0],</span></span><br><span class="line"><span class="string">       [0, 0, 3, 4],</span></span><br><span class="line"><span class="string">       [0, 0, 0, 0]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="csr---csr_matrix">CSR - <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2Mvc2NpcHkvcmVmZXJlbmNlL2dlbmVyYXRlZC9zY2lweS5zcGFyc2UuY3NyX21hdHJpeC5odG1s">csr_matrix<i class="fa fa-external-link-alt"></i></span></h3>
<h4 id="compressed-sparse-row-matrix-压缩稀疏行格式">Compressed Sparse Row Matrix 压缩稀疏行格式</h4>
<ul>
<li>csr_matrix是按行对矩阵进行压缩的</li>
<li>通过 <code>indices</code>, <code>indptr</code>，<code>data</code> 来确定矩阵。
<ul>
<li><code>data</code> 表示矩阵中的非零数据</li>
<li>对于第 <code>i</code> 行而言，该行中非零元素的列索引为 <code>indices[indptr[i]:indptr[i+1]]</code></li>
<li>可以将 <code>indptr</code> 理解成利用其自身索引 <code>i</code> 来指向第 <code>i</code> 行元素的列索引</li>
<li>根据<code>[indptr[i]:indptr[i+1]]</code>，我就得到了该行中的非零元素个数，如
<ul>
<li>若 <code>index[i] = 3</code> 且 <code>index[i+1] = 3</code> ，则第 <code>i</code> 行的没有非零元素</li>
<li>若 <code>index[j] = 6</code> 且 <code>index[j+1] = 7</code> ，则第 <code>j</code> 行的非零元素的列索引为 <code>indices[6:7]</code></li>
</ul></li>
<li>得到了行索引、列索引，相应的数据存放在： <code>data[indptr[i]:indptr[i+1]]</code></li>
</ul></li>
</ul>
<p><img data-src="/resource/images/da-scipy-sparse-csr.gif" /></p>
<ul>
<li>对于矩阵第 <code>0</code> 行，我们需要先得到其非零元素列索引
<ul>
<li>由 <code>indptr[0] = 0</code> 和 <code>indptr[1] = 2</code> 可知，第 <code>0</code> 行有两个非零元素。</li>
<li>它们的列索引为 <code>indices[0:2] = [0, 2]</code> ，且存放的数据为 <code>data[0] = 8</code> ， <code>data[1] = 2</code> </li>
<li>因此矩阵第 <code>0</code> 行的非零元素 <code>csr[0][0] = 8</code> 和 <code>csr[0][2] = 2</code> </li>
</ul></li>
<li>对于矩阵第 <code>4</code> 行，同样我们需要先计算其非零元素列索引
<ul>
<li>由 <code>indptr[4] = 3</code> 和 <code>indptr[5] = 6</code> 可知，第 <code>4</code> 行有3个非零元素。</li>
<li>它们的列索引为 <code>indices[3:6] = [2, 3，4]</code> ，且存放的数据为 <code>data[3] = 7</code> ，<code>data[4] = 1</code> ，<code>data[5] = 2</code></li>
<li>因此矩阵第 <code>4</code> 行的非零元素 <code>csr[4][2] = 7</code> ， <code>csr[4][3] = 1</code> 和 <code>csr[4][4] = 2</code></li>
</ul></li>
</ul>
<h4 id="适用场景-1">适用场景</h4>
<ul>
<li>常用于读入数据后进行稀疏矩阵计算，运算高效</li>
</ul>
<h4 id="优缺点-1">优缺点</h4>
<h5 id="优点-1">①优点</h5>
<ul>
<li>高效的稀疏矩阵算术运算</li>
<li>高效的行切片</li>
<li>快速地矩阵矢量积运算</li>
</ul>
<h5 id="缺点-1">②缺点</h5>
<ul>
<li>较慢地列切片操作（可以考虑CSC）</li>
<li>转换到稀疏结构代价较高（可以考虑LIL，DOK）</li>
</ul>
<h4 id="实例化">实例化</h4>
<ul>
<li><code>csr_matrix(D)</code>：D代表密集矩阵；</li>
<li><code>csr_matrix(S)</code>：S代表其他类型稀疏矩阵</li>
<li><code>csr_matrix((M, N), [dtype])</code>：构建一个shape为M*N的空矩阵，默认数据类型是<code>d</code>，</li>
<li><code>csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)]))</code>
<ul>
<li>三者关系：<code>a[row_ind[k], col_ind[k]] = data[k]</code></li>
</ul></li>
<li><code>csr_matrix((data, indices, indptr), [shape=(M, N)])</code>
<ul>
<li>第i行的列索引存储在其中<code>indices[indptr[i]:indptr[i+1]]</code></li>
<li>其对应值存储在中<code>data[indptr[i]:indptr[i+1]]</code></li>
</ul></li>
</ul>
<h4 id="特殊属性-1">特殊属性</h4>
<ul>
<li><code>data</code> ：稀疏矩阵存储的值，一维数组</li>
<li><code>indices</code> ：存储矩阵有有非零值的列索引</li>
<li><code>indptr</code> ：类似指向列索引的指针数组</li>
<li><code>[has_sorted_indices](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.bsr_matrix.has_sorted_indices.html#scipy.sparse.bsr_matrix.has_sorted_indices)</code>：索引 <code>indices</code> 是否排序</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">indptr = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line">indices = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">data = np.array([<span class="number">8</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建矩阵</span></span><br><span class="line">csr = sparse.csr_matrix((data, indices, indptr))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转为array</span></span><br><span class="line">csr.toarray()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[1, 0, 2],</span></span><br><span class="line"><span class="string">       [0, 0, 3],</span></span><br><span class="line"><span class="string">       [4, 5, 6]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按row行来压缩</span></span><br><span class="line"><span class="comment"># 对于第i行，非0数据列是indices[indptr[i]:indptr[i+1]] 数据是data[indptr[i]:indptr[i+1]]</span></span><br><span class="line"><span class="comment"># 在本例中</span></span><br><span class="line"><span class="comment"># 第0行，有非0的数据列是indices[indptr[0]:indptr[1]] = indices[0:2] = [0,2]</span></span><br><span class="line"><span class="comment"># 数据是data[indptr[0]:indptr[1]] = data[0:2] = [1,2],所以在第0行第0列是1，第2列是2</span></span><br><span class="line"><span class="comment"># 第1行，有非0的数据列是indices[indptr[1]:indptr[2]] = indices[2:3] = [2]</span></span><br><span class="line"><span class="comment"># 数据是data[indptr[1]:indptr[2] = data[2:3] = [3],所以在第1行第2列是3</span></span><br><span class="line"><span class="comment"># 第2行，有非0的数据列是indices[indptr[2]:indptr[3]] = indices[3:6] = [0,1,2]</span></span><br><span class="line"><span class="comment"># 数据是data[indptr[2]:indptr[3]] = data[3:6] = [4,5,6],所以在第2行第0列是4，第1列是5,第2列是6</span></span><br></pre></td></tr></table></figure>
<h3 id="csc---csc_matrix">CSC - <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2Mvc2NpcHkvcmVmZXJlbmNlL2dlbmVyYXRlZC9zY2lweS5zcGFyc2UuY3NjX21hdHJpeC5odG1s">csc_matrix<i class="fa fa-external-link-alt"></i></span></h3>
<h4 id="compressed-sparse-column-matrix-压缩稀疏列矩阵">Compressed Sparse Column Matrix 压缩稀疏列矩阵</h4>
<ul>
<li>csc_matrix是按列对矩阵进行压缩的</li>
<li>通过 <code>indices</code>, <code>indptr</code>，<code>data</code> 来确定矩阵，可以对比CSR
<ul>
<li><code>data</code> 表示矩阵中的非零数据</li>
<li>对于第 <code>i</code> 列而言，该行中非零元素的行索引为<code>indices[indptr[i]:indptr[i+1]]</code></li>
<li>可以将 <code>indptr</code> 理解成利用其自身索引 <code>i</code> 来指向第 <code>i</code> 列元素的列索引</li>
<li>根据<code>[indptr[i]:indptr[i+1]]</code>，我就得到了该行中的非零元素个数，如
<ul>
<li>若 <code>index[i] = 1</code> 且 <code>index[i+1] = 1</code> ，则第 <code>i</code> 列的没有非零元素</li>
<li>若 <code>index[j] = 4</code> 且 <code>index[j+1] = 6</code> ，则第 <code>j</code>列的非零元素的行索引为 <code>indices[4:6]</code></li>
</ul></li>
<li>得到了列索引、行索引，相应的数据存放在： <code>data[indptr[i]:indptr[i+1]]</code></li>
</ul></li>
</ul>
<p><img data-src="/resource/images/da-scipy-sparse-csc.gif" /></p>
<ul>
<li>对于矩阵第 <code>0</code> 列，我们需要先得到其非零元素行索引
<ul>
<li>由 <code>indptr[0] = 0</code> 和 <code>indptr[1] = 1</code> 可知，第 <code>0</code>列行有1个非零元素。</li>
<li>它们的行索引为 <code>indices[0:1] = [0]</code> ，且存放的数据为 <code>data[0] = 8</code></li>
<li>因此矩阵第 <code>0</code> 行的非零元素 <code>csc[0][0] = 8</code></li>
</ul></li>
<li>对于矩阵第 <code>3</code> 列，同样我们需要先计算其非零元素行索引
<ul>
<li>由 <code>indptr[3] = 4</code> 和 <code>indptr[4] = 6</code> 可知，第 <code>4</code> 行有2个非零元素。</li>
<li>它们的行索引为 <code>indices[4:6] = [4, 6]</code> ，且存放的数据为 <code>data[4] = 1</code> ，<code>data[5] = 9</code></li>
<li>因此矩阵第 <code>i</code> 行的非零元素 <code>csr[4][3] = 1</code> ， <code>csr[6][3] = 9</code></li>
</ul></li>
</ul>
<h4 id="应用场景">应用场景</h4>
<p>参考CSR</p>
<h4 id="优缺点-2">优缺点</h4>
<p>对比参考CSR</p>
<h4 id="实例化-1">实例化</h4>
<ul>
<li><code>csc_matrix(D)</code>：D代表密集矩阵；</li>
<li><code>csc_matrix(S)</code>：S代表其他类型稀疏矩阵</li>
<li><code>csc_matrix((M, N), [dtype])</code>：构建一个shape为M*N的空矩阵，默认数据类型是<code>d</code>，</li>
<li><code>csc_matrix((data, (row_ind, col_ind)), [shape=(M, N)]))</code>
<ul>
<li>三者关系：<code>a[row_ind[k], col_ind[k]] = data[k]</code></li>
</ul></li>
<li><code>csc_matrix((data, indices, indptr), [shape=(M, N)])</code>
<ul>
<li>第i列的列索引存储在其中<code>indices[indptr[i]:indptr[i+1]]</code></li>
<li>其对应值存储在中<code>data[indptr[i]:indptr[i+1]]</code></li>
</ul></li>
</ul>
<h4 id="特殊属性-2">特殊属性</h4>
<ul>
<li><code>data</code> ：稀疏矩阵存储的值，一维数组</li>
<li><code>indices</code> ：存储矩阵有有非零值的行索引</li>
<li><code>indptr</code> ：类似指向列索引的指针数组</li>
<li><code>[has_sorted_indices](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.bsr_matrix.has_sorted_indices.html#scipy.sparse.bsr_matrix.has_sorted_indices)</code>：索引 <code>indices</code> 是否排序</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">row = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">col = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">data = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建矩阵</span></span><br><span class="line">csc = sparse.csc_matrix((data, (row, col)), shape=(<span class="number">3</span>, <span class="number">3</span>)).toarray()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转为array</span></span><br><span class="line">csc.toarray()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[1, 0, 4],</span></span><br><span class="line"><span class="string">       [0, 0, 5],</span></span><br><span class="line"><span class="string">       [2, 3, 6]], dtype=int64)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按col列来压缩</span></span><br><span class="line"><span class="comment"># 对于第i列，非0数据行是indices[indptr[i]:indptr[i+1]] 数据是data[indptr[i]:indptr[i+1]]</span></span><br><span class="line"><span class="comment"># 在本例中</span></span><br><span class="line"><span class="comment"># 第0列，有非0的数据行是indices[indptr[0]:indptr[1]] = indices[0:2] = [0,2]</span></span><br><span class="line"><span class="comment"># 数据是data[indptr[0]:indptr[1]] = data[0:2] = [1,2],所以在第0列第0行是1，第2行是2</span></span><br><span class="line"><span class="comment"># 第1行，有非0的数据行是indices[indptr[1]:indptr[2]] = indices[2:3] = [2]</span></span><br><span class="line"><span class="comment"># 数据是data[indptr[1]:indptr[2] = data[2:3] = [3],所以在第1列第2行是3</span></span><br><span class="line"><span class="comment"># 第2行，有非0的数据行是indices[indptr[2]:indptr[3]] = indices[3:6] = [0,1,2]</span></span><br><span class="line"><span class="comment"># 数据是data[indptr[2]:indptr[3]] = data[3:6] = [4,5,6],所以在第2列第0行是4，第1行是5,第2行是6</span></span><br></pre></td></tr></table></figure>
<h3 id="bsr---bsr_matrix">BSR - <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2Mvc2NpcHkvcmVmZXJlbmNlL2dlbmVyYXRlZC9zY2lweS5zcGFyc2UuYnNyX21hdHJpeC5odG1s">bsr_matrix<i class="fa fa-external-link-alt"></i></span></h3>
<h4 id="block-sparse-row-matrix-分块压缩稀疏行格式">Block Sparse Row Matrix 分块压缩稀疏行格式</h4>
<ul>
<li>基于行的块压缩，与csr类似，都是通过<code>data</code>，<code>indices</code>，<code>indptr</code>来确定矩阵</li>
<li>与csr相比，只是data中的元数据由0维的数变为了一个矩阵（块），其余完全相同</li>
<li>块大小 <code>blocksize</code>
<ul>
<li>块大小 <code>(R, C)</code> 必须均匀划分矩阵 <code>(M, N)</code> 的形状。</li>
<li>R和C必须满足关系：<code>M % R = 0</code> 和 <code>N % C = 0</code></li>
</ul></li>
</ul>
<h4 id="应用场景-1">应用场景</h4>
<p>参考CSR</p>
<h4 id="优缺点-3">优缺点</h4>
<h5 id="优点-2">①优点</h5>
<ul>
<li>与csr很类似</li>
<li>更适合于适用于具有密集子矩阵的稀疏矩阵</li>
<li>在某些情况下比csr和csc计算更高效。</li>
</ul>
<h4 id="实例化-2">实例化</h4>
<ul>
<li><code>bsr_matrix(D)</code>：D代表密集矩阵；</li>
<li><code>bsr_matrix(S)</code>：S代表其他类型稀疏矩阵</li>
<li><code>bsr_matrix((M, N), [blocksize =(R,C), [dtype])</code>：构建一个shape为M*N的空矩阵，默认数据类型是<code>d</code>，</li>
<li><code>(data, ij), [blocksize=(R,C), shape=(M, N)]</code>
<ul>
<li>两者关系：<code>a[ij[0,k], ij[1,k]] = data[k]]</code></li>
</ul></li>
<li><code>bsr_matrix((data, indices, indptr), [shape=(M, N)])</code>
<ul>
<li>第i行的块索引存储在其中<code>indices[indptr[i]:indptr[i+1]]</code></li>
<li>其相应块值存储在中<code>data[indptr[i]:indptr[i+1]]</code></li>
</ul></li>
</ul>
<h4 id="特殊属性-3">特殊<strong>属性</strong></h4>
<ul>
<li><code>data</code> ：稀疏矩阵存储的值，一维数组</li>
<li><code>indices</code> ：存储矩阵有有非零值的列索引</li>
<li><code>indptr</code> ：类似指向列索引的指针数组</li>
<li><code>blocksize</code> ：矩阵的块大小</li>
<li><code>[has_sorted_indices](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.bsr_matrix.has_sorted_indices.html#scipy.sparse.bsr_matrix.has_sorted_indices)</code>：索引 <code>indices</code> 是否排序</li>
</ul>
<h4 id="代码示例-1">代码示例</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">indptr = np.array([<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">6</span>])</span><br><span class="line">indices = np.array([<span class="number">0</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">data = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]).repeat(<span class="number">4</span>).reshape(<span class="number">6</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建矩阵</span></span><br><span class="line">bsr = bsr_matrix((data, indices, indptr), shape=(<span class="number">6</span>,<span class="number">6</span>)).todense()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转为array</span></span><br><span class="line">bsr.todense()</span><br><span class="line">matrix([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="dok--dok_matrix">DOK- <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2Mvc2NpcHkvcmVmZXJlbmNlL2dlbmVyYXRlZC9zY2lweS5zcGFyc2UuZG9rX21hdHJpeC5odG1s">dok_matrix<i class="fa fa-external-link-alt"></i></span></h3>
<h4 id="dictionary-of-keys-matrix-按键字典矩阵">Dictionary of Keys Matrix 按键字典矩阵</h4>
<ul>
<li>采用字典来记录矩阵中不为0的元素</li>
<li>字典的 <code>key</code> 存的是记录元素的位置信息的元组， <code>value</code> 是记录元素的具体值</li>
</ul>
<h4 id="适用场景-2">适用场景</h4>
<ul>
<li>逐渐添加矩阵的元素</li>
</ul>
<h4 id="优缺点-4">优缺点</h4>
<h5 id="优点-3">①优点</h5>
<ul>
<li>对于递增的构建稀疏矩阵很高效，比如定义该矩阵后，想进行每行每列更新值，可用该矩阵。</li>
<li>可以高效访问单个元素，只需要O(1)</li>
</ul>
<h5 id="缺点-2">②缺点</h5>
<ul>
<li>不允许重复索引（coo中适用），但可以很高效的转换成coo后进行重复索引</li>
</ul>
<h4 id="实例化方法-1">实例化方法</h4>
<ul>
<li><code>dok_matrix(D)</code>：D代表密集矩阵；</li>
<li><code>dok_matrix(S)</code>：S代表其他类型稀疏矩阵</li>
<li><code>dok_matrix((M, N), [dtype])</code>：构建一个shape为M*N的空矩阵，默认数据类型是<code>d</code>，</li>
</ul>
<h4 id="代码示例-2">代码示例</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dok = sparse.dok_matrix((<span class="number">5</span>, <span class="number">5</span>), dtype=np.float32)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        dok[i,j] = i+j    <span class="comment"># 更新元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># zero elements are accessible</span></span><br><span class="line">dok[(<span class="number">0</span>, <span class="number">0</span>)]  <span class="comment"># = 0</span></span><br><span class="line"></span><br><span class="line">dok.keys()</span><br><span class="line"><span class="comment"># &#123;(0, 0), ..., (4, 4)&#125;</span></span><br><span class="line"></span><br><span class="line">dok.toarray()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[0. 1. 2. 3. 4.]</span></span><br><span class="line"><span class="string"> [1. 2. 3. 4. 5.]</span></span><br><span class="line"><span class="string"> [2. 3. 4. 5. 6.]</span></span><br><span class="line"><span class="string"> [3. 4. 5. 6. 7.]</span></span><br><span class="line"><span class="string"> [4. 5. 6. 7. 8.]]</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="lil---lil_matrix">LIL - <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2Mvc2NpcHkvcmVmZXJlbmNlL2dlbmVyYXRlZC9zY2lweS5zcGFyc2UubGlsX21hdHJpeC5odG1s">lil_matrix<i class="fa fa-external-link-alt"></i></span></h3>
<h4 id="linked-list-matrix-链表矩阵">Linked List Matrix 链表矩阵</h4>
<ul>
<li>使用两个列表存储非0元素data</li>
<li>rows保存非零元素所在的列</li>
<li>可以使用列表赋值来添加元素，如 <code>lil[(0, 0)] = 8</code> </li>
</ul>
<p><img data-src="/resource/images/da-scipy-sparse-lil.gif" /></p>
<ul>
<li><code>lil[(0, -1)] = 4</code> ：第0行的最后一列元素为4</li>
<li><code>lil[(4, 2)] = 5</code> ：第4行第2列的元素为5</li>
</ul>
<h4 id="适用场景-3">适用场景</h4>
<ul>
<li>适用的场景是逐渐添加矩阵的元素（且能快速获取行相关的数据）</li>
<li>需要注意的是，该方法插入一个元素最坏情况下可能导致线性时间的代价，所以要确保对每个元素的索引进行预排序</li>
</ul>
<h4 id="优缺点-5">优缺点</h4>
<h5 id="优点-4">①优点</h5>
<ul>
<li>适合递增的构建成矩阵</li>
<li>转换成其它存储方式很高效</li>
<li>支持灵活的切片</li>
</ul>
<h5 id="缺点-3">②缺点</h5>
<ul>
<li>当矩阵很大时，考虑用coo</li>
<li>算术操作，列切片，矩阵向量内积操作慢</li>
</ul>
<h4 id="实例化方法-2">实例化方法</h4>
<ul>
<li><code>lil_matrix(D)</code>：D代表密集矩阵；</li>
<li><code>lil_matrix(S)</code>：S代表其他类型稀疏矩阵</li>
<li><code>lil_matrix((M, N), [dtype])</code>：构建一个shape为M*N的空矩阵，默认数据类型是<code>d</code></li>
</ul>
<h4 id="特殊属性-4">特殊属性</h4>
<ul>
<li><strong><code>data</code></strong>：存储矩阵中的非零数据</li>
<li><strong><code>rows</code></strong>：存储每个非零元素所在的列（行信息为列表中索引所表示）</li>
</ul>
<h4 id="代码示例-3">代码示例</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建矩阵</span></span><br><span class="line">lil = sparse.lil_matrix((<span class="number">6</span>, <span class="number">5</span>), dtype=int)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置数值</span></span><br><span class="line"><span class="comment"># set individual point</span></span><br><span class="line">lil[(<span class="number">0</span>, <span class="number">-1</span>)] = <span class="number">-1</span></span><br><span class="line"><span class="comment"># set two points</span></span><br><span class="line">lil[<span class="number">3</span>, (<span class="number">0</span>, <span class="number">4</span>)] = [<span class="number">-2</span>] * <span class="number">2</span></span><br><span class="line"><span class="comment"># set main diagonal</span></span><br><span class="line">lil.setdiag(<span class="number">8</span>, k=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set entire column</span></span><br><span class="line">lil[:, <span class="number">2</span>] = np.arange(lil.shape[<span class="number">0</span>]).reshape(<span class="number">-1</span>, <span class="number">1</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转为array</span></span><br><span class="line">lil.toarray()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[ 8,  0,  1,  0, -1],</span></span><br><span class="line"><span class="string">       [ 0,  8,  2,  0,  0],</span></span><br><span class="line"><span class="string">       [ 0,  0,  3,  0,  0],</span></span><br><span class="line"><span class="string">       [-2,  0,  4,  8, -2],</span></span><br><span class="line"><span class="string">       [ 0,  0,  5,  0,  8],</span></span><br><span class="line"><span class="string">       [ 0,  0,  6,  0,  0]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据</span></span><br><span class="line">lil.data</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([list([0, 2, 4]), list([1, 2]), list([2]), list([0, 2, 3, 4]),</span></span><br><span class="line"><span class="string">       list([2, 4]), list([2])], dtype=object)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">lil.rows</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[list([8, 1, -1])],</span></span><br><span class="line"><span class="string">       [list([8, 2])],</span></span><br><span class="line"><span class="string">       [list([3])],</span></span><br><span class="line"><span class="string">       [list([-2, 4, 8, -2])],</span></span><br><span class="line"><span class="string">       [list([5, 8])],</span></span><br><span class="line"><span class="string">       [list([6])]], dtype=object)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="dia---dia_matrix">DIA - <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2Mvc2NpcHkvcmVmZXJlbmNlL2dlbmVyYXRlZC9zY2lweS5zcGFyc2UuZGlhX21hdHJpeC5odG1s">dia_matrix<i class="fa fa-external-link-alt"></i></span></h3>
<h4 id="diagonal-matrix-对角存储格式">Diagonal Matrix 对角存储格式</h4>
<ul>
<li>dia_matrix通过两个数组确定： <code>data</code> 和 <code>offsets</code> 
<ul>
<li><code>data</code> ：对角线元素的值</li>
<li><code>offsets</code> ：第 <code>i</code> 个 <code>offsets</code> 是当前第 <code>i</code> 个对角线和主对角线的距离</li>
<li><code>data[k:]</code> 存储了 <code>offsets[k]</code> 对应的对角线的全部元素</li>
</ul></li>
</ul>
<p><img data-src="/resource/images/da-scipy-sparse-dia.gif" /></p>
<ul>
<li>当 <code>offsets[0] = 0</code> 时，表示该对角线即是主对角线，相应的值为 <code>[1, 2, 3, 4, 5]</code> </li>
<li>当 <code>offsets[2] = 2</code> 时，表示该对角线为主对角线向上偏移2个单位，相应的值为 <code>[11, 12, 13, 14, 15]</code>
<ul>
<li>但该对角线上元素仅有三个 ，于是采用先出现的元素无效的原则</li>
<li>即前两个元素对构造矩阵无效，故该对角线上的元素为 <code>[13, 14, 15]</code> </li>
</ul></li>
</ul>
<h4 id="适用场景-4">适用场景</h4>
<ul>
<li>最适合对角矩阵的存储方式</li>
</ul>
<h4 id="实例化方法-3">实例化方法</h4>
<ul>
<li><code>dia_matrix(D)</code>：D代表密集矩阵；</li>
<li><code>dia_matrix(S)</code>：S代表其他类型稀疏矩阵</li>
<li><code>dia_matrix((M, N), [dtype])</code>：构建一个shape为M*N的空矩阵，默认数据类型是<code>d</code>，</li>
<li><code>dia_matrix((data, offsets)), [shape=(M, N)]))</code>：
<ul>
<li><code>data[k,:]</code> 存储着对角偏移量为 <code>offset[k]</code> 的对角值</li>
</ul></li>
</ul>
<h4 id="特殊属性-5">特殊属性</h4>
<ul>
<li><code>data</code>：存储DIA对角值的数组</li>
<li><code>offsets</code>：存储DIA对角偏移量的数组</li>
</ul>
<h4 id="代码示例-4">代码示例</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">data = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">offsets = np.array([<span class="number">0</span>, <span class="number">-2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建矩阵</span></span><br><span class="line">dia = sparse.dia_matrix((data, offsets), shape=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据</span></span><br><span class="line">dia.data</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[[1 2 3 4]</span></span><br><span class="line"><span class="string">        [5 6 0 0]</span></span><br><span class="line"><span class="string">        [0 7 8 9]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转为array</span></span><br><span class="line">dia.toarray()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[1 7 0 0]</span></span><br><span class="line"><span class="string">       [0 2 8 0]</span></span><br><span class="line"><span class="string">       [5 0 3 9]</span></span><br><span class="line"><span class="string">       [0 6 0 4]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<hr />
<h2 id="矩阵格式对比">矩阵格式对比</h2>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;"><strong>COO</strong></th>
<th style="text-align: left;"><strong>DOK</strong></th>
<th style="text-align: left;"><strong>LIL</strong></th>
<th style="text-align: left;"><strong>CSR</strong></th>
<th style="text-align: left;"><strong>CSC</strong></th>
<th style="text-align: left;"><strong>BSR</strong></th>
<th style="text-align: left;"><strong>DIA</strong></th>
<th style="text-align: left;"><strong>Dense</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>indexing</strong></td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">no†</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">yes</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>write-only</strong></td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">yes</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>read-only</strong></td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">yes</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>low memory</strong></td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">no</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>PyData sparse</strong></td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">n/a</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="稀疏矩阵存取">稀疏矩阵存取</h2>
<h3 id="存储---save_npz">存储 - <code>save_npz</code></h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 存储为npz文件</span></span><br><span class="line">scipy.sparse.save_npz(<span class="string">&#x27;sparse_matrix.npz&#x27;</span>, sparse_matrix)</span><br></pre></td></tr></table></figure>
<h3 id="读取---load_npz">读取 - <code>load_npz</code></h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从npz文件中读取</span></span><br><span class="line">mat = sparse.load_npz(<span class="string">&#x27;./data/npz/test_x.npz&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="存储大小比较">存储大小比较</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.arange(<span class="number">100000</span>).reshape(<span class="number">1000</span>,<span class="number">100</span>)</span><br><span class="line">a[<span class="number">10</span>: <span class="number">300</span>] = <span class="number">0</span></span><br><span class="line">b = sparse.csr_matrix(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 稀疏矩阵压缩存储到npz文件</span></span><br><span class="line">sparse.save_npz(<span class="string">&#x27;b_compressed.npz&#x27;</span>, b, <span class="literal">True</span>)  <span class="comment"># 文件大小：100KB</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 稀疏矩阵不压缩存储到npz文件</span></span><br><span class="line">sparse.save_npz(<span class="string">&#x27;b_uncompressed.npz&#x27;</span>, b, <span class="literal">False</span>)  <span class="comment"># 文件大小：560KB</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 存储到普通的npy文件</span></span><br><span class="line">np.save(<span class="string">&#x27;a.npy&#x27;</span>, a)  <span class="comment"># 文件大小：391KB</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 存储到压缩的npz文件</span></span><br><span class="line">np.savez_compressed(<span class="string">&#x27;a_compressed.npz&#x27;</span>, a=a)  <span class="comment"># 文件大小：97KB• 1</span></span><br></pre></td></tr></table></figure>
<p>对于存储到npz文件中的CSR格式的稀疏矩阵，内容为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">data.npy</span><br><span class="line">format.npy</span><br><span class="line">indices.npy</span><br><span class="line">indptr.npy</span><br><span class="line">shape.npy</span><br></pre></td></tr></table></figure>
<h2 id="参考">参考</h2>
<blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2Mvc2NpcHkvcmVmZXJlbmNlL3NwYXJzZS5odG1s">Sparse matrices (scipy.sparse)<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9tYXR0ZWRpbmcuZ2l0aHViLmlvLzIwMTkvMDQvMjUvc3BhcnNlLW1hdHJpY2VzLw==">Sparse Matrices<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dpbnljZy9hcnRpY2xlL2RldGFpbHMvODA5NjcxMTI=">python稀疏矩阵的存储与表示<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2plZmZlcnkwMjA3L2FydGljbGUvZGV0YWlscy8xMDAwNjQ2MDI=">python scipy 稀疏矩阵详解<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BpcGlzb3JyeS9hcnRpY2xlL2RldGFpbHMvNDE3NjI5NDU=">SciPy教程 - 稀疏矩阵库scipy.sparse<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
]]></content>
      <categories>
        <category>DA - 数据分析</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data Science</tag>
        <tag>SciPy</tag>
      </tags>
  </entry>
  <entry>
    <title>【PPT设计】2018至2020年个人PPT设计汇总</title>
    <url>/2020/08/17/GD%20-%20%E5%B9%B3%E9%9D%A2%E8%AE%BE%E8%AE%A1/gd-ppt-conclusion-18-20/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>[更新中] 对2018至2020年所做过的PPT进行总结反思。</p>
<a id="more"></a>
<h2 id="前言">前言</h2>
<p>PPT设计所涉及到的结构和元素繁多，于是文章将按时间顺序分别从结构设计、元素和动画设计三方面来介绍。</p>
<h2 id="结构设计">结构设计</h2>
<h3 id="首页">首页</h3>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-18-wps-office-1.webp" alt="2018/11/20-大计基展演" /><figcaption aria-hidden="true">2018/11/20-大计基展演</figcaption>
</figure>
<ul>
<li>描述：这个PPT做的时间还是比较早的，当时为了契合WPS和OFFICE对比的主题，于是做的颜色和动画比较花哨</li>
<li>想法：颜色多而且都是重色彩的，过浓而影响视觉重心，可以减少些装饰元素未出现或不搭的颜色如粉、紫等等，再减少些颜色的饱和度，或许会改善些。</li>
</ul>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-srtp-1.webp" alt="2019/06/03-SRTP立项" /><figcaption aria-hidden="true">2019/06/03-SRTP立项</figcaption>
</figure>
<ul>
<li>描述：大概是第一次复现这副封面，比较适合首页信息密度较低的构图</li>
<li>想法：背景颜色稍微亮了些，右部分的构图不太稳</li>
</ul>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2019-train-4399-1.webp" alt="2019/08/30-实训结题" /><figcaption aria-hidden="true">2019/08/30-实训结题</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-1.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-english-pre-ai-1.webp" alt="2020/06/05-陈述与沟通Presentation" /><figcaption aria-hidden="true">2020/06/05-陈述与沟通Presentation</figcaption>
</figure>
<ul>
<li>这幅图还是挺喜欢的，虽然下面的文字排版不是很美观。主要新颖点在于自己复现的AI和外框是与图片高度相似的，这样为动画设计留足了空间。</li>
</ul>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-innovte-1.webp" alt="2020/06/24-市创立项" /><figcaption aria-hidden="true">2020/06/24-市创立项</figcaption>
</figure>
<h3 id="目录页">目录页</h3>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-srtp-2.webp" alt="2019/06/03-SRTP立项" /><figcaption aria-hidden="true">2019/06/03-SRTP立项</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2019-train-4399-2.webp" alt="2019/08/30-实训结题" /><figcaption aria-hidden="true">2019/08/30-实训结题</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-innovte-2.webp" alt="2020/06/24-市创立项" /><figcaption aria-hidden="true">2020/06/24-市创立项</figcaption>
</figure>
<h3 id="图示页">图示页</h3>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-srtp-3.webp" alt="2019/06/03-SRTP立项" /><figcaption aria-hidden="true">2019/06/03-SRTP立项</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2019-train-4399-4.webp" alt="2019/08/30-实训结题" /><figcaption aria-hidden="true">2019/08/30-实训结题</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-4.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-6.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-12.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-english-pre-ai-2.webp" alt="2020/06/05-陈述与沟通Presentation" /><figcaption aria-hidden="true">2020/06/05-陈述与沟通Presentation</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-english-pre-ai-5.webp" alt="2020/06/05-陈述与沟通Presentation" /><figcaption aria-hidden="true">2020/06/05-陈述与沟通Presentation</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-innovte-3.webp" alt="2020/06/24-市创立项" /><figcaption aria-hidden="true">2020/06/24-市创立项</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-innovte-4.webp" alt="2020/06/24-市创立项" /><figcaption aria-hidden="true">2020/06/24-市创立项</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-train-cquse-1.webp" alt="2020/07/09-软件综合实践结题" /><figcaption aria-hidden="true">2020/07/09-软件综合实践结题</figcaption>
</figure>
<h3 id="分点页">分点页</h3>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-srtp-4.webp" alt="2019/06/03-SRTP立项" /><figcaption aria-hidden="true">2019/06/03-SRTP立项</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2019-train-4399-5.webp" alt="2019/08/30-实训结题" /><figcaption aria-hidden="true">2019/08/30-实训结题</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2019-train-4399-8.webp" alt="2019/08/30-实训结题" /><figcaption aria-hidden="true">2019/08/30-实训结题</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-3.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-5.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-7.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-9.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-innovte-6.webp" alt="2020/06/24-市创立项" /><figcaption aria-hidden="true">2020/06/24-市创立项</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-train-cquse-2.webp" alt="2020/07/09-软件综合实践结题" /><figcaption aria-hidden="true">2020/07/09-软件综合实践结题</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-train-cquse-3.webp" alt="2020/07/09-软件综合实践结题" /><figcaption aria-hidden="true">2020/07/09-软件综合实践结题</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-train-cquse-4.webp" alt="2020/07/09-软件综合实践结题" /><figcaption aria-hidden="true">2020/07/09-软件综合实践结题</figcaption>
</figure>
<h3 id="数据页">数据页</h3>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2019-train-4399-6.webp" alt="2019/08/30-实训结题" /><figcaption aria-hidden="true">2019/08/30-实训结题</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2019-train-4399-7.webp" alt="2019/08/30-实训结题" /><figcaption aria-hidden="true">2019/08/30-实训结题</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-2.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-10.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-13.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<h3 id="对比页">对比页</h3>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-18-wps-office-2.webp" alt="2018/11/20-大计基展演" /><figcaption aria-hidden="true">2018/11/20-大计基展演</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-8.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-english-pre-ai-3.webp" alt="2020/06/05-陈述与沟通Presentation" /><figcaption aria-hidden="true">2020/06/05-陈述与沟通Presentation</figcaption>
</figure>
<h3 id="人物页">人物页</h3>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-11.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<h3 id="时间轴">时间轴</h3>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-14.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2020-innovte-5.webp" alt="2020/06/24-市创立项" /><figcaption aria-hidden="true">2020/06/24-市创立项</figcaption>
</figure>
<h3 id="结尾页">结尾页</h3>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-srtp-5.webp" alt="2019/06/03-SRTP立项" /><figcaption aria-hidden="true">2019/06/03-SRTP立项</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-2019-train-4399-9.webp" alt="2019/08/30-实训结题" /><figcaption aria-hidden="true">2019/08/30-实训结题</figcaption>
</figure>
<figure>
<img data-src="/resource/images/ppt/ad-ppt-19-hl-b-s-15.webp" alt="2020/04/09-虹霖品牌咨询" /><figcaption aria-hidden="true">2020/04/09-虹霖品牌咨询</figcaption>
</figure>
<h2 id="元素设计">元素设计</h2>
<h3 id="配色">配色</h3>
<h3 id="背景">背景</h3>
<h3 id="标题">标题</h3>
<h2 id="动画设计">动画设计</h2>
<h2 id="总结">总结</h2>
]]></content>
      <categories>
        <category>GD - 平面设计</category>
      </categories>
      <tags>
        <tag>Slide</tag>
      </tags>
  </entry>
  <entry>
    <title>【Blog】深度美化和定制Hexo和NexT方法</title>
    <url>/2020/08/16/ST%20-%20%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7/st-hexo-next-custom/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>虽然我们可以搜索到许多关于Hexo博客及NexT 8主题配置和美化的教程，但是，很多文章大都聚焦于某一些常用页面的优化和定制。随着版本更迭等情况导致框架可能出现调整时，这些聚焦于某一场景的方法可能会不再适用。本文将尽可能详细描述多个场景下的hexo及nexT定制方法并且提供通用的高度自定义方案。</p>
<a id="more"></a>
<h2 id="简介">简介</h2>
<p>Hexo的NexT主题采用njk来作为HTML预处理器，使用styl来扩展css，所以可以简单的理解成 <span class="math inline">\(html\subset njk\)</span> ，<span class="math inline">\(css \subset styl\)</span>。它们扩充了相应的功能和语法支持来更加高效的架构网页，当然，我们也完全可以使用html和css的语法来美化我们的网页。</p>
<h2 id="自定义css">自定义CSS</h2>
<p>如<span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9kb2NzL3RoZW1lLXNldHRpbmdzL2N1c3RvbS1maWxlcy5odG1s">NexT 8 文档<i class="fa fa-external-link-alt"></i></span>中所说，</p>
<blockquote>
<p>As with Data Files, you can place all custom layouts or styles in a specific location (for example: hexo/source/_data). Add the custom file to hexo/source/_data and uncomment the content under the custom_file_path section in the theme config file.</p>
</blockquote>
<p>我们可以在<code>hexo/source/_data</code>文件夹中自定义CSS/JS文件。如果想让hexo在渲染时自动引入这些文件，我们只需在<code>next/_config.yml</code>，将相应文件的注释取消。<br />
有个有趣的事情是在next主题目录文件夹下也有一个_data文件夹：<code>hexo/theme/next/source/_data</code>，官方没有给出具体介绍，如果我们将<code>next/_config.yml</code>的<code>custom_file_path</code>前加上<code>theme/next</code>也基本与<code>hexo/source/_data</code>目录等效，故这些不再赘述。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">custom_file_path:</span></span><br><span class="line">  <span class="comment">#head: source/_data/head.njk</span></span><br><span class="line">  <span class="comment">#header: source/_data/header.njk</span></span><br><span class="line">  <span class="comment">#sidebar: source/_data/sidebar.njk</span></span><br><span class="line">  <span class="comment">#postMeta: source/_data/post-meta.njk</span></span><br><span class="line">  <span class="comment">#postBodyEnd: source/_data/post-body-end.njk</span></span><br><span class="line">  <span class="comment">#footer: source/_data/footer.njk</span></span><br><span class="line">  <span class="comment">#bodyEnd: source/_data/body-end.njk</span></span><br><span class="line">  <span class="comment">#variable: source/_data/variables.styl</span></span><br><span class="line">  <span class="comment">#mixin: source/_data/mixins.styl</span></span><br><span class="line">  <span class="comment">#style: source/_data/styles.styl</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 定位到 hexo/theme/next/source/_data</span></span><br><span class="line">  <span class="comment">#style: theme/nextsource/_data/styles.styl</span></span><br></pre></td></tr></table></figure>
<p>此时，hexo再进行渲染时，也会引入这些文件，当其中的样式与默认样式冲突时，自定义样式优先级高，便会覆盖默认样式。但我们会发现，这种方式只能自动引入与上述文件重名的自定义文件，所以，我们可以将所有修改或者新添的样式写入<code>styles.styl</code>等文件。但是，这样的方法有些确定就是不方便管理。比如，我们打算自定义一个<code>custom-about.styl</code>文件来专门优化“关于”页面，这时我们就要手动的把该文件引入。有以下几种方法可供参考：</p>
<ol type="1">
<li>在<code>styles.styl</code>文件中引入</li>
</ol>
<p><figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="keyword">@import</span> <span class="string">&quot;custom-about.styl&quot;</span></span><br></pre></td></tr></table></figure></p>
<ol start="2" type="1">
<li>在<code>about.md</code>文件中引入</li>
</ol>
<p><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="xml"><span class="comment">&lt;!-- 注意文件路径是否正确 --&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text/css&quot;</span> <span class="attr">href</span>=<span class="string">&quot;_data/custom-about.styl&quot;</span>&gt;</span></span></span><br></pre></td></tr></table></figure></p>
<ol start="3" type="1">
<li>在<code>layout.njk</code>等其他会被自动渲染的文件中引入</li>
</ol>
<p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text/css&quot;</span> <span class="attr">href</span>=<span class="string">&quot;_data/custom-about.styl&quot;</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="覆盖默认样式">覆盖默认样式</h3>
<p>我们可以在<code>styles.styl</code>等文件中来定义class新的样式，最简单的方式就是通过<code>F12</code>来查看网页资源，然后选择想要修改的元素，查看元素所带的class，然后在<code>styles.styl</code>中重写即可。</p>
<h3 id="自定义图标">自定义图标</h3>
<p>NexT的默认图标库是Font Awesome，它并不包括很多国内主流网站的图标，比如Bilibili、知乎等。但我们可以通过自定义的方式来增加对这些图标的支持。下面以Bilibili为例</p>
<ol type="1">
<li>下载<code>bilibili.svg</code>，保存到<code>theme/next/source/images/bilibili.svg</code></li>
<li>在<code>theme/next/source/_data/styles.styl</code>添加样式</li>
</ol>
<p><figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-class">.fab</span><span class="selector-class">.fa-bilibili</span> &#123;</span><br><span class="line">  <span class="attribute">background</span>: <span class="built_in">url</span>(/images/bilibili.svg);</span><br><span class="line">  <span class="attribute">background-position</span>: <span class="number">50%</span> <span class="number">75%</span>;</span><br><span class="line">  <span class="attribute">background-repeat</span>: no-repeat;</span><br><span class="line">  <span class="attribute">height</span>: <span class="number">1rem</span>;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">1rem</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ol start="3" type="1">
<li>确保已经在<code>next/_config.xml</code>中开启了自定义文件路径</li>
</ol>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">custom_file_path:</span></span><br><span class="line">  <span class="attr">style:</span> <span class="string">source/_data/styles.styl</span></span><br></pre></td></tr></table></figure>
<ol start="4" type="1">
<li>在<code>next/_config.xml</code>配置相应图标</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">social:</span><br><span class="line">  Bilibili: https:&#x2F;&#x2F;space.bilibili.com&#x2F;userid&#x2F; || fab fa-bilibili</span><br></pre></td></tr></table></figure>
<h2 id="自定义js">自定义JS</h2>
<p>类比CSS。</p>
<h2 id="自定义markdown">自定义Markdown</h2>
<h3 id="markdown内嵌html">Markdown内嵌HTML</h3>
<p>作为标记语言，Markdown在某些场景下支持对HTML、CSS和JS的扩展，即我们可以在Markdown中写入HTML语法，甚至可以内嵌CSS和JS。以下为我的博客404页面的示例：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 404</span><br><span class="line">date: 2020-01-09 13:25:01</span><br><span class="line">layout: false</span><br><span class="line">commit: false</span><br><span class="line">permalink: /404</span><br><span class="line">---</span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;zh&quot;</span>&gt;</span></span></span><br><span class="line">  <span class="xml"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span></span><br><span class="line"><span class="code">    &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;</span></span><br><span class="line"><span class="code">    &lt;title&gt;404&lt;/title&gt;</span></span><br><span class="line"><span class="code">    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;</span></span><br><span class="line"><span class="code">    &lt;link rel=&quot;stylesheet&quot; href=&quot;/404/style.css&quot;&gt;</span></span><br><span class="line"><span class="code">  &lt;/head&gt;</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">  &lt;body style=&quot;margin: 0px&quot;&gt;</span></span><br><span class="line"><span class="code">  &lt;div class=&quot;container&quot;&gt;&lt;div class=&quot;row&quot;&gt;&lt;div class=&quot;col-md-6 align-self-center&quot;&gt;&lt;img src=&quot;/404/404.svg&quot;&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="code">  &lt;div class=&quot;col-md-6 align-self-center&quot;&gt;</span></span><br><span class="line"><span class="code">  &lt;h1&gt;404&lt;/h1&gt;&lt;h2&gt;UH OH! 页面丢失&lt;/h2&gt;&lt;p&gt;您所寻找的页面不存在。你可以点击下面的按钮，返回主页。&lt;/p&gt;</span></span><br><span class="line"><span class="code">  &lt;a href=&quot;https://www.geminilight.cn/&quot;&gt;&lt;button class=&quot;btn blue&quot;&gt;返回首页&lt;/button&gt;&lt;/a&gt;</span></span><br><span class="line"><span class="code">  &lt;/div&gt;</span></span><br><span class="line"><span class="code">  &lt;/div&gt;</span></span><br><span class="line"><span class="code">  &lt;/div&gt;</span></span><br><span class="line"><span class="code">  &lt;/body&gt;</span></span><br><span class="line"><span class="code">&lt;/html&gt;</span></span><br></pre></td></tr></table></figure>
<p>需要注意的是，在hexo渲染时，对markdown的缩进有较高的要求。至少在我实际操作过程中，如果HTML标签前有超过4个空格的缩进，便会被渲染为全文字。</p>
<h3 id="markdown渲染外部html文件">Markdown渲染外部html文件</h3>
<p>我们也可以在外部新建一个完整的html页面，然后在md文件中引入，并加入相应标签来引入渲染</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">span</span> <span class="attr">style</span>=<span class="string">&quot;width:100%; height:260;border:none;text-align:center&quot;</span>&gt;</span></span> </span><br><span class="line"><span class="code">  &lt;iframe allowtransparency=&quot;yes&quot; frameborder=&quot;0&quot; width=&quot;100%&quot; height=&quot;88&quot; src=&quot;url&quot;&gt;</span></span><br><span class="line"><span class="code"> &lt;/iframe&gt;</span></span><br><span class="line"><span class="code">&lt;/span&gt;</span></span><br></pre></td></tr></table></figure>
<p>url 指向的是独立的 HTML 文件的路径，可以直接放在我们的md文件下。<br />
此外，为了避免html文件被编译而被嵌套主题样式，我们配置主题的<code>_config.xml</code>来跳过对该文件的渲染：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 跳过渲染</span></span><br><span class="line"><span class="attr">skip-render:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">README.md</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">xxx.html</span>  <span class="comment"># 禁止渲染xxx.html文件</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">*.html</span>    <span class="comment"># 禁止渲染所有html文件</span></span><br></pre></td></tr></table></figure>
<h2 id="自定义页面">自定义页面</h2>
<h3 id="不含博客框架的页面">不含博客框架的页面</h3>
<p>这种类型的页面指完全自定义的，即整个页面不会出现博客的header、sidebar、footer等。比如，我们可以定制一个小游戏的404界面。这里以完全自定义的xxx页面为例，创建流程如下：</p>
<ol type="1">
<li>在hexo主目录文件夹下，输入命令：</li>
</ol>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">hexo new page xxx</span><br></pre></td></tr></table></figure>
<p>该命令会自动在<code>hexo/source</code>下创建<code>xxx/xxx.md</code>文件夹和md文件。</p>
<ol start="2" type="1">
<li>编辑<code>xxx.md</code>，添加<code>layout: false</code>属性</li>
</ol>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">title: xxx</span><br><span class="line">type: xxx</span><br><span class="line">layout: false</span><br></pre></td></tr></table></figure>
<p>设置<code>layout: false</code>会使hexo在渲染页面时，不自动为页面渲染基本的导航栏、侧边栏等，所以该页面需要完全自定义css和js。我们可以既可以完全在md中写js和css（并不推荐），也可以单独创建相应的JS和CSS文件再引入到md文件中，下面只对后者进行介绍。</p>
<ol start="3" type="1">
<li>在<code>hexo/_data</code>文件夹中新建js和css文件</li>
</ol>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">|</span><br><span class="line">|-- xxx.js</span><br><span class="line">|-- xxx.css</span><br></pre></td></tr></table></figure>
<ol start="4" type="1">
<li>在<code>xxx.md</code>中引入</li>
</ol>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text/css&quot;</span> <span class="attr">href</span>=<span class="string">&quot;_data/xxx.css&quot;</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="comment">&lt;!-- content --&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span> <span class="attr">src</span>=<span class="string">&quot;xxx.js&quot;</span>&gt;</span></span></span><br></pre></td></tr></table></figure>
<h3 id="含博客框架的子页面">含博客框架的子页面</h3>
<p>这种类型的页面指依赖于hexo生成的博客页面主框架，比如我们的Tags标签页、Categories分类页等。这种页面并非完全定制的，这也会带来一个优点：我们可以利用Hexo和NexT主题附带的样式和JS来制作的页面。</p>
<ol type="1">
<li>在hexo主目录文件夹下，输入命令：</li>
</ol>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">hexo new page xxx</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>同自定义css方法，可在<code>hexo/_data</code>编辑相应文件并引入渲染</li>
</ol>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">|</span><br><span class="line">|-- xxx.js</span><br><span class="line">|-- xxx.css</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2>
]]></content>
      <categories>
        <category>ST - 软件工具</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>【Python】with语句原理</title>
    <url>/2020/08/14/PL%20-%20%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/pl-python-with/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>在我们使用Python的时候，常使用到如下的代码块:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 文件读取</span></span><br><span class="line"><span class="keyword">with</span> open(file, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="comment"># CODE BLOCK #</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度计算</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> g:</span><br><span class="line">    <span class="comment"># CODE BLOCK #</span></span><br></pre></td></tr></table></figure>
<p>在很多场景中，使用with语句来可以让我们可以更好地来管理资源和简化代码，它可以看做是对try/finally模式的简化。它原理上是利用了上下文管理器，下文简要介绍将对其执行原理和自定义的方法。<a id="more"></a></p>
<h2 id="上下文管理器概念">上下文管理器概念</h2>
<h3 id="上下文管理协议context-management-protocol">上下文管理协议（Context Management Protocol）</h3>
<p>包含方法 <code>__enter__()</code> 和 <code>__exit__()</code> ，支持该协议的对象要实现这两个方法。</p>
<h3 id="上下文管理器context-manager">上下文管理器（Context Manager）</h3>
<p>支持上下文管理协议的对象，这种对象必须实现 <code>__enter__()</code> 和 <code>__exit__()</code> 方法。 上下文管理器定义执行with语句时要建立的运行时上下文，负责执行with语句块上下文中的进入与退出操作。 通常使用with语句调用上下文管理器，也可以通过直接调用其方法来使用。</p>
<ul>
<li><strong><code>__enter__()</code></strong>
<ul>
<li>with语句执行时，先获取上下文管理器对象，随后调用其 <code>__enter__()</code></li>
<li>若有 <code>as var</code> 语句，则将返回值赋给变量var</li>
<li>可以返回上下文管理器对象本身，也可以是其他相关对象</li>
</ul></li>
<li><strong><code>__exit__()</code></strong>
<ul>
<li>带有三个参数 <code>exc_type, exc_val, exc_tb</code></li>
<li>若上下文管理器对象执行无异常，则三个参数均为 <code>None</code></li>
<li>若发生异常，则三个参数分别为 异常类型，异常值和tracback信息</li>
</ul></li>
</ul>
<h2 id="with语句执行过程">with语句执行过程</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#   EXP: 表达式</span></span><br><span class="line"><span class="comment">#   VAR: 变量名，[as VAR][可选]</span></span><br><span class="line"><span class="comment"># BlOCK: 代码块</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> EXP <span class="keyword">as</span> VAR:</span><br><span class="line">    BLOCK</span><br></pre></td></tr></table></figure>
<p><img data-src="/resource/images/pl-python-with.png" width="100%" /></p>
<ol type="1">
<li>执行代码时，先执行 <code>EXPR</code> 语句，生成上下文管理器对象 context_manager；</li>
<li>获取上下文管理器的 <code>__exit()__</code> 方法，并保存起来用于之后的调用；</li>
<li>调用上下文管理器的 <code>__enter__()</code> 方法，且可将返回值赋给as语句变量；</li>
<li>执行BLOCK中的表达式；</li>
<li>不管是否执行过程中是否发生了异常，执行上下文管理器的 <code>__exit__()</code> 方法， 执行“清理”工作，如释放资源等。
<ol type="1">
<li>如果执行过程中没有出现异常，或者语句体中执行了语句 <code>break</code> / <code>continue</code> / <code>return</code> ，则以 <code>None</code> 作为参数调用 <code>__exit__(None, None, None)</code> ；</li>
<li>如果执行过程中出现异常，则使用sys.exc_info得到的异常信息为参数调用 <code>__exit__(exc_type, exc_value, exc_traceback)</code> ；</li>
</ol></li>
<li>出现异常时，如果 <code>__exit__(type, value, traceback)</code> 返回 <code>False</code> ，则会重新抛出异常，让with之外的语句逻辑来处理异常，这也是通用做法；如果返回True，则忽略异常，不再对异常进行处理。</li>
</ol>
<h2 id="自定义上下文管理器">自定义上下文管理器</h2>
<ul>
<li>它使代码更简练，可以简化try/finally模式</li>
<li>当代码异常产生时，<code>__exit__()</code> 会执行清理工作</li>
<li>可以对软件系统中的资源进行管理，比如数据库连接、共享资源的访问控制等<br /></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding = utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上下文管理器类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestWith</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;进入with语句的时候被调用</span></span><br><span class="line"><span class="string">           并将返回值赋给as语句的变量名</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">&#x27;__enter__&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;var&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span>(<span class="params">self, exc_type, exc_val, exc_tb</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;离开with的时候被with调用&quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">&#x27;__exit__&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># with后面必须跟一个上下文管理器</span></span><br><span class="line"><span class="comment"># 如果使用了as，则是把上下文管理器的 __enter__() 方法的返回值赋值给 target</span></span><br><span class="line"><span class="comment"># target 可以是单个变量，或者由“()”括起来的元组（不能是仅仅由“,”分隔的变量列表，必须加“()”）</span></span><br><span class="line"><span class="keyword">if</span> __name__ = <span class="string">&#x27;main&#x27;</span>:</span><br><span class="line">    <span class="keyword">with</span> TestWith() <span class="keyword">as</span> var:</span><br><span class="line">        print(var)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行结果</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">__enter__</span></span><br><span class="line"><span class="string">var</span></span><br><span class="line"><span class="string">__exit__</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>本例仅对应代码正常执行的流程，其他特殊情况不再一一列举，有兴趣可单独实验。</p>
<h2 id="参考">参考</h2>
<blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIuaWJtLmNvbS96aC9hcnRpY2xlcy9vcy1jbi1weXRob253aXRoLw==">浅谈 Python 的 with 语句<i class="fa fa-external-link-alt"></i></span> <br /> <span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcHl0aG9uYmFvL3AvMTEyMTEzNDcuaHRtbA==">Python中with用法详解<i class="fa fa-external-link-alt"></i></span> <br /> <span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC83NDU1Mjg3Nw==">Python中with使用<i class="fa fa-external-link-alt"></i></span> <br /></p>
</blockquote>
]]></content>
      <categories>
        <category>PL - 编程语言</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】GNN之GCN：Semi-Supervised Classification with Graph Convolutional Networks</title>
    <url>/2020/07/27/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/paper-dl-gnn-gcn/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>本文提出了一种基于卷积操作的图神经网络GCN，它将卷积操作适应于图结构，同时提取图结构和节点的特征信息，并在在半监督学习中获得了较好的效果提升。</p>
<p><strong>论文名称</strong>：Semi-Supervised Classification with Graph Convolutional Networks<br />
<strong>论文作者</strong>：Thomas N. Kipf, Max Welling<br />
<strong>发表期刊</strong>：ICLR-2017 (THU-A)<br />
<strong>研究方向</strong>：GNN 图神经网络<br />
<strong>关键技术</strong>：Graph Convolution<br />
<strong>主要创新</strong>：将多头注意力机制应用于图神经网络，来提升特征提取效果。<br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE2MDkuMDI5MDc=">下载论文<i class="fa fa-external-link-alt"></i></span> | <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RraXBmL2djbj91dG1fc291cmNlPWNhdGFseXpleC5jb20=">查看源码<i class="fa fa-external-link-alt"></i></span></p>
<a id="more"></a>
<h2 id="论文简介">论文简介</h2>
<h3 id="缩写释义">缩写释义</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">缩写</th>
<th>描述</th>
<th>全称</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">GNN</td>
<td>图神经网络</td>
<td>Graph Neural Network</td>
</tr>
<tr class="even">
<td style="text-align: center;">GCN</td>
<td>图卷积网络</td>
<td>Graph Convolutional Network</td>
</tr>
</tbody>
</table>
<h3 id="研究背景">研究背景</h3>
<ul>
<li></li>
</ul>
<h3 id="主要贡献">主要贡献</h3>
<ul>
<li>提出了一种简单、有效且可以直接在图上进行的卷积操作的神经网络</li>
<li>证明了GCN可以快速的和可伸缩处理半监督节点分类问题</li>
</ul>
<h2 id="模型算法">模型算法</h2>
<h3 id="多层gcn传播规则">多层GCN传播规则</h3>
<p>多层的 Graph Convolutional Network (GCN) 传播规则如下</p>
<p><span class="math display">\[H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)\]</span></p>
<p>式中，</p>
<ul>
<li><span class="math inline">\(\tilde{A}=A+I_{N}\)</span>是无向图<span class="math inline">\(\mathcal{G}\)</span>的邻接矩阵与自连接的单位矩阵的加和</li>
<li><span class="math inline">\(\tilde{D}_{i i}=\sum_{j} \tilde{A}_{i j}\)</span></li>
<li><span class="math inline">\(W^{(l)}\)</span>是分层可训练的权重矩阵</li>
<li><span class="math inline">\(\sigma (·)\)</span>是一个激活函数</li>
<li><span class="math inline">\(H^{(l)} \in \mathbb{R}^{N \times D}\)</span>是第<span class="math inline">\(l\)</span>层的激活矩阵，且<span class="math inline">\(H^{(l)}=X\)</span></li>
</ul>
<h3 id="基于谱方法的图卷积">基于谱方法的图卷积</h3>
<p>图的谱卷积操作被定义为信号<span class="math inline">\(x \in \mathbb{R}^N\)</span>与傅里叶域滤波器<span class="math inline">\(g_{\theta}=\operatorname{diag}(\theta)\)</span>的乘积：</p>
<p>式中，<span class="math inline">\(U\)</span>是归一化图拉普拉斯特征向量的矩阵，满足</p>
<p><span class="math display">\[L=I_{N}-D^{-\frac{1}{2}} A D^{-\frac{1}{2}}=U \Lambda U^{\top}\]</span></p>
<p>其特征值<span class="math inline">\(\Lambda\)</span>的对角矩阵和<span class="math inline">\(U^{\top} x\)</span>是<span class="math inline">\(x\)</span>的图傅里叶变换。我们可以理解<span class="math inline">\(g_{\theta}\)</span>为<span class="math inline">\(L\)</span>特征值的函数，如<span class="math inline">\(g_{\theta}(\Lambda)\)</span>。</p>
<p>但大型图的特征分解是高计算需求的。为了避免这个问题，<span class="math inline">\(g_{\theta}(\Lambda)\)</span>可以通过切比雪夫多项式<span class="math inline">\(T_k(x)\)</span>的截断展开很好地逼近到第<span class="math inline">\(K^{th}\)</span>阶：</p>
<p><span class="math display">\[g_{\theta} \star x=U g_{\theta} U^{\top} x\]</span></p>
<p>式中，<span class="math inline">\(\tilde{\Lambda}=\frac{2}{\lambda_{\max }} \Lambda-I_{N}\)</span>，<span class="math inline">\(\lambda_{\max }\)</span>表示L的最大特征值。<span class="math inline">\(\theta&#39; \in \mathbb{R}^K\)</span>是切比雪夫系数的向量表示。切比雪夫多项式递归定义为<span class="math inline">\(T_{k}(x) = 2 x T_{k-1}(x)-T_{k-2}(x)\)</span>, <span class="math inline">\(T_{0}(x)=1, T_1(x)=x\)</span>。</p>
<p>因此，信号<span class="math inline">\(x\)</span>和滤波<span class="math inline">\(g_{\theta^{\prime}}\)</span>的卷积操作可以定义为 <span class="math display">\[g_{\theta^{\prime}}(\Lambda) \approx \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{\Lambda})\]</span> 式中，<span class="math inline">\(\tilde{L}=\frac{2}{\lambda_{\max }} L-I_{N}\)</span>，因为<span class="math inline">\(\left(U \Lambda U^{\top}\right)^{k}=U \Lambda^{k} U^{\top}\)</span>。注意，这个表达式现在是K-local，因为它是Laplacian中的第K阶多项式，即它只依赖于距离中心节点（第K阶邻域）最大K步的节点。</p>
<p><span class="math display">\[g_{\theta^{\prime}} \star x \approx \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{L}) x\]</span></p>
<h2 id="模型改进">模型改进</h2>
<h2 id="性能评估">性能评估</h2>
<h3 id="数据集">数据集</h3>
<h3 id="评估指标">评估指标</h3>
<h2 id="总结思考">总结思考</h2>
<ul>
<li></li>
</ul>
]]></content>
      <categories>
        <category>RP - 科研论文</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Automatic Virtual Network Embedding - A DRL Approach with GCN</title>
    <url>/2020/07/13/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/paper-nfv-vne-dlr-gcn/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>本篇论文将强化学习A3C算法与图卷积神经网络GCN相结合，并且设置了多目标的奖励函数，提出了一种更加高效的虚拟网络嵌入算法。</p>
<h2 id="论文简介">论文简介</h2>
<p><strong>论文名称</strong>：Automatic Virtual Network Embedding: A Deep Reinforcement Learning Approach with Graph Convolutional Networks<br />
<strong>论文作者</strong>：Zhongxia Yan, Jingguo Ge, Y ulei Wu, Senior Member , IEEE, Liangxiong Li, Tong Li<br />
<strong>发表期刊</strong>：JSAC-2020 (CCF-A)<br />
<strong>研究方向</strong>：NFV 网络功能虚拟化<br />
<strong>关键技术</strong>：虚拟网络嵌入, 强化学习, 图卷积神经网络<br />
<strong>主要创新</strong>：强化学习结合图卷积神经网络、并行的强化学习框架、多目标的奖励函数<br />
<span class="exturl" data-url="aHR0cHM6Ly9vcmUuZXhldGVyLmFjLnVrL3JlcG9zaXRvcnkvYml0c3RyZWFtL2hhbmRsZS8xMDg3MS80MDc5OS9ZYW4tSlNBQy0yMDIwLnBkZj9zZXF1ZW5jZT0xJmFtcDtpc0FsbG93ZWQ9eQ==">下载论文<i class="fa fa-external-link-alt"></i></span></p>
<a id="more"></a>
<p><strong>词汇缩写</strong></p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">缩写</th>
<th>描述</th>
<th>全名</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">VNE</td>
<td>虚拟网络嵌入</td>
<td>Virtual Network Embedding</td>
</tr>
<tr class="even">
<td style="text-align: center;">VNR</td>
<td>虚拟网络请求</td>
<td>Virtual Network Request</td>
</tr>
<tr class="odd">
<td style="text-align: center;">RL</td>
<td>强化学习</td>
<td>Reinforcement Learning</td>
</tr>
<tr class="even">
<td style="text-align: center;">GCN</td>
<td>图卷积神经网络</td>
<td>Graph Convolutional Network</td>
</tr>
<tr class="odd">
<td style="text-align: center;">A3C</td>
<td></td>
<td>Asynchronous Advantage Actor-Critic</td>
</tr>
</tbody>
</table>
<h2 id="问题建模">问题建模</h2>
<h3 id="底层网络">底层网络</h3>
<p>加权无向图 <span class="math inline">\(G_s = (N_s, L_s, A^n, A^l)\)</span></p>
<ul>
<li><span class="math inline">\(N_s\)</span>：物理节点的集合</li>
<li><span class="math inline">\(L_s\)</span>：物理链路的集合</li>
<li><span class="math inline">\(A^n\)</span>：节点属性，如CPU processing capability, memory space and node reliability</li>
<li><span class="math inline">\(A^l\)</span>：链路属性，如bandwidth, latency and packet loss rate</li>
</ul>
<p>本文仅考虑CPU资源和带宽资源分别作为节点和链路的属性</p>
<h3 id="虚拟网络请求">虚拟网络请求</h3>
<p>虚拟网络：加权无向图 <span class="math inline">\(G_v = (N_v, L_v, R^n, R^l)\)</span></p>
<ul>
<li><span class="math inline">\(N_v\)</span>：虚拟节点的集合</li>
<li><span class="math inline">\(L_v\)</span>：虚拟链路的集合</li>
<li><span class="math inline">\(R_n\)</span>：虚拟节点请求的集合</li>
<li><span class="math inline">\(R_l\)</span>：虚拟链路请求的集合</li>
</ul>
<p>虚拟网络请求：<span class="math inline">\(VNR = (G_v, t_a, t_d)\)</span></p>
<ul>
<li><span class="math inline">\(t_a\)</span>：VNR到达时间</li>
<li><span class="math inline">\(t_d\)</span>：VNR离开时间</li>
</ul>
<h3 id="问题约束">问题约束</h3>
<p>映射问题 <span class="math inline">\(M: G_{v}\left(N_{v}, L_{v}\right) \rightarrow G_{s}^{\prime}\left(N_{s}^{\prime}, L_{s}^{\prime}\right)\)</span></p>
<ul>
<li>节点映射：CPU资源约束</li>
<li>链路映射：带宽资源约束</li>
</ul>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-ex-problem.png" /></p>
<h3 id="优化目标">优化目标</h3>
<h4 id="接受率">接受率</h4>
<p>底层网络在时间<span class="math inline">\(T\)</span>接受率为</p>
<p><span class="math display">\[AC_Ratio(T) = \frac{\sum_{t=0}^{T}{NUM\_VNR\_S}}{\sum_{t=0}^{T}{NUM\_VNR}}\]</span></p>
<ul>
<li><span class="math inline">\(NUM\_VNR\_S\)</span>：成功嵌入的VNR数量</li>
<li><span class="math inline">\(NUM\_VNR\)</span>：VNR的总数量</li>
</ul>
<h4 id="长期平均收益">长期平均收益</h4>
<p>单个VNR <span class="math inline">\(G_v\)</span> 被嵌入成功的收益为</p>
<p><span class="math display">\[\operatorname{Rev}\left(G_{v}\right)=\sum_{n_{v} \in N_{v}} \operatorname{CPU}\left(n_{v}\right)+\sum_{l_{v} \in L_{v}} B W\left(l_{v}\right)\]</span></p>
<p>底层网络长期平均收益为</p>
<p><span class="math display">\[Rev(T) = \frac{\sum_{t=0}^{T}Rev(G^t_v)}{T}\]</span></p>
<ul>
<li><span class="math inline">\(G^t_v\)</span>：表示时间<span class="math inline">\(t\)</span>是被成功嵌入的VNR</li>
</ul>
<h4 id="运行时间">运行时间</h4>
<p>权衡性能与时效，适应实时场景。</p>
<h3 id="vne">VNE</h3>
<h2 id="算法模型">算法模型</h2>
<p>为了适应RL框架，本文Agent使用单步决策，即每一次决策生成的是一个虚拟节点的放置策略</p>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-algo-rl-framework.png" /></p>
<p>当获取一个虚拟节点的放置策略后，尝试将其进行放置</p>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-algo-1-vne-procedure.png" /></p>
<h3 id="环境-environment">环境 Environment</h3>
<p>由底层网络及其相关设定构成</p>
<h3 id="状态-state">状态 State</h3>
<p>所需状态表示如下：</p>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-algo-state.png" /></p>
<h3 id="动作-action">动作 Action</h3>
<p>在Agent生成单个虚拟节点的放置决策后，需尝试对其进行放置，此时要满足以下约束</p>
<ul>
<li>被选物理节点可用资源 &gt; 当前虚拟节点资源请求</li>
<li>存在带宽资源充足的链路来放置虚拟链路</li>
</ul>
<p>对于链路的搜索，本文采用混合搜索策略：优先尝试最短路径，若失败则尝试其他次优路径。</p>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-ex-hybrid-search.png" /></p>
<h3 id="代理-agent">代理 Agent</h3>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-algo-rl-agent.png" /></p>
<h4 id="gcn">GCN</h4>
<h4 id="策略生成">策略生成</h4>
<h4 id="a3c-并行训练">A3C 并行训练</h4>
<h4 id="master">Master</h4>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-algo-2-training-master.png" /></p>
<h4 id="worker">Worker</h4>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-algo-3-training-worker.png" /></p>
<h3 id="奖励-rewords">奖励 Rewords</h3>
<ul>
<li>动作反馈 action feedback</li>
</ul>
<p><span class="math display">\[r_{a}=\left\{\begin{array}{cl}
  100 \gamma_{a} &amp; a_{t} \text {is successful} \\
  -100 \gamma_{a} &amp;  \text {otherwise}
\end{array}\right.\]</span></p>
<ul>
<li>成本效益 cost-efficient</li>
</ul>
<p><span class="math display">\[r_{c} =  \frac{\delta(revenue)}{\delta(cost)}\]</span></p>
<ul>
<li>负载均衡 Load balancing</li>
</ul>
<p><span class="math display">\[r_{s} = \frac{S_CPU_Remaining[a]}{S_CPU_Max[a]}\]</span></p>
<ul>
<li>资格痕迹 eligibility trace</li>
</ul>
<p><span class="math display">\[e g b_{-}{trace}_{t}[i]=\left\{\begin{array}{cl}
  \gamma_{e}\left(e g b_{-}{trace}_{t-1}[i]+1\right) &amp; i==a_{t} \\
  \gamma_{e} {egb}_{-} {trace}_{t-1}[i] &amp; \text { otherwise }
\end{array}\right.\]</span></p>
<p>故最终对于动作<span class="math inline">\(a{t}\)</span>奖励函数（Reward Function）为</p>
<p><span class="math display">\[Reward[a_{t}]=\frac{r_{a} r_{c} r_{s}}{e g b{-}trace[a_{t}]+\epsilon}\]</span></p>
<h2 id="性能评估">性能评估</h2>
<h3 id="设计实验">设计实验</h3>
<ul>
<li>底层网络拓扑
<ul>
<li>使用参数 <span class="math inline">\(\alpha = 0.5\)</span> 和 <span class="math inline">\(\beta = 0.2\)</span> 的 Waxman 随机图来生成一个底层网络拓扑</li>
<li>该网络具有100个结点和500条边（模拟一个中型的ISP）。</li>
<li>随机分配每个节点的CPU数量和边的带宽大小为50~100个单位。</li>
</ul></li>
<li>虚拟网络请求
<ul>
<li>随机生成VNR时满足Possion process（泊松分布），每组评估持续50000个时间单位</li>
<li>即当VNR预期到达率为4个/100个时间单位时，则约有2000个VNR</li>
<li>每个VRN的生成周期满足平均值为500的指数分布</li>
<li>每个VNR的数量均匀分布在2~10之间</li>
<li>初始化VNR中的节点CPU需求和链路带宽需求为0~30的均匀分布</li>
<li>每对节点有50%的可能性形成边</li>
</ul></li>
<li>动态设置参数
<ul>
<li>VNR的到达率、节点及链路资源的分配、每个VNR的节点数量</li>
<li>我们可以通过调节这三个参数来评估各种VNE场景</li>
</ul></li>
<li>测试阶段
<ul>
<li>学习代理仅使用actor网络生成嵌入策略，来从底层网络拓扑中选择合适的节点托管当前的虚拟节点</li>
<li>该代理已被训练72小时，经历了70000次训练迭代，进行了近1680000次不同的VNR</li>
</ul></li>
</ul>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-exp-parameter-settings.png" /></p>
<h3 id="对比算法">对比算法</h3>
<ul>
<li>R-ViNE<br />
使用基于确定的取整（rounding-based）的方法来获得与VNE问题对应的MIP的线性规划松弛（linear programming relaxation），以最小化VNR的成本</li>
<li>D-ViNE<br />
和RR-ViNE，但特殊在其取整方法是随机的</li>
<li>GRC<br />
一种基于全局资源容量管理的节点排序算法</li>
<li>MCVNE<br />
一种基于强化学习的 Monte-Carlo MCTS 动作空间搜索算法</li>
<li>NodeRank<br />
一种节点排序算法，灵感来自与Google的PageRank算法</li>
</ul>
<p>它们基本覆盖了当前大部分算法的观点</p>
<h3 id="评估指标">评估指标</h3>
<h4 id="vnr-到达率测试">VNR 到达率测试</h4>
<p>实际场景：虚拟网络总在忙碌时频繁接受VNR请求，空闲时则反之。<br />
实验模拟：将到达率由4个/100时间单位逐渐增至20个/100时间单位，步长为2。</p>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-1.png" /></p>
<p>结果分析：该算法在VNR请求较频繁时，接受率和平均收益明显优于其他算法。</p>
<h4 id="资源请求测试">资源请求测试</h4>
<p>实际场景：不同的网络服务具有不同的资源需求模式，比如：</p>
<ul>
<li>计算密集型任务需要更多的节点资源（CPU）</li>
<li>而通信密集型任务需要更多的链路资源（带宽）</li>
</ul>
<p>实验模拟：将节点与链路资源需求的逐渐由[0,30]升至[0,100]的平均分布，步长为10。</p>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-2.png" /></p>
<p>结果分析：随着资源需求越来越多，嵌入的成功率也会都明显随之降低，但该算法的表现依然是最忧的。</p>
<h4 id="节点数量扩展性测试">节点数量扩展性测试</h4>
<p>实际场景：企业级用户对网络服务需求量较大，个人用户服务使用的节点数量较小。<br />
实验模拟：将VNR中的虚拟节点数量从[2,10]的均匀分布增加到[2,32]，步长为2。</p>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-3.png" /></p>
<p>结果分析：该算法优势明显，但可以发现当节点逐渐增加时，接受率下降明显</p>
<p>原因分析：</p>
<ul>
<li>因为每个VNR都必须作为一个整体成功地嵌入；一个更大的VNR意味着在嵌入的中间步骤中失败的机会更多；</li>
<li>因为同一VNR中的两个虚拟节点不能共享一个特定的基板节点，单个VNR中的更多节点限制了候选动作空间</li>
</ul>
<h4 id="平均运行时间统计">平均运行时间统计</h4>
<p>平均运行时间指VNE算法处理一个完整VNF的平均时间开销</p>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-4.png" /></p>
<h3 id="验证测试">验证测试</h3>
<p>在相同条件下，与其他基于强化学习的VNE算法进行比较</p>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-8.png" /></p>
<h4 id="训练效率及收敛性">训练效率及收敛性</h4>
<p>不同算法在相同实验条件下的训练效果对比，见下图(a)<br />
实验结果：可以发现，在这组实验中，平均收益的优势要比接受率的明显<br />
原因分析：当VNR的节点数量较少时，所有算法都可以有很好的表现；但当节点增多后，该算法可以带来更多的潜在收益（多指标的Reward）。</p>
<h4 id="资源请求测试验证">资源请求测试验证</h4>
<p>在不同的资源请求情况下，对这些算法进行测试，见下图(b)</p>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-5.png" /></p>
<p>实验结果：在不同数量的资源请求情况下，该算法的接受率均是最优的；此外，可以发现CNN的性能表现始终是最差的<br />
原因分析：用GCN代替传统的CNN进行特征提取可以带来更好地性能。</p>
<h3 id="其他参数下模型的可行性研究">其他参数下模型的可行性研究</h3>
<h4 id="附加网络拓扑和参数">附加网络拓扑和参数</h4>
<p>CSTNET：中国网络运营商，见下图(a)</p>
<ul>
<li>红色边为100Gb带宽的链路</li>
<li>绿色边为10Gb带宽的链路</li>
<li>橙色边为2.5Gb带宽的链路</li>
<li>黑色边为1Gb带宽的链路</li>
<li>边的权重：平均传输延迟 average transmission latency（毫秒），</li>
</ul>
<p>对比算法：Noderank<br />
实验设计：</p>
<ul>
<li>模拟了一些随机的点对点的数据传输任务</li>
<li>数据传输速率为 [500Mbps,3Gbps] 的均匀分布</li>
</ul>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-6.png" /></p>
<p>实验结果：A3C+GCN 模型的延迟更少</p>
<h4 id="其他指标">其他指标</h4>
<ul>
<li>节点资源利用率：VNRs使用的节点基板资源量/资源总量</li>
<li>链路资源利用率：VNRs使用的链路基板资源量/资源总量</li>
</ul>
<p>实验设计：同平均到达率测试</p>
<p><img data-src="/resource/images/paper/paper-nfv-vne-drl-gcn-7.png" /></p>
<p>实验结果：A3C+GCN算法的这两项指标均为最佳</p>
<h2 id="主要创新">主要创新</h2>
<ul>
<li>基于RL+GCN的自动虚拟网络嵌入算法</li>
<li>并行的策略梯度训练方法</li>
<li>多指标的奖励函数</li>
</ul>
<h2 id="总结思考">总结思考</h2>
<ul>
<li>GCN较CNN可以更好地提取非欧数据的特征</li>
<li>A3C算法较其他RL算法性能表现更佳</li>
<li>并行的梯度训练策略更适应于实际场景</li>
<li>多指标的Reward可以让Agent学习到更好地策略来提高收益</li>
<li>在模型评估部分，实验的设计比较完善</li>
</ul>
]]></content>
      <categories>
        <category>RP - 科研论文</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>Paper</tag>
        <tag>NFV</tag>
        <tag>DRL</tag>
      </tags>
  </entry>
  <entry>
    <title>【学术讲座】ZJU 陈为教授：大数据与可视化技术</title>
    <url>/2020/03/21/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/lecture-zju-professor-chenwei-date-visonlization/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>浙江大学陈为教授关于大数据与可视化技术及其应用的讲座。</p>
<a id="more"></a>
<h2 id="大数据思维">大数据思维</h2>
<h3 id="一数据核心原理">一、数据核心原理</h3>
<p>从“流程”核心转变为“数据”核心</p>
<h3 id="二数据价值原理">二、数据价值原理</h3>
<p>由功能即价值转变为数据即价值</p>
<h3 id="三全样本原理">三、全样本原理</h3>
<p>从抽样转变为需要全部数据样本</p>
<h3 id="四关注效率原理">四、关注效率原理</h3>
<p>由关注精确度转变为关注效率</p>
<h3 id="五利用相关性取代因果关系">五、利用相关性取代因果关系</h3>
<p>不需要知道为什么</p>
<h3 id="六从采样到全样本">六、从采样到全样本</h3>
<p>抽样 != 全样</p>
<h3 id="七从精确到模糊">七、从精确到模糊</h3>
<p>大数据简单算法比小数据复杂性算法更有效</p>
<h3 id="八安迪比尔定律">八、安迪·比尔定律</h3>
<p>软件硬件相互要求<br />
大数据时代生产越来越过剩<br />
大数据分析应用的三个层次<br />
描述性分析-&gt; 预测性分析 -&gt; 指导性分析</p>
<ol type="1">
<li>关注发生了什么，呈现事物</li>
<li>关注“可能发生什么”，呈现趋势</li>
<li>关注“选择做什么”，呈现不同决策的后果</li>
</ol>
<h2 id="数据可视化">数据可视化</h2>
<h3 id="概念原理">概念原理</h3>
<p>创建并研究数据的<strong>视觉表达 (Visual Representation)</strong></p>
<ul>
<li>输入：数据（data）</li>
<li>输出：视觉形式（visual form）</li>
<li>目标：深入理解（insight）</li>
</ul>
<p><img data-src="/resource/images/lecture-zju-chenwei-data-visualization-1.png" /></p>
<h3 id="主要任务">主要任务</h3>
<ul>
<li>表示数据 - Represent</li>
<li>分析数据 - Analyze</li>
<li>交流数据 - Communicate</li>
</ul>
<p><img data-src="/resource/images/lecture-zju-chenwei-data-visualization-2.png" /></p>
<p>思维系统</p>
<p><img data-src="/resource/images/lecture-zju-chenwei-data-visualization-3.png" /></p>
<h3 id="重要应用">重要应用</h3>
<p><img data-src="/resource/images/lecture-zju-chenwei-data-visualization-4.png" /></p>
<h4 id="科学研究">科学研究</h4>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553101983-592e3de3-6399-4d7e-94fd-5520cb5d7436.jpeg" alt="Screenshot_2020-03-21-19-29-04-321_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-29-04-321_com.tencent.mm.jpg</figcaption>
</figure>
<h4 id="物联网与智慧城市">物联网与智慧城市</h4>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553093853-82ae1835-1ee9-446e-a9d7-1062ec4bd990.jpeg" alt="Screenshot_2020-03-21-19-31-43-189_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-31-43-189_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553096563-2e705eda-f52a-4d9d-8501-56e8d9f4a20b.jpeg" alt="Screenshot_2020-03-21-19-31-00-805_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-31-00-805_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553100901-44982283-79e6-4f40-bbde-844fd79a9312.jpeg" alt="Screenshot_2020-03-21-19-32-44-746_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-32-44-746_com.tencent.mm.jpg</figcaption>
</figure>
<h4 id="互联网与社交媒体">互联网与社交媒体</h4>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553116198-c5ab1e00-48c1-40e4-91cf-f4946189fef8.jpeg" alt="Screenshot_2020-03-21-19-33-59-913_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-33-59-913_com.tencent.mm.jpg</figcaption>
</figure>
<h4 id="可视化战役">可视化战役</h4>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553117639-2c9b41bf-e710-47fe-a845-1127481da915.jpeg" alt="Screenshot_2020-03-21-19-22-01-271_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-22-01-271_com.tencent.mm.jpg</figcaption>
</figure>
<h4 id="可视化技术原理">可视化技术原理</h4>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553108397-496f8771-8244-4a14-b3e0-1de3c4f563a7.jpeg" alt="Screenshot_2020-03-21-19-36-29-829_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-36-29-829_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553107177-df65738b-5da3-4e12-97b4-41634e49bb05.jpeg#" alt="Screenshot_2020-03-21-19-38-51-711_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-38-51-711_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553103214-3be220b5-f82b-449a-8236-99179c056d5f.jpeg" alt="Screenshot_2020-03-21-19-39-35-005_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-39-35-005_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553113310-36dfe174-5469-4332-9562-dd2342328f4c.jpeg" alt="Screenshot_2020-03-21-19-41-55-010_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-41-55-010_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553106046-4592657f-b6db-40c6-b015-1c8295265d90.jpeg" alt="Screenshot_2020-03-21-19-43-19-126_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-43-19-126_com.tencent.mm.jpg</figcaption>
</figure>
<h4 id="疫情可视化成果">疫情可视化成果</h4>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553104562-2fdc936c-d4fc-4389-b45c-3bcf70f540e0.jpeg" alt="Screenshot_2020-03-21-19-43-50-613_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-43-50-613_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553122434-af75c7be-d6a2-4b55-aadb-78466962b343.jpeg" alt="Screenshot_2020-03-21-19-46-02-513_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-46-02-513_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553118922-106f8ca6-72a4-45b4-b1c1-1f273a7f4ab4.jpeg" alt="Screenshot_2020-03-21-19-46-10-852_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-46-10-852_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553110693-67c79c0d-856c-4a8d-b443-0737a958a752.jpeg" alt="Screenshot_2020-03-21-19-47-49-742_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-47-49-742_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553095086-c642ff91-c59c-4f2c-be98-f46d2c27eb4b.jpeg" alt="Screenshot_2020-03-21-19-48-35-127_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-48-35-127_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553121370-dc268ad9-7e44-489e-88aa-6a7eecfaa959.jpeg" alt="Screenshot_2020-03-21-19-50-08-299_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-50-08-299_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553099079-0f2287d6-5f6f-4672-929b-dbc8ced4c212.jpeg" alt="Screenshot_2020-03-21-19-52-23-743_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-52-23-743_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553123618-26a9e0b7-8519-488b-9ae0-507597ec0127.jpeg" alt="Screenshot_2020-03-21-19-55-31-803_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-55-31-803_com.tencent.mm.jpg</figcaption>
</figure>
<h5 id="疫情可视化公益活动">疫情可视化公益活动</h5>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553097863-0e4f4174-04d5-4d15-98a1-e662a8316425.jpeg" alt="Screenshot_2020-03-21-19-55-47-774_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-19-55-47-774_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553109583-ae8af672-4d48-4d36-a56e-bb5906c69f87.jpeg" alt="Screenshot_2020-03-21-20-04-33-705_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-20-04-33-705_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553114681-a682bc1d-6485-4060-b11c-9caf09c55fcb.jpeg" alt="Screenshot_2020-03-21-20-08-01-604_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-20-08-01-604_com.tencent.mm.jpg</figcaption>
</figure>
<figure>
<img data-src="https://cdn.nlark.com/yuque/0/2020/jpeg/991157/1586553111955-ea479654-ba5b-4a85-828c-a786583d293b.jpeg" alt="Screenshot_2020-03-21-20-11-09-734_com.tencent.mm.jpg" /><figcaption aria-hidden="true">Screenshot_2020-03-21-20-11-09-734_com.tencent.mm.jpg</figcaption>
</figure>
<h2 id="拓展阅读">拓展阅读</h2>
<blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuamlxaXpoaXhpbi5jb20vYXJ0aWNsZXMvMjAxOS0wNC0xMS0z">浙江大学陈为“大数据可视化”<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yMDkyOTcwNw==">彻底颠覆你认知的10条大数据思维<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
<h2 id="我的心得">我的心得</h2>
]]></content>
      <categories>
        <category>MY - 感悟随笔</category>
      </categories>
      <tags>
        <tag>Research</tag>
        <tag>Data Visualization</tag>
      </tags>
  </entry>
  <entry>
    <title>【科研思维】高效的机器学习研究者</title>
    <url>/2020/02/25/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/research-effictive-ml-researcher/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>△ 高效的机器学习研究者 | 彻底的自我管理能力 + 坚持到底的决心<br />
☆ John Schulman &amp; 吴恩达  </p>
<a id="more"></a>
<hr />
<h2 id="笔记">笔记</h2>
<h3 id="挑选研究问题">1. 挑选研究问题</h3>
<h4 id="提升研究品位">（1）提升研究品位</h4>
<ul>
<li><strong>阅读论文</strong> - 认真评论与探讨</li>
<li><strong>研究小组</strong> - 吸收他人研究经验</li>
<li><strong>同行建议</strong> - 吸收他人的看法</li>
<li><strong>成果产出</strong> - 思考最有可能产出成果的研究方向/问题</li>
</ul>
<h4 id="研究的原动力">（2）研究的原动力</h4>
<ul>
<li><strong>想法驱动 - 测试某些想法</strong></li>
</ul>
<p>深刻理解研究主题以获得更多突破，避免与其他研究者想法相似</p>
<ul>
<li><strong>目标驱动 - 实现某些功能</strong></li>
</ul>
<p>注重通用性，将自己定义在通用解决方案中<br />
△ 任何领域的机器学习的新想法都与某些目标有关</p>
<h4 id="研究目标高远">（3）研究目标高远</h4>
<p>10% 的改善 OR  10 倍的提升？</p>
<ul>
<li>较大的目标下：增量研究（10% 的提升）是最有效的</li>
<li>增加的复杂性：取决于增量研究的性能提升</li>
</ul>
<h3 id="研究是条旅途">2. 研究是条旅途</h3>
<p>在不清楚终点的旅途中，不断朝着更好地结果前进：</p>
<h4 id="记录笔记">（1）记录笔记</h4>
<p>每日总结 &amp; 每周总结 = 事情 + 想法 + 成果</p>
<ul>
<li>记录想法</li>
<li>整理收获</li>
<li>时间管理</li>
</ul>
<h4 id="是否换坑">（2）是否换坑？</h4>
<p>过于频繁地切换想法比呆在原地不动的故障概率更高<br />
可以设置固定的时间去尝试那些新想法以拓宽知识面</p>
<h3 id="发展目光长远">3. 发展目光长远</h3>
<h4 id="走出舒适区充实机器学习领域知识">走出舒适区，充实机器学习领域知识</h4>
<ul>
<li><strong>教材书</strong>： 集中的方式来吸取知识，巩固基础</li>
<li><strong>学位论文</strong>：了解研究方向的背景、现状和展望</li>
<li><strong>前沿论文：</strong>关注领域前沿，并自己复现对比</li>
</ul>
<h3 id="读论文的建议">4. 读论文的建议</h3>
<h4 id="阅读进度法">（1）阅读进度法</h4>
<p>  每一篇列一行，表示从 0 到 100 的阅读进度</p>
<ul>
<li>很重要：仔细读到进度100%</li>
<li>不想要：10%确定是否放弃阅读</li>
</ul>
<h4 id="多次浏览法">（2）多次浏览法</h4>
<ul>
<li><strong>第一遍 标题、摘要和图表</strong>  - 论文讲什么</li>
<li><strong>第二遍 前言、结语和图表</strong>  - 论文主要思想</li>
<li><strong>第三遍 纵览论文主体</strong>      - 把握整体脉络（数学推导可跳过）</li>
<li><strong>第四遍 阅读所有内容</strong>      - 遇难可跳过以后攻坚</li>
</ul>
<h3 id="团队合作">5. 团队合作</h3>
<h4 id="多与同学同事交流">多与同学/同事交流</h4>
<p><strong>途径</strong>：向他们解释不理解的观点或算法，并说明自己尝试做的东西<br />
<strong>目的</strong>：更容易地发现错误和潜在问题，吸收其他人提出的想法</p>
<hr />
<h2 id="我的收获">我的收获</h2>
<ol type="1">
<li>研究方向和问题的确定是研究的第一步，选择合适且“有品位”的问题</li>
<li>确定了研究目标就要坚持不懈地阅读和实验，且实验的想法和目标很重要</li>
<li>要培养有效的研究习惯，如记好笔记、坚持不懈等</li>
<li>目标长远，走出舒适区，丰富领域各方面知识</li>
<li>论文阅读要讲求阅读方法以提高研究效率</li>
<li>研究不是单打独斗，要注重团队合作，从团队交流中收获新的想法</li>
</ol>
<blockquote>
<p>来源： <span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvUU1ETktDMC1zWnU1cDhMZE1JVUxmZw==">机器之心公众号<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
]]></content>
      <categories>
        <category>RP - 科研论文</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Research</tag>
      </tags>
  </entry>
  <entry>
    <title>【科研思维】科研之路</title>
    <url>/2020/01/14/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/research-great-researcher/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>关于一些学者对科研历程、科研方法和科研精神的总结。（上次更新：2020/11/06）</p>
<p>整理自：</p>
<ul>
<li>智源研究院</li>
</ul>
<p>学者：</p>
<ul>
<li><span class="exturl" data-url="aHR0cDovL2luZm8ucnVjLmVkdS5jbi9hY2FkZW1pY19wcm9mZXNzb3IucGhwP3RlYWNoZXJfaWQ9NTU=">RUC 赵鑫 副教授<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5jcy50c2luZ2h1YS5lZHUuY24vcHVibGlzaC9jcy80NjE2LzIwMTEvMjAxMTAzMzAxMDE5Mzk3ODc0ODM1NDkvMjAxMTAzMzAxMDE5Mzk3ODc0ODM1NDlfLmh0bWw=">THU 唐杰 教授<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cDovL25scC5jc2FpLnRzaW5naHVhLmVkdS5jbi9+bHp5L2luZGV4X2NuLmh0bWw=">THU 刘知远 副教授<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cDovL3Blb3BsZS51Y2FzLmFjLmNuL35zaGVuaHVhd2Vp">UCAS 沈华伟 研究员<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<a id="more"></a>
<hr />
<h2 id="学生">学生</h2>
<h3 id="本科生的问题-ruc-赵鑫-副教授">本科生的问题 —— RUC 赵鑫 副教授</h3>
<p>选择太多，太多时间花在选择上，而没有沉下心来做科研；倘若静下心来一直做，肯定会做出很好的成果。</p>
<h3 id="博士生的三种境界-thu-刘知远-副教授">博士生的三种境界 —— THU 刘知远 副教授</h3>
<ul>
<li>境界一：能够解决开放问题——面对一片未知，找出一条路。（文献调研）</li>
<li>境界二：具备科研方向感——知道哪些问题是重要且能被解决的。（找到问题）</li>
<li>境界三：负有领域责任感——对领域未来发展，负有责任感；领域兴亡，匹夫有责。（领域责任）</li>
</ul>
<h3 id="赠予同学的三句话-ruc-赵鑫-副教授">赠予同学的三句话 —— RUC 赵鑫 副教授</h3>
<p>送给各位来我网页驻足同学三句话 童第周先生的两句话：</p>
<ul>
<li>（1）“一定要争气。我并不比别人笨。别人能办到的事，我经过努力，一定也能办到。”</li>
<li>（2）“一定要争气。中国人并不比外国人笨。外国人认为很难办的事，我们中国人经过努力，一定能办到。”</li>
</ul>
<p>还有一句《追梦赤子心》的歌词： - （3）“为了心中的美好，不妥协，一直到老”</p>
<h2 id="学生---学者">学生 -&gt; 学者</h2>
<h3 id="从学生到学者的四个阶段-thu-唐杰教授">从学生到学者的四个阶段 —— THU 唐杰教授</h3>
<ul>
<li>阶段一：在引路人的带领下，踏踏实实把一件事做到极致；（Top Conference or Top Journal）</li>
<li>阶段二：想一个Idea，在被告知哪些不能做后，把能做的部分做到极致；（Top Conference or Top Journal）</li>
<li>阶段三：完全独立地想一个Idea，并独立地做完、做好； （Independent）</li>
<li>阶段四：能够带着第一阶段的人，引导他把一件事情做好。（Expand）</li>
</ul>
<h2 id="学者">学者</h2>
<h3 id="优秀学者的三层境界-cas-沈华伟-研究员">优秀学者的三层境界 —— CAS 沈华伟 研究员</h3>
<ul>
<li>1、解决问题，为领域发展做贡献；（科研）</li>
<li>2、传道受业，带起一批人；（团队）</li>
<li>3、突破自我，服务社会。（贡献）</li>
</ul>
<p>学而优则仕(X)，学而优则向其他领域拓展做贡献</p>
<h3 id="正确的科研道路-ruc-赵鑫-副教授">正确的科研道路 —— RUC 赵鑫 副教授</h3>
<p>AI -&gt; 研究点</p>
<ul>
<li>打基础：入门资源</li>
<li>看论文：快速浏览 -&gt; 感兴趣方向 -&gt; 顶会发展 -&gt; 哪些想做 -&gt; 研究点</li>
<li>研究点：综述文章 -&gt; 研究进展与脉络</li>
</ul>
<h3 id="学术生涯的核心精神-thu-唐杰教授">学术生涯的核心精神 —— THU 唐杰教授</h3>
<ol type="1">
<li>专注，专注，再专注 不要在乎别人的眼光，只管专注做</li>
<li>做事情追求极致</li>
</ol>
<h3 id="青源talk的寄语-cas-沈华伟-研究员">青源talk的寄语 —— CAS 沈华伟 研究员</h3>
<p>经得起质疑， 耐得住寂寞，扛得住压力。</p>
]]></content>
      <categories>
        <category>RP - 科研论文</category>
      </categories>
      <tags>
        <tag>Research</tag>
      </tags>
  </entry>
  <entry>
    <title>【学术讲座】潘复生院士谈科学创新</title>
    <url>/2019/09/22/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/lecture-academician-pan-fu-sheng/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>重庆大学材料学院潘复生院士关于科学创新的讲座</p>
<a id="more"></a>
<h2 id="科学发展现状">科学发展现状</h2>
<p>资源匮乏<br />
信息科技依赖<br />
生物科学<br />
学科交叉对知识要求越来越高</p>
<h3 id="我国发展现状">我国发展现状</h3>
<p>16个重大专项</p>
<p>科技实力低于美国<br />
科技投入远低于发达国家<br />
基础研究待加强<br />
科学素养低（官员政策制定）科普</p>
<h2 id="创新素质培养">创新素质培养</h2>
<h3 id="学会合作">学会合作</h3>
<p>团结他人（比自己优秀的和略差与自己的）</p>
<h3 id="学会放弃">学会放弃</h3>
<p>（选择自己最喜欢的方向）</p>
<h3 id="学会学习">学会学习</h3>
<p>（ 知识学习能力远比知识记忆或积累能力更重要<br />
学习能力比死记硬背更重要<br />
基本工具一定要好（英语，计算机）<br />
学会忘记有时比学会记住更重要 ）</p>
<h3 id="学会分析和怀疑提问问题">学会分析和怀疑+提问问题</h3>
<p>（不鼓励小中学生瞎猜怀疑=》知识积累+理解世界）<br />
大学生学会怀疑</p>
<h3 id="学会表达">学会表达</h3>
<p>（让他人理解，好的表达扬长避短，站在对方角度来表达）</p>
<h2 id="科研技巧">科研技巧</h2>
<ol type="1">
<li>重视偶然性才有必然性</li>
<li>交流和讨论（潘与汉诺丁-英欧支持+国家支持）</li>
<li>仿生学（创新启示）</li>
<li>做多数人喜欢的事情（做好不喜欢的事，做自己喜欢的事做的会更好）</li>
<li>逆向思维是成功的捷径</li>
</ol>
<h2 id="我的感悟">我的感悟</h2>
]]></content>
      <categories>
        <category>RP - 科研论文</category>
      </categories>
      <tags>
        <tag>Lecture</tag>
        <tag>Innovatation</tag>
        <tag>Research</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习知识体系</title>
    <url>/2019/08/22/ML%20-%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/dl-introduction/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><a id="more"></a>
<h3 id="基础网络">基础网络</h3>
<ul>
<li>CNN</li>
<li>RNN</li>
<li>GNN</li>
</ul>
<h3 id="应用场景">应用场景</h3>
<ul>
<li>CV 计算机视觉</li>
<li>NLP 自然语言处理</li>
<li>KG 知识图谱</li>
</ul>
<h3 id="主流框架">主流框架</h3>
<ul>
<li>TensorFlow</li>
<li>PyTorch</li>
<li>PaddlePaddle</li>
<li>MindSpore</li>
</ul>
]]></content>
      <categories>
        <category>ML - 机器学习</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Start the Journey of My Blog</title>
    <url>/2019/08/03/MY%20-%20%E6%84%9F%E6%82%9F%E9%9A%8F%E7%AC%94/my-start-my-blog/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script>
    <div id="aplayer-pCjvrqCT" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="0046Ze2d2A7jeA" data-server="tencent" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#ad7a86"
    ></div>
<h2 id="welcome-to-my-blog">Welcome to My Blog</h2>
<p>Gemini向光性<br />
困在双子星座的流浪旅人<br />
<a href="/about">了解更多</a></p>
<a id="more"></a>
<h2 id="involved-fields">Involved Fields</h2>
<p>博客计划主要涉及以下内容：</p>
<h3 id="技术">技术</h3>
<ul>
<li>机器学习：传统机器学习、深度学习、强化学习</li>
<li>数据分析：Numpy、Pandas、Scipy、Matlibplot</li>
<li>全栈开发：Vue、jQuery、MySQL、SpringBoot、Django</li>
<li>编程语言：Python、C/C++、Java、JS/TS</li>
</ul>
<h3 id="金融">金融</h3>
<ul>
<li>宏观经济</li>
<li>微观经济</li>
<li>企业分析</li>
<li>金融分析</li>
</ul>
<h3 id="设计">设计</h3>
<ul>
<li>PPT设计</li>
<li>平面设计</li>
<li>视频剪辑</li>
</ul>
<h3 id="生活">生活</h3>
<ul>
<li>随笔</li>
<li>音乐</li>
<li>摄影</li>
<li>影视</li>
<li>游戏</li>
<li>烹饪</li>
</ul>
<p>但深知自己才疏博浅且臻爱原创，又不愿随波逐流，故更新进度随缘。</p>
]]></content>
      <categories>
        <category>MY - 感悟随笔</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>【Python】基础语法总结</title>
    <url>/2019/01/11/PL%20-%20%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/pl-python-fundamental-syntax/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script>
]]></content>
      <categories>
        <category>PL - 编程语言</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>【游戏人生】双子Gemini——我们都在寻找着某个人</title>
    <url>/2017/08/07/GL%20-%20%E6%B8%B8%E6%88%8F%E4%BA%BA%E7%94%9F/game-gemini/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script>
    <div id="aplayer-OLRplmSq" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="0004CQiv4NUbmR" data-server="tencent" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#ad7a86"
    ></div>
<blockquote class="blockquote-center">
<p>“我们都在寻找着 某个人”——《你的名字。》</p>

</blockquote>
<p>暗夜中的荧树依旧沉睡着，它已经很久很久没有发过光了，久的人们都早已忘记了那个传说......</p>
<a id="more"></a>
<h2 id="一相遇前尘如是">一、相遇：前尘如是</h2>
<p>少了星光的装点，夜空暗淡了许多。我心有所念地张望着四周，却又不知在找寻着谁。明明毫无目的地来这儿，却又诚惶诚恐地害怕着错过。空荡荡的心将无神的目光映向不远处沉眠的荧树，然后莫名其妙地叹息。准备离开时的回眸，让我们的目光在朦胧的暮色下交错，我似乎懂了，懂得了命运织缠的意义。</p>
<p><img data-src="/resource/images/game/game-gemini-1.png" /></p>
<p>时间放佛在那一刻停格，空气也随之凝滞，任由荧树的年轮刻录下心动和心悸： 你尚未出现时，我的生命平静，轩昂阔步行走，动辄料事如神；如今惶乱，怯弱，像冰融的春水，一流就流向你，又不知你在何处（木心）。可明明你就在我面前，我却又不知所措。</p>
<p><img data-src="/resource/images/game/game-gemini-2.png" /></p>
<p>空气不再沉默——似乎有种引力牵引着我慢步走向了你。脚步声在寂寥的夜空中徘徊，你退回荧树枝下，等候着我的到来。但当手被牵起时，你却皱了皱眉，有些许心痛，却又不知痛从何而来。彼此只手相依，只手迎风，曼舞空凉。执子之手，挥洒荧光。古树开始苏醒，重现荧光，夜空渐被点亮，直至整个银河宛若天堂。</p>
<h2 id="二别离心恸幽思">二、别离：心恸幽思</h2>
<p><img data-src="/resource/images/game/game-gemini-3.png" /></p>
<p>面前是无数陨石阻隔，彼岸即是梦寐以求的星河。无数次的碰撞，无数次的跌倒，无数次的逃亡。看着身旁遍体鳞伤的你，我心如刀割。头脑开始眩晕，身体开始麻木，但始终未停止撞击。终于，你，停了下来，慢慢下坠。此时，我，早已泪眼婆娑，椎心泣血。抱紧昏倒的你，看着你迂回的伤痕，痛恨自己的无力。</p>
<p><img data-src="/resource/images/game/game-gemini-4.png" /></p>
<p>恸哭一声——微光亦可昼亮，化作流星花火，划破无边银河。天空被渲染成晨曦般的颜色，陨石被狠狠撞碎坠落，星际被灿烂夺去轮廓。我回首望向了你，微笑着轻抚了一下，拭去你眼角的泪花。凝视着你，沉默，却又更多话语。回顾此生，路途虽险，却不惧混沌黑夜，不畏疾风骤雨，只因有你相伴左右，不曾离弃。</p>
<p><img data-src="/resource/images/game/game-gemini-6.png" /></p>
<p>原谅我，不再陪你，望这最后一星余晖燃尽守护你；<br />
失去我，日月如常，世上最美的星云就在前方；<br />
答应我，生活下去，未来你会有更好的相遇。</p>
<p>星辉殆尽，堙灭散落，银河再归混沌，我已一别永年。就这样，我离开了你，无声无息，了无痕迹。</p>
<h2 id="三追忆念旧顾逝">三、追忆：念旧顾逝</h2>
<p>最后一丝星火被黑夜淹没，前方不再有陨石阻隔，她向前扑去，想要如往常般依偎着他，却痛痛地跌倒在地。眼泪肆意挥霍着，润湿了瞳孔，模糊了每个角落；打在了心口，放任痛楚清晰地游走，倒带往昔彼此依偎的甜蜜：</p>
<p><img data-src="/resource/images/game/game-gemini-14.png" /> <img data-src="/resource/images/game/game-gemini-15.png" /> <img data-src="/resource/images/game/game-gemini-16.png" /> <img data-src="/resource/images/game/game-gemini-17.png" /> <img data-src="/resource/images/game/game-gemini-19.png" /></p>
<p>荧树下他许诺让她看到世间最美的星云，带她浪迹天涯。 黎明的曙光照亮孤寂的夜空，驱赶着不羁的黑暗，他们双手相牵，追赶着晨曦，一起见证日出的那一抹微光。正午的太阳愈渐熠亮，他们在风中嬉戏，在路旁依靠，感受着对方的呼吸，心透着无声的默契。黄昏悄悄降临，彩霞中奔跑的身影停下了脚步，最浪漫不过彼此并肩欣赏夕阳。</p>
<p>时而细雨微作，他们轻歌慢步；时而骤风忽起，他们依偎拥抱。他们在文明废墟中相伴而游，见证希望；在极地冰川中相依而行，战克风霜；在富丽殿堂中相视而笑，共享荣光。音符弹奏着甜蜜，图腾印刻下笑容。</p>
<p>日出日落，朝暮共同分享，无惧颠簸跌宕；<br />
路途遥远，同游山高水长，风景彼此共赏。</p>
<p>而这一切已成过往……</p>
<h2 id="四辉煌星夜如斯">四、辉煌：星夜如斯</h2>
<p>任何一种环境或一个人，初次见面就预感到离别的隐痛时，你必定是爱上他了。（黄永玉）她明白了初遇时的心悸，不是青涩，亦不是恐惧，而是她早已预见他们命中注定要分离。 霓虹渐显倪端，微光开始蔓延，泪花随风飘落，伊人难以忘怀。星光映在她的脸庞，尽是泪行。手紧紧的攥着，慢步向前，她的眼神中少了悲伤，多了空灵，即使面前魂牵梦绕的星云也扬不起她嘴角的一丝微笑。</p>
<p>景虽美，光虽亮，可，泪已尽，心已灰。</p>
<p>没有形单影只的倾诉，没有感天动地的恸哭，甚至没有了恋人别离的苦楚，她走向星云的中央，目光死死的盯着他星辉散尽的方向，双臂展开，静静释怀……</p>
<p><img data-src="/resource/images/game/game-gemini-12.png" /></p>
<p>混沌银河骤然生光，星云再次燃起辉亮——她竭尽星辉在祈愿，不求时间重返，只求来世重逢。那一刻，宇宙听到了她的心声，苍穹为之感染。夜空灰暗千年，不再寂寥，星辉绚丽，占据每一寸黑暗。她最后一次俯瞰世间，不再有任何羁绊，微微一笑，随风消散。</p>
<p><img data-src="/resource/images/game/game-gemini-13.png" /></p>
<p>周边的星辰说那是他们一生中见过的最美最亮的星辉：冰冷中透着暖意，凄美却又不失灿烂，那不是悲伤，而是诠释，是渴盼，是希望。 言语停了下来，荧树又重归于暗淡... ...</p>
<h2 id="五来世情缘何去">五、来世：情缘何去</h2>
<p>银河浩渺无垠，星辰繁若樱花，来生相距光年十万，抑或咫尺之遥，可否会重逢再会？如若他日于星海偶遇，你我会擦肩而过，彼此不识，还是会心动如昔，情缘再续？ 命运最神秘的不是变幻莫测无人知晓，而是冥冥之中自有定向。</p>
<hr />
<p>荧树沉寂了万年，传说不再被流传，但至今仍会有人说：</p>
<blockquote class="blockquote-center">
<p>一次告别 天上就会有颗星 又熄灭</p>

</blockquote>
]]></content>
      <categories>
        <category>MY - 感悟随笔</category>
      </categories>
      <tags>
        <tag>Game</tag>
        <tag>Sentiment</tag>
      </tags>
  </entry>
</search>
