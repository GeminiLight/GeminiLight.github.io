<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-32x32.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-32x32.ico">
  <link rel="mask-icon" href="/images/favicon-32x32.ico" color="#222">
  <meta name="google-site-verification" content="RrL9pY7_YVrRxyUv7G4ksTPRGYJmECxw-uI8iVOz7vI">
  <meta name="baidu-site-verification" content="AozFAayi1Z">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.geminilight.cn","root":"/","scheme":"Gemini","version":"8.0.0-rc.5","exturl":true,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"path":"search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="随着深度学习模型的不断增大、数据的不断增多，并行计算成为了解决机器学习训练难题的一种主流技术。本文主要是对Shusen Wang的《分布式机器学习》课程的笔记记录和扩展，仅是对机器学习中并行计算方式的概述。">
<meta property="og:type" content="article">
<meta property="og:title" content="【分布式ML】机器学习中的并行计算">
<meta property="og:url" content="https://www.geminilight.cn/2021/02/19/ML%20-%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/dml-parallel-computing-and-machine-learning/index.html">
<meta property="og:site_name" content="Gemini向光性">
<meta property="og:description" content="随着深度学习模型的不断增大、数据的不断增多，并行计算成为了解决机器学习训练难题的一种主流技术。本文主要是对Shusen Wang的《分布式机器学习》课程的笔记记录和扩展，仅是对机器学习中并行计算方式的概述。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-1.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-2.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-3.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-4.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-5.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-6.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-7.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-8.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-9.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-10.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-11.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-12.gif">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-13.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-14.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-15.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-16.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-17.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-18.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-19.png">
<meta property="og:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-20.png">
<meta property="article:published_time" content="2021-02-18T16:00:00.000Z">
<meta property="article:modified_time" content="2021-02-22T02:13:25.501Z">
<meta property="article:author" content="GeminiLight">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="分布式">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.geminilight.cn/resource/images/dml/dml-parallel-computing-and-machine-learning-1.png">


<link rel="canonical" href="https://www.geminilight.cn/2021/02/19/ML%20-%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/dml-parallel-computing-and-machine-learning/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>【分布式ML】机器学习中的并行计算 | Gemini向光性</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?81607a4529b9e096ccf0ac76663fbc7c";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  <script>
    !function(e,t,n,g,i){e[i]=e[i]||function(){(e[i].q=e[i].q||[]).push(arguments)},n=t.createElement("script"),tag=t.getElementsByTagName("script")[0],n.async=1,n.src=('https:'==document.location.protocol?'https://':'http://')+g,tag.parentNode.insertBefore(n,tag)}(window,document,"script","assets.growingio.com/2.1/gio.js","gio");
    gio('init', 'UA-175467019-1', {});
    gio('send');
  </script>


  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Gemini向光性</h1>
      <i class="logo-line"></i>
    </a>
    
      <p class="site-subtitle" itemprop="description">The light always goes on</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container">
  <div class="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-text">概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97"><span class="nav-text">什么是并行计算？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E9%9C%80%E8%A6%81%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97"><span class="nav-text">为什么机器学习中需要并行计算？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97"><span class="nav-text">并行计算与分布式计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E5%AE%9E%E4%BE%8B"><span class="nav-text">房价预测实例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E9%A2%84%E6%B5%8B%E5%99%A8"><span class="nav-text">线性预测器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%AE%AD%E7%BB%83"><span class="nav-text">并行梯度下降训练</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5"><span class="nav-text">概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E4%BF%A1-communication"><span class="nav-text">通信 Communication</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E5%85%B1%E4%BA%AB-share-memory"><span class="nav-text">内存共享 Share memory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92-message-passing"><span class="nav-text">消息传递 Message passing</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84-architecture"><span class="nav-text">架构 Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#client-server"><span class="nav-text">Client-Server</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#peer-to-peer"><span class="nav-text">Peer-to-Peer</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5%E6%80%A7"><span class="nav-text">同步性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%B9%E5%90%8C%E6%AD%A5-bulk-synchronous"><span class="nav-text">批同步 Bulk Synchronous</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%82%E6%AD%A5-asynchronous"><span class="nav-text">异步 Asynchronous</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E6%80%A7-parallelism"><span class="nav-text">并行性 Parallelism</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="nav-text">数据并行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C"><span class="nav-text">模型并行</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%88%90%E6%9C%AC%E4%B8%8E%E5%8A%A0%E9%80%9F%E6%AF%94-cost-speedup-ratio"><span class="nav-text">成本与加速比 Cost &amp; Speedup Ratio</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%88%90%E6%9C%AC"><span class="nav-text">计算成本</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E4%BF%A1%E6%88%90%E6%9C%AC"><span class="nav-text">通信成本</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5%E6%88%90%E6%9C%AC"><span class="nav-text">同步成本</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6"><span class="nav-text">主流框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#mapreduce"><span class="nav-text">MapReduce</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A1%86%E6%9E%B6%E7%89%B9%E7%82%B9"><span class="nav-text">框架特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B"><span class="nav-text">计算流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%97%B6%E9%97%B4%E6%B6%88%E8%80%97"><span class="nav-text">时间消耗</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#parameter-server"><span class="nav-text">Parameter Server</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A1%86%E6%9E%B6%E7%89%B9%E7%82%B9-1"><span class="nav-text">框架特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-1"><span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%97%B6%E9%97%B4%E6%B6%88%E8%80%97-1"><span class="nav-text">时间消耗</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90"><span class="nav-text">性能分析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decentralized-network"><span class="nav-text">Decentralized Network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A1%86%E6%9E%B6%E7%89%B9%E7%82%B9-2"><span class="nav-text">框架特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-2"><span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90-1"><span class="nav-text">性能分析</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-text">参考</span></a></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="GeminiLight"
      src="/images/avatar.webp">
  <p class="site-author-name" itemprop="name">GeminiLight</p>
  <div class="site-description" itemprop="description">stay hungry, stay foolish</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dlbWluaWxpZ2h0" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;geminilight"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnd0Zmx5MjAxOEAxNjMuY29t" title="E-Mail → mailto:wtfly2018@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS9nZW1pbmlfbGlnaHQ=" title="ZhiHu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;gemini_light"><i class="fa fa-book fa-fw"></i>ZhiHu</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9zcGFjZS5iaWxpYmlsaS5jb20vMTc1Mzg2MDMz" title="BiliBili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;175386033"><i class="fa fa-tv fa-fw"></i>BiliBili</span>
      </span>
  </div>






  <div class="links-of-blogroll animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <span class="exturl" data-url="aHR0cHM6Ly93d3cueXVxdWUuY29tL2Jvb2tzL3NoYXJlLzhlZTc2OTRiLTFiMjgtNDE3Yy05MjFmLTQ1YWY0Zjg0ZDcwYz8jIOOAik1MIC0g5py65Zmo5a2m5Lmg44CL" title="https:&#x2F;&#x2F;www.yuque.com&#x2F;books&#x2F;share&#x2F;8ee7694b-1b28-417c-921f-45af4f84d70c?# 《ML - 机器学习》">我的机器学习笔记</span>
        </li>
    </ul>
  </div>

      </section>



<div style="">
  <canvas id="canvas" style="width:60%; margin: 20px 0 0 0;">当前浏览器不支持canvas，请更换浏览器后再试</canvas>
</div>
<script>
(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();
</script>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.geminilight.cn/2021/02/19/ML%20-%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/dml-parallel-computing-and-machine-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.webp">
      <meta itemprop="name" content="GeminiLight">
      <meta itemprop="description" content="stay hungry, stay foolish">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gemini向光性">
    </span>

    
    
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【分布式ML】机器学习中的并行计算
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-02-19 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-19T00:00:00+08:00">2021-02-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">ML - 机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4.3k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>随着深度学习模型的不断增大、数据的不断增多，并行计算成为了解决机器学习训练难题的一种主流技术。本文主要是对Shusen Wang的《分布式机器学习》课程的笔记记录和扩展，仅是对机器学习中并行计算方式的概述。</p>
<a id="more"></a>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-1.png" /></p>
<h2 id="概述">概述</h2>
<h3 id="什么是并行计算">什么是并行计算？</h3>
<p>使用多个处理器来加速计算过程，使时钟运行时间更短 多个处理器的情况包括</p>
<ul>
<li>一台计算机上有多个CPU</li>
<li>多个计算机的CPU</li>
</ul>
<h3 id="为什么机器学习中需要并行计算">为什么机器学习中需要并行计算？</h3>
<ul>
<li>深度学习模型大。模型参数越来越多</li>
<li>训练数据多。比如ImageNet有14M张图片</li>
<li>计算资源成本高。在大模型上训练大数据集的必然结构</li>
</ul>
<h3 id="并行计算与分布式计算">并行计算与分布式计算</h3>
<p>分布式计算可以看做是一种并行计算。两者的界限是模糊的，但从机器学习研究人员角度出发</p>
<ul>
<li><p>并行计算 Parallel computing</p>
<ul>
<li>数据或模型被分划分成多份，但这些CPU都属于一台电脑，并行计算任务仍在一台电脑节点执行</li>
</ul></li>
<li><p>分布式计算 Distributed computing</p>
<ul>
<li>数据或模型被分划分成多份，且在不同的节点上进行计算</li>
</ul></li>
</ul>
<h3 id="房价预测实例">房价预测实例</h3>
<h4 id="线性预测器">线性预测器</h4>
<ul>
<li><p>输入：<span class="math inline">\(X \in \mathbb{R}^d\)</span>（有关房价的特征）</p></li>
<li><p>预测：<span class="math inline">\(f(X) = X^T W\)</span>（房价）</p>
<ul>
<li>假设有<span class="math inline">\(d\)</span>个房价相关的特征，如面积、年限等</li>
<li><span class="math inline">\(f(X) = w_1x_1 + w_2x_2 + w_3x_3 + \cdots + w_dx_d\)</span></li>
<li>其中<span class="math inline">\(x_i\)</span>表示特征，<span class="math inline">\(w_i\)</span>是其权重</li>
</ul></li>
</ul>
<h4 id="训练模型">训练模型</h4>
<ul>
<li><p>训练集</p>
<ul>
<li>输入：<span class="math inline">\(X_1, \cdots, X_n \in \mathbb{R}^d\)</span></li>
<li>标签：<span class="math inline">\(y_1, \cdots, y_n \in \mathbb{R}\)</span></li>
</ul></li>
<li><p>损失函数：<span class="math inline">\(L(\mathbf{W})=\sum_{i=1}^{n} \frac{1}{2}\left(\mathbf{X}_{i}^{T} \mathbf{W}-y_{i}\right)^{2}\)</span></p></li>
<li><p>最小二乘回归：<span class="math inline">\(\mathbf{W}^{*}=\underset{\mathbf{W}}{\operatorname{argmin}} L(\mathbf{W})\)</span></p></li>
<li><p>梯度下降：</p>
<ul>
<li>梯度：<span class="math inline">\(g(W)=\frac{\partial L(\mathrm{W})}{\partial \mathrm{W}}=\sum_{i=1}^{n} \frac{\partial \frac{1}{2}\left(\mathbf{X}_{i}^{T} \mathrm{W}-y_{i}\right)^{2}}{\partial \mathrm{W}}=\sum_{i=1}^{n}\left(\mathbf{X}_{i}^{T} \mathbf{W}-y_{i}\right) \mathbf{X}_{i}\)</span></li>
<li><span class="math inline">\(g(\mathbf{W})=\sum_{i=1}^{n} g_{i}(\mathbf{W}), \text { where } g_{i}(\mathbf{W})=\left(\mathbf{X}_{i}^{T} \mathbf{W}-y_{i}\right) \mathbf{X}_{i}\)</span></li>
<li><span class="math inline">\(W_{t+1} = W_t - \alpha \cdot g(W_t)\)</span></li>
</ul></li>
</ul>
<p>我们可以发现，当仅一个处理器处理大的模型`和海量数据的时候，计算量是巨大的。</p>
<h4 id="并行梯度下降训练">并行梯度下降训练</h4>
<p>并行梯度下降的基本思想便是利用多个处理器来分别利用自己的数据计算梯度，最后通过聚合或其他方式来实现并行计算梯度下降以加速模型训练过程。 下面的图示便展示了两个处理器各自利用一半数据计算梯度<span class="math inline">\(\tilde{g}_1\)</span>、<span class="math inline">\(\tilde{g}_2\)</span>，然后将两个结果进行聚合，实现了并行梯度下降：</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-2.png" /></p>
<h2 id="概念">概念</h2>
<h3 id="通信-communication">通信 Communication</h3>
<h4 id="内存共享-share-memory">内存共享 Share memory</h4>
<p>当处理器共用一个内存，它们可以访问相同的数据，直接进行通信</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-3.png" /></p>
<h4 id="消息传递-message-passing">消息传递 Message passing</h4>
<p>当存在多个节点的时候，不同节点上的处理器所访问的内存是不同的。这种情况下，需要通过消息传递来进行通信</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-4.png" /></p>
<h3 id="架构-architecture">架构 Architecture</h3>
<h4 id="client-server">Client-Server</h4>
<p>一个节点作为Server来协调其他节点，其他节点作为Worker来执行计算任务</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-5.png" /></p>
<h4 id="peer-to-peer">Peer-to-Peer</h4>
<p>每个节点都是平等的，它们都有邻居，邻居之间可以通信</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-6.png" /></p>
<h3 id="同步性">同步性</h3>
<h4 id="批同步-bulk-synchronous">批同步 Bulk Synchronous</h4>
<p>当全部Worker节点发送梯度信息到Server后，Server才进行一次参数更新</p>
<h4 id="异步-asynchronous">异步 Asynchronous</h4>
<p>当一个Worker将计算出的梯度传回Server时，Server随即进行一次参数更新</p>
<h3 id="并行性-parallelism">并行性 Parallelism</h3>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-7.png" /></p>
<h4 id="数据并行">数据并行</h4>
<p>这是主流的并行方式。将数据划分成若干份，分别在拥有整个模型的Worker上进行梯度计算，不同Worker间梯度计算是并行的</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-8.png" /></p>
<h4 id="模型并行">模型并行</h4>
<p>当模型太大了以致于不能把整个模型载入一个GPU中，可以考虑把整个模型按层或其他方式分解成若干份。</p>
<ul>
<li>比如每个不同的节点计算着整个模型的不同的层，计算着不同的层的梯度。</li>
<li>模型较后部分的计算必须等前面计算完成，因此不同节点间的计算实际是串行的。但每个部分计算互不妨碍，更像是流水线结构</li>
</ul>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-9.png" /></p>
<h3 id="成本与加速比-cost-speedup-ratio">成本与加速比 Cost &amp; Speedup Ratio</h3>
<p><span class="math display">\[加速比 = \frac{使用一个节点消耗的时钟时间}{使用m个节点消耗的时钟时间}\]</span></p>
<p>实际加速比是低于<span class="math inline">\(m\)</span>倍的：存在其他成本，如通信和同步</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-10.png" /></p>
<h4 id="计算成本">计算成本</h4>
<p>节点执行计算任务所消耗的时间，与模型大小、数据量和节点计算性能有关。</p>
<h4 id="通信成本">通信成本</h4>
<p>数据传输所消耗的时间，通常由通信复杂度和延迟所决定 <span class="math display">\[T = \frac{\text{complexity}}{\text{bandwith}} + \text{latency}\]</span></p>
<ul>
<li><p>通信复杂度</p>
<ul>
<li>即Server与Worker传输的数据大小</li>
<li>随模型参数量和Worker节点数量增加而增长</li>
</ul></li>
<li><p>延迟</p>
<ul>
<li>数据传输时可能会产生延迟</li>
<li>取决于计算机网络状况</li>
</ul></li>
</ul>
<h4 id="同步成本">同步成本</h4>
<p>当采用同步的计算方法时（如MapReduce），就可能会产生同步成本，因为不同节点完成相同任务的时间可能是不同的。 Stragger节点</p>
<ul>
<li>一个严重慢于其他节点的Worker，比如因为特殊情况而重启</li>
<li>同步并行计算方法实际时钟时间消耗取决于最慢的节点</li>
</ul>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-11.png" /></p>
<h2 id="主流框架">主流框架</h2>
<table>
<thead>
<tr class="header">
<th></th>
<th><strong>通信</strong></th>
<th><strong>架构</strong></th>
<th><strong>并行性</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MapReduce</td>
<td>Message passing</td>
<td>client-server</td>
<td>synchronous</td>
</tr>
<tr class="even">
<td>Parameter Server</td>
<td>Message passing</td>
<td>client-server</td>
<td>asynchronous</td>
</tr>
<tr class="odd">
<td>Decentralized</td>
<td>Message passing</td>
<td>peer-to-peer</td>
<td>synchronous or asynchronous</td>
</tr>
</tbody>
</table>
<h3 id="mapreduce">MapReduce</h3>
<p>由Google提出</p>
<ul>
<li>本质思想：Server广播参数，Worker执行Map运算并将结果返回Server，Server执行Reduce，从而完成一次参数更新</li>
<li>开源实现：Hadoop、Spark等</li>
</ul>
<h4 id="框架特点">框架特点</h4>
<ul>
<li>架构：client-server</li>
<li>通信：消息传递</li>
<li>并行方式：批量同步</li>
</ul>
<h4 id="计算流程">计算流程</h4>
<ol type="1">
<li><code>Broadcast</code> ：Server将参数发送给每个Worker</li>
<li><code>Map</code> ：每个worker利用自己的数据进行Map操作</li>
<li><code>Reduce</code> ：Worker将Map后的结果发送到Server，Server进行Reduce操作</li>
</ol>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-12.gif" /></p>
<h4 id="梯度下降">梯度下降</h4>
<p><strong>数据并行 Data Parallelism</strong>：在Worker节点之间划分数据（节点具有数据子集） 在本例中，我们假设有<span class="math inline">\(m\)</span>个节点，因此将数据分成<span class="math inline">\(m\)</span>份，每个节点拥有<span class="math inline">\(\frac{1}{m}\)</span>数据</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-13.png" /></p>
<ol type="1">
<li><p><code>Broadcast</code> ：Server广播最新的模型参数<span class="math inline">\(W_t\)</span>到各个Worker节点</p></li>
<li><p><code>Map</code> ：Worker在本地执行计算任务</p>
<ol type="1">
<li>将<span class="math inline">\((X_i, y_i, W_t)\)</span>映射到<span class="math inline">\(g_i = (X^T_i W_t - y_i)X_i\)</span></li>
<li>获得<span class="math inline">\(n\)</span>个梯度向量：<span class="math inline">\(g_i, \cdots, g_n\)</span>（<span class="math inline">\(n\)</span>是指样本个数）</li>
</ol></li>
<li><p><code>Reduce</code> ：计算梯度和<span class="math inline">\(g = \sum^n_{i=1} g_i\)</span></p>
<ol type="1">
<li>Worker在本地将自己计算出的梯度进行求和得到一个向量</li>
<li>Server根据每个Worker计算出的向量再进行一次求和</li>
</ol></li>
<li><p><code>Update</code> ：Server更新参数 <span class="math inline">\(W_{t+1} \leftarrow W_t - \alpha \cdot g\)</span></p></li>
</ol>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-14.png" /></p>
<h4 id="时间消耗">时间消耗</h4>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-15.png" /></p>
<h3 id="parameter-server">Parameter Server</h3>
<p>在<span class="exturl" data-url="aHR0cDovL3dlYi5lZWNzLnVtaWNoLmVkdS9+bW9zaGFyYWYvUmVhZGluZ3MvUGFyYW1ldGVyLVNlcnZlci5wZGY=">Scaling distributed machine learning with the parameter server<i class="fa fa-external-link-alt"></i></span>中被提出，</p>
<ul>
<li>本质思想：根据每个Worker传回的数据立即进行参数更新，即异步</li>
<li>开源实现：Ray</li>
</ul>
<h4 id="框架特点-1">框架特点</h4>
<ul>
<li>架构：client-server</li>
<li>通信：消息传递</li>
<li>并行方式：异步</li>
</ul>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-16.png" /></p>
<h4 id="梯度下降-1">梯度下降</h4>
<p>同样的，数据被划成<span class="math inline">\(m\)</span>个子集分到每个Worker节点</p>
<ul>
<li><p><strong>第<span class="math inline">\(i\)</span>个Worker节点重复：</strong></p>
<ul>
<li>从Server拉取最新参数<span class="math inline">\(W\)</span></li>
<li>利用本地数据和参数<span class="math inline">\(W\)</span>计算梯度<span class="math inline">\(\tilde{g}_i\)</span></li>
<li>将梯度推送到Server</li>
</ul></li>
<li><p><strong>Server执行：</strong></p>
<ul>
<li>从一个Worker收到梯度<span class="math inline">\(\tilde{g}_i\)</span></li>
<li>参数更新<span class="math inline">\(W \leftarrow W - \alpha \cdot \tilde{g}_i\)</span></li>
</ul></li>
</ul>
<h4 id="时间消耗-1">时间消耗</h4>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-17.png" /></p>
<h4 id="性能分析">性能分析</h4>
<ul>
<li><p>优点</p>
<ul>
<li>比同步的MapReduce更快</li>
</ul></li>
<li><p>缺点</p>
<ul>
<li>理论上，收敛速度更慢</li>
<li>如果某些节点比其他节点慢很多，梯度可能受损</li>
</ul></li>
</ul>
<p>图示：Wroker 3 在<span class="math inline">\(t_1\)</span>时刻才完成了一次梯度计算，并返回Server进行更新。而此时Server的参数已经被Worker 1 和 2 的梯度更新了数次，该梯度可能是不利于模型性能的</p>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-18.png" /></p>
<h3 id="decentralized-network">Decentralized Network</h3>
<h4 id="框架特点-2">框架特点</h4>
<ul>
<li>架构：P2P</li>
<li>通信：消息传递，且节点只与其邻居进行通信</li>
<li>并行方式：异步或同步</li>
</ul>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-19.png" /></p>
<h4 id="梯度下降-2">梯度下降</h4>
<p>第<span class="math inline">\(i\)</span>个节点重复执行</p>
<ul>
<li>使用本地参数<span class="math inline">\(\tilde{W_t}\)</span>来计算梯度<span class="math inline">\(\tilde{g_t}\)</span></li>
<li>拉取邻居节点的梯度，表示为<span class="math inline">\(\{\tilde{W_k}\}\)</span></li>
<li>计算梯度平均值 <span class="math inline">\(\tilde{W} \leftarrow \text{weighted average of } \tilde{W_i} \text{ and } \{\tilde{W_k}\}\)</span></li>
<li>更新参数 <span class="math inline">\(\tilde{W} \leftarrow \tilde{W} - \alpha \cdot \tilde{g}_i\)</span></li>
</ul>
<h4 id="性能分析-1">性能分析</h4>
<p>被证明是可收敛的，但收敛情况取决于网络的连接情况</p>
<ul>
<li>当网络的连接率是较好的时候，模型可以很快收敛</li>
<li>当节点间连接较少时，它可能不收敛</li>
</ul>
<p><img data-src="/resource/images/dml/dml-parallel-computing-and-machine-learning-20.png" /></p>
<h2 id="参考">参考</h2>
<blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1nVmNuT2U2X2M2USZhbXA7bGlzdD1QTHZPTzBidGxvUm5zNmVnWHVlaVJqdTREWFFqTlJKUWQ1">Shusen Wang《分布式机器学习》<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzUzODUxMDE0L2Fuc3dlci8xNTg3OTQ3NTI=">李哲龙. 分布式机器学习里的 数据并行 和 模型并行 各是什么意思？ 知乎<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9tZWRpdW0uY29tL0BqZXJ1OTIvZ2VvLWRpc3RyaWJ1dGVkLW1hY2hpbmUtbGVhcm5pbmctYW4tb3ZlcnZpZXctZWUzZmM0MmEwMzE5">Jeru Luke. Geo-Distributed Machine Learning: An Overview. Medium<i class="fa fa-external-link-alt"></i></span><br />
Li and others: Scaling distributed machine learning with the parameter server. In OSDI, 2014.</p>
</blockquote>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2019\08\22\ML - 机器学习\dl-introduction\" rel="bookmark">深度学习知识体系</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\03\02\ML - 机器学习\course-collection-on-ai\" rel="bookmark">【AI】人工智能课程集</a></div>
    </li>
  </ul>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Gemini向光性
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://www.geminilight.cn/2021/02/19/ML%20-%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/dml-parallel-computing-and-machine-learning/" title="【分布式ML】机器学习中的并行计算">https://www.geminilight.cn/2021/02/19/ML - 机器学习/dml-parallel-computing-and-machine-learning/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC96aC1DTg=="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" rel="tag"># 分布式</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/01/13/RP%20-%20%E7%A7%91%E7%A0%94%E8%AE%BA%E6%96%87/paper-nfv-vnf-placement-latency-aware-ne/" rel="prev" title="【论文笔记】Latency-aware VNF Chain Deployment with Efficient Resource Reuse at Network">
                  <i class="fa fa-chevron-left"></i> 【论文笔记】Latency-aware VNF Chain Deployment with Efficient Resource Reuse at Network
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/03/02/ML%20-%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/course-collection-on-ai/" rel="next" title="【AI】人工智能课程集">
                  【AI】人工智能课程集 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
      </footer>
    
  </article>
  
  
  



      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">GeminiLight</span>
</div>

      








    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.0/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1.15.0/dist/lozad.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>












  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      const script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>





  <script async type="text/javascript" src="/js/custom/cursor-firework.js"></script>

